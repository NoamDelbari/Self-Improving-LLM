{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "917ed43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets transformers openai bitsandbytes accelerate python-dotenv huggingface_hub huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a665b2d",
   "metadata": {},
   "source": [
    "## Setup ENV var config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba646b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Configuration ===\n",
      "Dataset: voidful/StrategyQA\n",
      "Model: microsoft/Phi-3.5-mini-instruct\n",
      "Batch size: 8\n",
      "4-bit quantization: True\n",
      "GPT-4 dry run: False\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Dataset parameters\n",
    "DATASET_NAME = os.getenv('DATASET_NAME', 'voidful/StrategyQA')\n",
    "TRAIN_SAMPLES = int(os.getenv('TRAIN_SAMPLES', '100'))\n",
    "RANDOM_SEED = int(os.getenv('RANDOM_SEED', '42'))\n",
    "USE_FULL_DATASET = os.getenv('USE_FULL_DATASET', 'False').lower() in ('true', '1', 't')\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "MODEL_NAME = os.getenv('MODEL_NAME', 'microsoft/phi-2')\n",
    "MAX_NEW_TOKENS = int(os.getenv('MAX_NEW_TOKENS', '35'))\n",
    "BATCH_SIZE = int(os.getenv('BATCH_SIZE', '8'))\n",
    "USE_4BIT = os.getenv('USE_4BIT', 'True').lower() in ('true', '1', 't')\n",
    "MAX_SEQ_LENGTH = int(os.getenv('MAX_SEQ_LENGTH', '512'))\n",
    "HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN', '')\n",
    "\n",
    "# Generation parameters\n",
    "DO_SAMPLE = os.getenv('DO_SAMPLE', 'False').lower() in ('true', '1', 't')\n",
    "TEMPERATURE = float(os.getenv('TEMPERATURE', '0.7'))\n",
    "\n",
    "# GPT-4 parameters\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')\n",
    "GPT4_MODEL = os.getenv('GPT4_MODEL', 'gpt-4')\n",
    "GPT4_MAX_TOKENS = int(os.getenv('GPT4_MAX_TOKENS', '150'))\n",
    "GPT4_TEMPERATURE = float(os.getenv('GPT4_TEMPERATURE', '0.3'))\n",
    "DRY_RUN = os.getenv('DRY_RUN', 'True').lower() in ('true', '1', 't')\n",
    "\n",
    "# Student Draft Generation\n",
    "STUDENT_MAX_TOKENS = int(os.getenv('STUDENT_MAX_TOKENS', '200'))\n",
    "STUDENT_TEMPERATURE = float(os.getenv('STUDENT_TEMPERATURE', '0.7'))\n",
    "STUDENT_BATCH_SIZE = int(os.getenv('STUDENT_BATCH_SIZE', '8'))\n",
    "\n",
    "# Enhanced Evaluation Generation\n",
    "EVAL_MAX_TOKENS = int(os.getenv('EVAL_MAX_TOKENS', '256'))\n",
    "EVAL_TEMPERATURE = float(os.getenv('EVAL_TEMPERATURE', '0.7'))\n",
    "EVAL_BATCH_SIZE = int(os.getenv('EVAL_BATCH_SIZE', '4'))\n",
    "\n",
    "# Quick Evaluation\n",
    "QUICK_EVAL_MAX_TOKENS = int(os.getenv('QUICK_EVAL_MAX_TOKENS', '5'))\n",
    "\n",
    "# Training Configuration\n",
    "# Phase A Training\n",
    "PHASE_A_EPOCHS = int(os.getenv('PHASE_A_EPOCHS', '3'))\n",
    "PHASE_A_BATCH_SIZE = int(os.getenv('PHASE_A_BATCH_SIZE', '1'))\n",
    "PHASE_A_LEARNING_RATE = float(os.getenv('PHASE_A_LEARNING_RATE', '1e-4'))\n",
    "PHASE_A_WARMUP_RATIO = float(os.getenv('PHASE_A_WARMUP_RATIO', '0.1'))\n",
    "PHASE_A_WEIGHT_DECAY = float(os.getenv('PHASE_A_WEIGHT_DECAY', '0.01'))\n",
    "\n",
    "# Phase B Training\n",
    "PHASE_B_EPOCHS = int(os.getenv('PHASE_B_EPOCHS', '3'))\n",
    "PHASE_B_BATCH_SIZE = int(os.getenv('PHASE_B_BATCH_SIZE', '4'))\n",
    "PHASE_B_LEARNING_RATE = float(os.getenv('PHASE_B_LEARNING_RATE', '5e-5'))\n",
    "PHASE_B_WARMUP_RATIO = float(os.getenv('PHASE_B_WARMUP_RATIO', '0.1'))\n",
    "PHASE_B_WEIGHT_DECAY = float(os.getenv('PHASE_B_WEIGHT_DECAY', '0.01'))\n",
    "\n",
    "# Progressive Curriculum Training\n",
    "CURRICULUM_STAGE1_EPOCHS = int(os.getenv('CURRICULUM_STAGE1_EPOCHS', '1'))\n",
    "CURRICULUM_STAGE1_LEARNING_RATE = float(os.getenv('CURRICULUM_STAGE1_LEARNING_RATE', '5e-5'))\n",
    "CURRICULUM_STAGE1_WARMUP_RATIO = float(os.getenv('CURRICULUM_STAGE1_WARMUP_RATIO', '0.1'))\n",
    "CURRICULUM_STAGE1_WEIGHT_DECAY = float(os.getenv('CURRICULUM_STAGE1_WEIGHT_DECAY', '0.01'))\n",
    "\n",
    "CURRICULUM_STAGE2_EPOCHS = int(os.getenv('CURRICULUM_STAGE2_EPOCHS', '2'))\n",
    "CURRICULUM_STAGE2_LEARNING_RATE = float(os.getenv('CURRICULUM_STAGE2_LEARNING_RATE', '3e-5'))\n",
    "CURRICULUM_STAGE2_WARMUP_RATIO = float(os.getenv('CURRICULUM_STAGE2_WARMUP_RATIO', '0.1'))\n",
    "CURRICULUM_STAGE2_WEIGHT_DECAY = float(os.getenv('CURRICULUM_STAGE2_WEIGHT_DECAY', '0.01'))\n",
    "\n",
    "# Validation Configuration\n",
    "HIGH_CONFIDENCE_THRESHOLD = float(os.getenv('HIGH_CONFIDENCE_THRESHOLD', '0.8'))\n",
    "MEDIUM_CONFIDENCE_THRESHOLD = float(os.getenv('MEDIUM_CONFIDENCE_THRESHOLD', '0.5'))\n",
    "LOW_CONFIDENCE_THRESHOLD = float(os.getenv('LOW_CONFIDENCE_THRESHOLD', '0.3'))\n",
    "VALIDATION_ACCEPTANCE_THRESHOLD = float(os.getenv('VALIDATION_ACCEPTANCE_THRESHOLD', '0.3'))\n",
    "\n",
    "# Quality Thresholds\n",
    "VALID_QUALITY_THRESHOLD = float(os.getenv('VALID_QUALITY_THRESHOLD', '0.5'))\n",
    "CORRECTED_QUALITY_THRESHOLD = float(os.getenv('CORRECTED_QUALITY_THRESHOLD', '0.3'))\n",
    "\n",
    "# Token Emphasis Configuration\n",
    "EMPHASIS_MULTIPLIER = float(os.getenv('EMPHASIS_MULTIPLIER', '2.5'))\n",
    "ADAPTIVE_EMPHASIS = os.getenv('ADAPTIVE_EMPHASIS', 'True').lower() in ('true', '1', 't')\n",
    "\n",
    "# Memory and Performance\n",
    "GRADIENT_CHECKPOINTING = os.getenv('GRADIENT_CHECKPOINTING', 'True').lower() in ('true', '1', 't')\n",
    "FP16 = os.getenv('FP16', 'False').lower() in ('true', '1', 't')\n",
    "BF16 = os.getenv('BF16', 'True').lower() in ('true', '1', 't')\n",
    "DATALOADER_NUM_WORKERS = int(os.getenv('DATALOADER_NUM_WORKERS', '0'))\n",
    "DATALOADER_PERSISTENT_WORKERS = os.getenv('DATALOADER_PERSISTENT_WORKERS', 'False').lower() in ('true', '1', 't')\n",
    "SKIP_MEMORY_METRICS = os.getenv('SKIP_MEMORY_METRICS', 'True').lower() in ('true', '1', 't')\n",
    "\n",
    "# Logging and Monitoring\n",
    "LOGGING_STEPS = int(os.getenv('LOGGING_STEPS', '10'))\n",
    "SAVE_STRATEGY = os.getenv('SAVE_STRATEGY', 'epoch')\n",
    "REPORT_TO = os.getenv('REPORT_TO', 'none')\n",
    "LOAD_BEST_MODEL_AT_END = os.getenv('LOAD_BEST_MODEL_AT_END', 'False').lower() in ('true', '1', 't')\n",
    "\n",
    "# Evaluation Configuration\n",
    "EVAL_SIZE = int(os.getenv('EVAL_SIZE', '100'))\n",
    "ENHANCED_EVAL_BATCH_SIZE = int(os.getenv('ENHANCED_EVAL_BATCH_SIZE', '4'))\n",
    "# File paths\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(parent_dir, os.getenv(\"DATA_DIR\", \"data\"))\n",
    "RAW_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "SAMPLE_TRAIN_PATH = os.path.join(DATA_DIR, '/train/sample_train.jsonl')\n",
    "STUDENT_DRAFTS_PATH = os.path.join(DATA_DIR, '/student/student_drafts.jsonl')\n",
    "CLEANED_STUDENT_DRAFTS_PATH = os.path.join(DATA_DIR, '/student/cleaned_student_drafts.jsonl')\n",
    "TEACHER_OUTPUTS_PATH = os.path.join(DATA_DIR, '/teacher/teacher_outputs.jsonl')\n",
    "BASELINE_PATH = os.path.join(DATA_DIR, '/train/train_baseline.jsonl')\n",
    "COT_PATH = os.path.join(DATA_DIR, '/train/train_cot.jsonl')\n",
    "\n",
    "# Print configuration\n",
    "print(\"=== Configuration ===\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"4-bit quantization: {USE_4BIT}\")\n",
    "print(f\"GPT-4 dry run: {DRY_RUN}\")\n",
    "print(\"==\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa20cd01",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f38bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files in:\n",
      "- Train: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\\strategyqa_train.jsonl\n",
      "- Val: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\\strategyqa_validation.jsonl\n",
      "- Test: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\\strategyqa_test.jsonl\n",
      "Files exist: True\n",
      "Loading dataset from local files...\n",
      "Sample size: 200 examples\n",
      "Full training set saved to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\\strategyqa_train.jsonl\n",
      "Validation set saved to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\\strategyqa_validation.jsonl\n",
      "Sampled train set (≈200 entries) saved to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\sample_train.jsonl\n",
      "Combined train+val set (1603 entries) saved to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\full_train_val.jsonl\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "def save_jsonl(data, filepath):\n",
    "    \"\"\"Save data to a JSONL file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "def load_jsonl(filepath):\n",
    "    \"\"\"Load data from a JSONL file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "# Check if files exist\n",
    "train_path = os.path.join(RAW_DIR, 'strategyqa_train.jsonl')\n",
    "val_path = os.path.join(RAW_DIR, 'strategyqa_validation.jsonl')\n",
    "test_path = os.path.join(RAW_DIR, 'strategyqa_test.jsonl')\n",
    "\n",
    "print(\"Looking for files in:\")\n",
    "print(f\"- Train: {train_path}\")\n",
    "print(f\"- Val: {val_path}\")\n",
    "print(f\"- Test: {test_path}\")\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "files_exist = all(os.path.exists(p) for p in [train_path, val_path, test_path])\n",
    "print(f\"Files exist: {files_exist}\")\n",
    "\n",
    "if files_exist:\n",
    "    print(\"Loading dataset from local files...\")\n",
    "    train_data = load_jsonl(train_path)\n",
    "    val_data = load_jsonl(val_path)\n",
    "    test_data = load_jsonl(test_path)\n",
    "else:\n",
    "    print(\"Downloading and saving dataset...\")\n",
    "    # Load the dataset from HuggingFace\n",
    "    dataset = load_dataset(DATASET_NAME)\n",
    "\n",
    "    train_data = list(dataset['train'])\n",
    "    val_data = list(dataset['validation'])\n",
    "    test_data = list(dataset['test'])\n",
    "\n",
    "    # Save to files for future use\n",
    "    save_jsonl(train_data, train_path)\n",
    "    save_jsonl(val_data, val_path)\n",
    "    save_jsonl(test_data, test_path)\n",
    "\n",
    "    print(f\"Train: {len(train_data)} examples\")\n",
    "    print(f\"Val: {len(val_data)} examples\")\n",
    "    print(f\"Test: {len(test_data)} examples\")\n",
    "\n",
    "# Create sample training data\n",
    "sample_train_path = os.path.join(DATA_DIR, '/train/sample_train.jsonl')\n",
    "\n",
    "if not USE_FULL_DATASET:\n",
    "    # Create a smaller sample for faster development\n",
    "    import random\n",
    "    random.seed(RANDOM_SEED)\n",
    "    target_train_sampled = random.sample(train_data, min(TRAIN_SAMPLES, len(train_data)))\n",
    "    print(f\"Sample size: {len(target_train_sampled)} examples\")\n",
    "else:\n",
    "    # Use all training data\n",
    "    target_train_sampled = train_data\n",
    "    print(f\"Using full training set: {len(target_train_sampled)} examples\")\n",
    "\n",
    "# Also create combined train+validation for Q&A-CoT (more data)\n",
    "full_train_val = train_data + val_data\n",
    "full_train_val_path = os.path.join(DATA_DIR, '/train/full_train_val.jsonl')\n",
    "\n",
    "# Save both sampled and full datasets\n",
    "save_jsonl(target_train_sampled, sample_train_path)\n",
    "save_jsonl(full_train_val, full_train_val_path)\n",
    "\n",
    "print(f\"Full training set saved to {train_path}\")\n",
    "print(f\"Validation set saved to {val_path}\")\n",
    "print(f\"Sampled train set (≈{TRAIN_SAMPLES} entries) saved to {sample_train_path}\")\n",
    "print(f\"Combined train+val set ({len(full_train_val)} entries) saved to {full_train_val_path}\")\n",
    "\n",
    "# Update file paths based on choice\n",
    "if USE_FULL_DATASET:\n",
    "    # Update the global path variables to point to the full dataset\n",
    "    SAMPLE_TRAIN_PATH = os.path.join(DATA_DIR, 'full_train_val.jsonl')\n",
    "    print(f\"📊 Updated SAMPLE_TRAIN_PATH to use full dataset: {SAMPLE_TRAIN_PATH}\")\n",
    "    # Update file names to avoid confusion\n",
    "    STUDENT_DRAFTS_PATH = os.path.join(DATA_DIR, '/student/student_drafts_full.jsonl')\n",
    "    CLEANED_STUDENT_DRAFTS_PATH = os.path.join(DATA_DIR, '/student/cleaned_student_drafts_full.jsonl')\n",
    "    TEACHER_OUTPUTS_PATH = os.path.join(DATA_DIR, 'teacher_outputs_full.jsonl')\n",
    "    BASELINE_PATH = os.path.join(DATA_DIR, 'train_baseline_full.jsonl')\n",
    "    COT_PATH = os.path.join(DATA_DIR, 'train_cot_full.jsonl')\n",
    "    print(f\"📊 Updated output paths to use '_full' suffix for clarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd52f1d",
   "metadata": {},
   "source": [
    "## Generate Student Drafts\n",
    "\n",
    "In this section we load a base language model (e.g. `meta-llama/Llama-2-7b-hf` or `gpt2`) and generate a short *student draft* for each question in the sampled training set.  A draft consists of a yes/no answer followed by one or two clarifying questions, as specified in the data‑generation loop.  Adjust the model name based on your available hardware and licences.\n",
    "\n",
    "> **Tip:** On Colab, you can enable a GPU via *Runtime → Change runtime type → GPU* and use half‑precision weights to reduce memory usage.  For demonstration, we use `gpt2` (which is small) to keep the example runnable on CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c00340ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model: microsoft/Phi-3.5-mini-instruct\n",
      "Loading model in 4-bit quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory after model load: 2.26 GB\n",
      "Loading dataset from c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\sample_train.jsonl with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 200 examples [00:00, 6861.96 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 1408.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def setup_dataset(input_path: str, tokenizer, batch_size: int = BATCH_SIZE):\n",
    "    \"\"\"Load and prepare dataset for GPU processing.\"\"\"\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset('json', data_files=input_path, split='train')\n",
    "\n",
    "    # Keep the original questions for reference\n",
    "    original_questions = dataset['question']\n",
    "\n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['question'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            return_tensors=None  # Return as list, not tensors\n",
    "        )\n",
    "\n",
    "    # Apply tokenization\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Create a custom dataset that includes both tokenized data and original questions\n",
    "    class QADataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, tokenized_data, original_questions):\n",
    "            self.tokenized_data = tokenized_data\n",
    "            self.original_questions = original_questions\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.tokenized_data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {\n",
    "                'input_ids': torch.tensor(self.tokenized_data[idx]['input_ids']),\n",
    "                'attention_mask': torch.tensor(self.tokenized_data[idx]['attention_mask']),\n",
    "                'question': self.original_questions[idx]\n",
    "            }\n",
    "            return item\n",
    "\n",
    "    # Create custom dataset\n",
    "    custom_dataset = QADataset(tokenized_dataset, original_questions)\n",
    "\n",
    "    # Create DataLoader\n",
    "    loader = DataLoader(\n",
    "        custom_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False  # Keep order for output matching\n",
    "    )\n",
    "\n",
    "    return loader\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model setup\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Use 4-bit quantization if enabled and on GPU\n",
    "if device.type == 'cuda' and USE_4BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    print(\"Loading model in 4-bit quantization...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading model in standard precision...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# Ensure the tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Memory after model load: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# Load and prepare dataset (fix path)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sample_train_path_full = os.path.join(parent_dir, SAMPLE_TRAIN_PATH)\n",
    "print(f\"Loading dataset from {sample_train_path_full} with batch size {BATCH_SIZE}\")\n",
    "train_loader = setup_dataset(sample_train_path_full, tokenizer, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01902644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages_self_questioning(q: str):\n",
    "    \"\"\"\n",
    "    SIMPLIFIED Self-Questioning Format - Focus on working generation first\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a reasoning assistant that answers yes/no questions with brief reasoning.\\n\\n\"\n",
    "                   \"RESPONSE FORMAT:\\n\"\n",
    "                   \"Question 1: [Ask a relevant question about the topic]\\n\"\n",
    "                   \"Answer 1: [Answer with facts]\\n\"\n",
    "                   \"Therefore: [Brief conclusion]\\n\"\n",
    "                   \"Final answer: Yes (or No)\\n\\n\"\n",
    "                   \"IMPORTANT: Always end with 'Final answer: Yes' or 'Final answer: No'. Never write 'Uncertain' or 'Maybe'.\"},\n",
    "\n",
    "        {\"role\": \"user\", \"content\": \"Did Aristotle use a laptop?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Question 1: Did laptops exist in Aristotle's time?\\n\"\n",
    "                                        \"Answer 1: No, laptops were invented much later.\\n\"\n",
    "                                        \"Therefore: Aristotle could not have used something that didn't exist.\\n\"\n",
    "                                        \"Final answer: No\"},\n",
    "\n",
    "        {\"role\": \"user\", \"content\": q},\n",
    "    ]\n",
    "\n",
    "def generate_batch_drafts_simplified(batch):\n",
    "    \"\"\"Generate drafts with SIMPLIFIED approach - no complex constraints\"\"\"\n",
    "    import torch\n",
    "    import gc\n",
    "\n",
    "    # Set padding side to left for generation\n",
    "    tokenizer.padding_side = 'left'\n",
    "\n",
    "    # Create simple prompts\n",
    "    prompts = [\n",
    "        tokenizer.apply_chat_template(build_messages_self_questioning(q),\n",
    "                                    tokenize=False,\n",
    "                                    add_generation_prompt=True)\n",
    "        for q in batch[\"question\"]\n",
    "    ]\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH\n",
    "    ).to(device)\n",
    "\n",
    "    # SIMPLIFIED generation - no complex constraints\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=STUDENT_MAX_TOKENS,\n",
    "            do_sample=True,\n",
    "            temperature=STUDENT_TEMPERATURE,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    # Decode outputs - handle both tensor and dict outputs\n",
    "    prompt_lens = inputs[\"attention_mask\"].sum(dim=1)\n",
    "    if hasattr(outputs, 'sequences'):\n",
    "        decoded = [tokenizer.decode(seq[p_len:], skip_special_tokens=True).strip()\n",
    "                   for seq, p_len in zip(outputs.sequences, prompt_lens)]\n",
    "    else:\n",
    "        decoded = [tokenizer.decode(seq[p_len:], skip_special_tokens=True).strip()\n",
    "                   for seq, p_len in zip(outputs, prompt_lens)]\n",
    "    \n",
    "    # Simple post-processing - just ensure we have some content\n",
    "    drafts = []\n",
    "    for response in decoded:\n",
    "        if len(response.strip()) < 10:  # Too short\n",
    "            # Simple fallback\n",
    "            drafts.append(\"Question 1: What factors are relevant?\\nAnswer 1: Need to consider the key aspects.\\nTherefore: Based on available information.\\nFinal answer: No\")\n",
    "        else:\n",
    "            drafts.append(response)\n",
    "\n",
    "    # Memory cleanup\n",
    "    del inputs, outputs\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return drafts\n",
    "\n",
    "# Use simplified version\n",
    "generate_batch_drafts = generate_batch_drafts_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6810fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing student drafts to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\student_drafts.jsonl\n",
      "Processing 25 batches (starting from batch 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:   0%|          | 0/25 [00:00<?, ?it/s]c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\.venv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:457: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "Generating drafts:   4%|▍         | 1/25 [01:03<25:19, 63.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:   8%|▊         | 2/25 [01:28<15:46, 41.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  12%|█▏        | 3/25 [02:31<18:44, 51.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  16%|█▌        | 4/25 [04:19<25:37, 73.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  20%|██        | 5/25 [06:06<28:32, 85.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  24%|██▍       | 6/25 [06:42<21:42, 68.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  28%|██▊       | 7/25 [07:08<16:27, 54.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  32%|███▏      | 8/25 [09:24<22:49, 80.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  36%|███▌      | 9/25 [12:05<28:14, 105.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  40%|████      | 10/25 [14:46<30:40, 122.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/25 completed. GPU Memory: 2.27 GB\n",
      "✅ Memory cleanup completed after batch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  44%|████▍     | 11/25 [17:26<31:17, 134.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  48%|████▊     | 12/25 [18:11<23:10, 107.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  52%|█████▏    | 13/25 [19:38<20:12, 101.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  56%|█████▌    | 14/25 [22:18<21:47, 118.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 14/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  60%|██████    | 15/25 [23:17<16:47, 100.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  64%|██████▍   | 16/25 [25:57<17:48, 118.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 16/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  68%|██████▊   | 17/25 [28:37<17:29, 131.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 17/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  72%|███████▏  | 18/25 [30:47<15:14, 130.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  76%|███████▌  | 19/25 [31:37<10:38, 106.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 19/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  80%|████████  | 20/25 [32:43<07:51, 94.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20/25 completed. GPU Memory: 2.27 GB\n",
      "✅ Memory cleanup completed after batch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  84%|████████▍ | 21/25 [35:25<07:38, 114.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 21/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  88%|████████▊ | 22/25 [38:08<06:27, 129.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 22/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  92%|█████████▏| 23/25 [40:10<04:14, 127.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 23/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  96%|█████████▌| 24/25 [40:36<01:36, 96.69s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 24/25 completed. GPU Memory: 2.27 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts: 100%|██████████| 25/25 [41:05<00:00, 98.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 25/25 completed. GPU Memory: 2.27 GB\n",
      "\n",
      "📝 Copying 200 results to final output file...\n",
      "✅ Student drafts written to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\student_drafts.jsonl\n",
      "📊 Total examples processed: 200\n",
      "🔄 Checkpoint file: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\student_drafts.jsonl.checkpoint\n",
      "Using optimized student prompts with error recovery and checkpointing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bars\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Create output directory if it doesn't exist (fix path)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "student_drafts_path_full = os.path.join(parent_dir, STUDENT_DRAFTS_PATH)\n",
    "os.makedirs(os.path.dirname(student_drafts_path_full), exist_ok=True)\n",
    "\n",
    "# Check if we already have partial results (for resuming)\n",
    "checkpoint_file = student_drafts_path_full + '.checkpoint'\n",
    "processed_count = 0\n",
    "existing_results = []\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    print(f\"Found checkpoint file, loading existing results...\")\n",
    "    with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
    "        existing_results = [json.loads(line) for line in f]\n",
    "    processed_count = len(existing_results)\n",
    "    print(f\"Resuming from {processed_count} already processed examples\")\n",
    "\n",
    "# Process batches and write outputs with error handling\n",
    "print(f\"Writing student drafts to {student_drafts_path_full}\")\n",
    "print(f\"Processing {len(train_loader)} batches (starting from batch {processed_count // BATCH_SIZE})\")\n",
    "\n",
    "batch_start_idx = processed_count // BATCH_SIZE\n",
    "all_results = existing_results.copy()\n",
    "\n",
    "with open(checkpoint_file, 'a' if existing_results else 'w', encoding='utf-8') as checkpoint_f:\n",
    "    try:\n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc='Generating drafts', initial=batch_start_idx)):\n",
    "            # Skip already processed batches\n",
    "            if batch_idx < batch_start_idx:\n",
    "                continue\n",
    "\n",
    "            # Get original questions directly from the batch\n",
    "            questions = batch['question']\n",
    "\n",
    "            # Move input_ids and attention_mask to device\n",
    "            input_batch = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                # Generate drafts for the batch with error handling\n",
    "                drafts = generate_batch_drafts({'question': questions})\n",
    "\n",
    "                # Write results to checkpoint file immediately\n",
    "                batch_results = []\n",
    "                for q, draft in zip(questions, drafts):\n",
    "                    out_rec = {\n",
    "                        'question': q,\n",
    "                        'student_draft': draft\n",
    "                    }\n",
    "                    batch_results.append(out_rec)\n",
    "                    all_results.append(out_rec)\n",
    "                    checkpoint_f.write(json.dumps(out_rec, ensure_ascii=False) + '\\n')\n",
    "\n",
    "                # Flush to disk after each batch\n",
    "                checkpoint_f.flush()\n",
    "\n",
    "                # Print progress\n",
    "                if device.type == 'cuda':\n",
    "                    memory_gb = torch.cuda.memory_allocated()/1e9\n",
    "                    print(f\"Batch {batch_idx+1}/{len(train_loader)} completed. GPU Memory: {memory_gb:.2f} GB\")\n",
    "\n",
    "                # Periodic cleanup every 10 batches\n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    print(f\"✅ Memory cleanup completed after batch {batch_idx+1}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"❌ Error in batch {batch_idx}: {e}\")\n",
    "                print(f\"🔄 Attempting to continue with next batch...\")\n",
    "\n",
    "                # Clean up memory after error\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                # Save partial results for failed batch\n",
    "                for q in questions:\n",
    "                    failed_rec = {\n",
    "                        'question': q,\n",
    "                        'student_draft': 'Answer: Error during generation\\nQuestions: What went wrong? How to fix this?'\n",
    "                    }\n",
    "                    all_results.append(failed_rec)\n",
    "                    checkpoint_f.write(json.dumps(failed_rec, ensure_ascii=False) + '\\n')\n",
    "\n",
    "                checkpoint_f.flush()\n",
    "                continue\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n🛑 Generation interrupted by user. Progress saved to checkpoint.\")\n",
    "        print(f\"📊 Processed {len(all_results)} examples so far\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Unexpected error: {e}\")\n",
    "        print(f\"📊 Progress saved. Processed {len(all_results)} examples\")\n",
    "\n",
    "# Copy final results to main output file\n",
    "print(f\"\\n📝 Copying {len(all_results)} results to final output file...\")\n",
    "with open(student_drafts_path_full, 'w', encoding='utf-8') as out_f:\n",
    "    for result in all_results:\n",
    "        out_f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"✅ Student drafts written to {student_drafts_path_full}\")\n",
    "print(f\"📊 Total examples processed: {len(all_results)}\")\n",
    "print(f\"🔄 Checkpoint file: {checkpoint_file}\")\n",
    "print(\"Using optimized student prompts with error recovery and checkpointing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82074b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from abc import ABC, abstractmethod\n",
    "import logging\n",
    "\n",
    "class ValidationStatus(Enum):\n",
    "    VALID = \"valid\"\n",
    "    INVALID = \"invalid\"\n",
    "    CORRECTED = \"corrected\"\n",
    "    FAILED = \"failed\"\n",
    "\n",
    "@dataclass\n",
    "class ValidationResult:\n",
    "    status: ValidationStatus\n",
    "    original_text: str\n",
    "    cleaned_text: Optional[str]\n",
    "    confidence_score: float  # 0.0 - 1.0\n",
    "    error_messages: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "    def is_valid(self) -> bool:\n",
    "        return self.status in [ValidationStatus.VALID, ValidationStatus.CORRECTED]\n",
    "\n",
    "@dataclass\n",
    "class StudentResponse:\n",
    "    answer: str  # Yes/No/Uncertain\n",
    "    reasoning: str\n",
    "    questions: List[str]\n",
    "    confidence_score: float\n",
    "    validation_errors: List[str]\n",
    "    quality_metrics: Dict[str, float]\n",
    "\n",
    "class BaseResponseValidator(ABC):\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    @abstractmethod\n",
    "    def validate(self, response_text: str) -> ValidationResult:\n",
    "        pass\n",
    "\n",
    "    def _calculate_base_confidence(self, parsing_success: bool, content_quality: float) -> float:\n",
    "        \"\"\"Calculate base confidence score from parsing and content quality.\"\"\"\n",
    "        parsing_score = 1.0 if parsing_success else 0.3\n",
    "        return (parsing_score * 0.6) + (content_quality * 0.4)\n",
    "\n",
    "class StudentDraftValidator(BaseResponseValidator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.answer_patterns = [\n",
    "            r'Final answer:\\s*(Yes|No)',  # New primary pattern\n",
    "            r'(?:Answer|The answer is)\\s*:?\\s*(Yes|No|Uncertain)\\b',\n",
    "            r'\\*\\*Answer\\*\\*\\s*:?\\s*(Yes|No|Uncertain)\\b',\n",
    "            r'\\b(Yes|No)\\b(?=\\s*[.!]?\\s*$)',  # Final yes/no\n",
    "        ]\n",
    "        self.question_patterns = [\n",
    "            r'Questions?\\s*:?\\s*(.+?)(?=\\n|$)',\n",
    "            r'Question\\s+\\d+\\s*:?\\s*(.+?)(?=\\n|Answer|$)',\n",
    "            r'[^.!?]*\\?',  # Any sentence ending with ?\n",
    "        ]\n",
    "\n",
    "    def validate(self, response_text: str) -> ValidationResult:\n",
    "        \"\"\"Validate student draft with Q&A format support.\"\"\"\n",
    "        errors = []\n",
    "        metadata = {}\n",
    "\n",
    "        # Layer 1: Structural Parsing\n",
    "        parsed_data = self._parse_structure(response_text)\n",
    "        parsing_success = parsed_data is not None\n",
    "\n",
    "        if not parsing_success:\n",
    "            errors.append(\"Failed to parse basic structure\")\n",
    "            # Attempt correction\n",
    "            corrected = self._attempt_correction(response_text)\n",
    "            if corrected:\n",
    "                parsed_data = self._parse_structure(corrected)\n",
    "                if parsed_data:\n",
    "                    parsing_success = True\n",
    "                    errors.append(\"Structure corrected automatically\")\n",
    "\n",
    "        # Layer 2: Content Validation\n",
    "        if parsed_data:\n",
    "            quality_metrics = self._assess_content_quality(parsed_data, response_text)\n",
    "            metadata['quality_metrics'] = quality_metrics\n",
    "\n",
    "            # Check Q&A format support\n",
    "            qa_format = self._detect_qa_format(response_text)\n",
    "            metadata['qa_format_detected'] = qa_format\n",
    "\n",
    "            if qa_format:\n",
    "                qa_quality = self._assess_qa_quality(response_text)\n",
    "                metadata['qa_quality'] = qa_quality\n",
    "                quality_metrics['overall'] = (quality_metrics['overall'] + qa_quality) / 2\n",
    "        else:\n",
    "            quality_metrics = {'overall': 0.0, 'answer_quality': 0.0, 'reasoning_quality': 0.0}\n",
    "\n",
    "        # Layer 4: Confidence Scoring\n",
    "        confidence_score = self._calculate_confidence(\n",
    "            parsing_success, quality_metrics, len(errors)\n",
    "        )\n",
    "\n",
    "        # Determine final status - require valid Yes/No answer\n",
    "        answer = parsed_data.get('answer') if parsed_data else None\n",
    "        has_valid_answer = answer and answer in ['Yes', 'No']\n",
    "        \n",
    "        if parsing_success and quality_metrics['overall'] >= 0.7 and has_valid_answer:\n",
    "            status = ValidationStatus.VALID\n",
    "            cleaned_text = self._format_output(parsed_data) if parsed_data else response_text\n",
    "        elif parsing_success and quality_metrics['overall'] >= 0.5 and has_valid_answer:\n",
    "            status = ValidationStatus.CORRECTED\n",
    "            cleaned_text = self._format_output(parsed_data) if parsed_data else response_text\n",
    "        else:\n",
    "            status = ValidationStatus.INVALID\n",
    "            cleaned_text = None\n",
    "\n",
    "        return ValidationResult(\n",
    "            status=status,\n",
    "            original_text=response_text,\n",
    "            cleaned_text=cleaned_text,\n",
    "            confidence_score=confidence_score,\n",
    "            error_messages=errors,\n",
    "            metadata=metadata\n",
    "        )\n",
    "\n",
    "    def _extract_last_final_answer(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract the LAST occurrence of final answer patterns (prefer last answer).\"\"\"\n",
    "        # Look for all patterns and find the last one\n",
    "        all_matches = []\n",
    "        \n",
    "        # Pattern 1: Final answer: Yes/No (preferred)\n",
    "        for match in re.finditer(r'Final answer:\\s*(Yes|No)', text, re.IGNORECASE):\n",
    "            all_matches.append((match.end(), match.group(1).capitalize()))\n",
    "        \n",
    "        # Pattern 2: The answer is **Yes/No**\n",
    "        for match in re.finditer(r'The answer is\\s*\\*\\*\\s*(Yes|No)\\s*\\*\\*', text, re.IGNORECASE):\n",
    "            all_matches.append((match.end(), match.group(1).capitalize()))\n",
    "        \n",
    "        # Pattern 3: **Answer**: Yes/No\n",
    "        for match in re.finditer(r'\\*\\*Answer\\*\\*\\s*:?\\s*(Yes|No)', text, re.IGNORECASE):\n",
    "            all_matches.append((match.end(), match.group(1).capitalize()))\n",
    "        \n",
    "        # Return the last match (highest position)\n",
    "        if all_matches:\n",
    "            # Sort by position and return the last answer\n",
    "            all_matches.sort(key=lambda x: x[0])\n",
    "            return all_matches[-1][1]\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _parse_structure(self, text: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Parse response into structured components.\"\"\"\n",
    "        # Try Q&A format first (new format)\n",
    "        qa_match = self._parse_qa_format(text)\n",
    "        if qa_match:\n",
    "            return qa_match\n",
    "\n",
    "        # Fallback to traditional Answer/Questions format\n",
    "        answer = None\n",
    "        questions = []\n",
    "\n",
    "        # Extract answer using last answer preference\n",
    "        answer = self._extract_last_final_answer(text)\n",
    "        \n",
    "        # If no final answer found, try other patterns\n",
    "        if not answer:\n",
    "            for pattern in self.answer_patterns[1:]:  # Skip first pattern (already tried)\n",
    "                matches = list(re.finditer(pattern, text, re.IGNORECASE | re.MULTILINE))\n",
    "                if matches:\n",
    "                    # Get the last match\n",
    "                    answer = matches[-1].group(1).strip().capitalize()\n",
    "                    break\n",
    "\n",
    "        # Extract questions\n",
    "        for pattern in self.question_patterns:\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
    "            if matches:\n",
    "                questions.extend([q.strip() for q in matches if q.strip()])\n",
    "                break\n",
    "\n",
    "        if answer or questions:\n",
    "            return {\n",
    "                'answer': answer,\n",
    "                'questions': questions,\n",
    "                'reasoning': text,  # Full text as reasoning\n",
    "                'format_type': 'traditional'\n",
    "            }\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _parse_qa_format(self, text: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Parse new Q&A interleaved format.\"\"\"\n",
    "        # Look for Question N: ... Answer N: ... pattern\n",
    "        qa_pattern = r'Question\\s+(\\d+)\\s*:?\\s*(.+?)(?=Answer\\s+\\1|$).*?Answer\\s+\\1\\s*:?\\s*(.+?)(?=Question\\s+\\d+|Therefore|\\*\\*Answer\\*\\*|Final answer|$)'\n",
    "        matches = re.findall(qa_pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "        if matches:\n",
    "            questions = []\n",
    "            reasoning_parts = []\n",
    "\n",
    "            for num, question, answer in matches:\n",
    "                questions.append(question.strip())\n",
    "                reasoning_parts.append(f\"Q{num}: {question.strip()}\\nA{num}: {answer.strip()}\")\n",
    "\n",
    "            # Extract final answer using last answer preference\n",
    "            final_answer = self._extract_last_final_answer(text)\n",
    "            \n",
    "            # Fallback to old patterns if needed\n",
    "            if not final_answer:\n",
    "                final_answer_match = re.search(r'(?:Therefore|The answer is|\\*\\*Answer\\*\\*)\\s*:?\\s*(Yes|No|Uncertain)', text, re.IGNORECASE)\n",
    "                final_answer = final_answer_match.group(1).capitalize() if final_answer_match else None\n",
    "\n",
    "            return {\n",
    "                'answer': final_answer,\n",
    "                'questions': questions,\n",
    "                'reasoning': '\\n\\n'.join(reasoning_parts),\n",
    "                'format_type': 'qa_interleaved',\n",
    "                'qa_pairs': len(matches)\n",
    "            }\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _detect_qa_format(self, text: str) -> bool:\n",
    "        \"\"\"Detect if text uses Q&A interleaved format.\"\"\"\n",
    "        qa_pattern = r'(?:Question\\s+\\d+\\s*:.*?Answer\\s+\\d+\\s*:|Answer\\s*:.*?Questions?\\s*:)'\n",
    "        return bool(re.search(qa_pattern, text, re.DOTALL | re.IGNORECASE))\n",
    "\n",
    "    def _assess_content_quality(self, parsed_data: Dict[str, Any], full_text: str) -> Dict[str, float]:\n",
    "        \"\"\"Assess quality of parsed content.\"\"\"\n",
    "        # Answer quality\n",
    "        answer = parsed_data.get('answer', '')\n",
    "        answer_quality = 1.0 if answer in ['Yes', 'No'] else 0.0  # Ban Uncertain answers\n",
    "\n",
    "        # Questions quality\n",
    "        questions = parsed_data.get('questions', [])\n",
    "        if questions:\n",
    "            # Quality based on question count, length, and question marks\n",
    "            question_count_score = min(len(questions) / 3, 1.0)  # Optimal: 2-3 questions\n",
    "            has_question_marks = sum(1 for q in questions if '?' in q) / len(questions)\n",
    "            avg_length = sum(len(q.split()) for q in questions) / len(questions)\n",
    "            length_score = min(avg_length / 8, 1.0)  # Optimal: ~8 words\n",
    "\n",
    "            questions_quality = (question_count_score + has_question_marks + length_score) / 3\n",
    "        else:\n",
    "            questions_quality = 0.0\n",
    "\n",
    "        # Reasoning quality (basic heuristics)\n",
    "        reasoning = parsed_data.get('reasoning', '')\n",
    "        reasoning_length = len(reasoning.split())\n",
    "        reasoning_quality = min(reasoning_length / 50, 1.0)  # Rough heuristic\n",
    "\n",
    "        # Overall quality\n",
    "        overall_quality = (answer_quality * 0.4 + questions_quality * 0.4 + reasoning_quality * 0.2)\n",
    "\n",
    "        return {\n",
    "            'overall': overall_quality,\n",
    "            'answer_quality': answer_quality,\n",
    "            'questions_quality': questions_quality,\n",
    "            'reasoning_quality': reasoning_quality\n",
    "        }\n",
    "\n",
    "    def _assess_qa_quality(self, text: str) -> float:\n",
    "        \"\"\"Assess quality specific to Q&A format.\"\"\"\n",
    "        qa_pairs = re.findall(r'Question\\s+(\\d+)\\s*:.*?Answer\\s+\\1\\s*:', text, re.DOTALL | re.IGNORECASE)\n",
    "        if not qa_pairs:\n",
    "            return 0.0\n",
    "\n",
    "        # Quality based on number of Q&A pairs and coherence\n",
    "        pair_count_score = min(len(qa_pairs) / 3, 1.0)  # Optimal: 2-3 pairs\n",
    "\n",
    "        # Check for logical progression (sequential numbering)\n",
    "        sequential_score = 1.0\n",
    "        for i, pair_num in enumerate(qa_pairs, 1):\n",
    "            if int(pair_num) != i:\n",
    "                sequential_score -= 0.2\n",
    "\n",
    "        return max(0.0, (pair_count_score + max(0.0, sequential_score)) / 2)\n",
    "\n",
    "    def _attempt_correction(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Attempt to correct common formatting issues.\"\"\"\n",
    "        # Common corrections\n",
    "        corrected = text\n",
    "\n",
    "        # Fix missing colons\n",
    "        corrected = re.sub(r'\\b(Answer|Questions?)\\b(?!:)', r'\\1:', corrected)\n",
    "\n",
    "        # Fix answer format\n",
    "        corrected = re.sub(r'\\b(yes|no)\\b(?!\\w)', lambda m: m.group(1).capitalize(), corrected, flags=re.IGNORECASE)\n",
    "\n",
    "        # Add missing questions if only answer present\n",
    "        if 'Answer:' in corrected and 'Question' not in corrected:\n",
    "            corrected += '\\nQuestions: What additional context would help clarify this?'\n",
    "\n",
    "        return corrected if corrected != text else None\n",
    "\n",
    "    def _calculate_confidence(self, parsing_success: bool, quality_metrics: Dict[str, float], error_count: int) -> float:\n",
    "        \"\"\"Calculate overall confidence score.\"\"\"\n",
    "        base_confidence = self._calculate_base_confidence(parsing_success, quality_metrics['overall'])\n",
    "\n",
    "        # Penalize for errors\n",
    "        error_penalty = min(error_count * 0.1, 0.3)\n",
    "\n",
    "        # Bonus for high-quality answers\n",
    "        quality_bonus = 0.1 if quality_metrics['answer_quality'] >= 0.9 else 0.0\n",
    "\n",
    "        return max(0.0, min(1.0, base_confidence - error_penalty + quality_bonus))\n",
    "\n",
    "    def _format_output(self, parsed_data: Dict[str, Any]) -> str:\n",
    "        \"\"\"Format parsed data into clean output.\"\"\"\n",
    "        answer = parsed_data.get('answer', 'No')  # Removed 'Uncertain' fallback - force decisive answer\n",
    "        questions = parsed_data.get('questions', [])\n",
    "\n",
    "        if parsed_data.get('format_type') == 'qa_interleaved':\n",
    "            # Preserve Q&A format with final answer\n",
    "            reasoning = parsed_data.get('reasoning', '')\n",
    "            return f\"{reasoning}\\n\\nFinal answer: {answer}\"\n",
    "        else:\n",
    "            # Traditional format\n",
    "            questions_text = '; '.join(questions) if questions else 'What additional information is needed?'\n",
    "            return f\"Answer: {answer}\\nQuestions: {questions_text}\"\n",
    "\n",
    "# Legacy function wrappers for backward compatibility\n",
    "def clean_student_draft(draft_text):\n",
    "    \"\"\"Legacy wrapper - now uses professional validation.\"\"\"\n",
    "    validator = StudentDraftValidator()\n",
    "    result = validator.validate(draft_text)\n",
    "\n",
    "    if result.is_valid():\n",
    "        return result.cleaned_text\n",
    "    else:\n",
    "        # Fallback to original extraction for compatibility\n",
    "        return extract_answer_and_questions(draft_text)\n",
    "\n",
    "def extract_answer_and_questions(text):\n",
    "    \"\"\"Enhanced extraction with better pattern matching.\"\"\"\n",
    "    validator = StudentDraftValidator()\n",
    "    parsed = validator._parse_structure(text)\n",
    "\n",
    "    if parsed:\n",
    "        return validator._format_output(parsed)\n",
    "\n",
    "    # Ultimate fallback - removed 'Uncertain', force 'No' \n",
    "    return \"Answer: No\\nQuestions: What additional information is needed?\"\n",
    "\n",
    "# Load and clean student drafts with enhanced validation\n",
    "student_drafts_path_full = os.path.join(parent_dir, STUDENT_DRAFTS_PATH)\n",
    "print(f\"Loading student drafts from {student_drafts_path_full}\")\n",
    "\n",
    "with open(student_drafts_path_full, 'r', encoding='utf-8') as f:\n",
    "    student_drafts = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Total student drafts: {len(student_drafts)}\")\n",
    "\n",
    "# Initialize professional validation\n",
    "validator = StudentDraftValidator()\n",
    "cleaned_drafts = []\n",
    "validation_statistics = {\n",
    "    'total': len(student_drafts),\n",
    "    'valid': 0,\n",
    "    'corrected': 0,\n",
    "    'invalid': 0,\n",
    "    'high_confidence': 0,  # >= 0.8\n",
    "    'medium_confidence': 0,  # 0.5 - 0.8\n",
    "    'low_confidence': 0,  # < 0.5\n",
    "    'qa_format_detected': 0,\n",
    "    'traditional_format': 0,\n",
    "    'quality_scores': []\n",
    "}\n",
    "\n",
    "print(\"\\n=== PROFESSIONAL STUDENT DRAFT VALIDATION ===\")\n",
    "print(\"Using multi-layered validation with Q&A format support...\")\n",
    "\n",
    "for i, draft in enumerate(student_drafts):\n",
    "    original_text = draft['student_draft']\n",
    "\n",
    "    # Apply professional validation\n",
    "    result = validator.validate(original_text)\n",
    "\n",
    "    # Update statistics\n",
    "    validation_statistics['quality_scores'].append(result.confidence_score)\n",
    "\n",
    "    if result.status == ValidationStatus.VALID:\n",
    "        validation_statistics['valid'] += 1\n",
    "    elif result.status == ValidationStatus.CORRECTED:\n",
    "        validation_statistics['corrected'] += 1\n",
    "    else:\n",
    "        validation_statistics['invalid'] += 1\n",
    "\n",
    "    # Confidence tiers\n",
    "    if result.confidence_score >= HIGH_CONFIDENCE_THRESHOLD:\n",
    "        validation_statistics['high_confidence'] += 1\n",
    "    elif result.confidence_score >= MEDIUM_CONFIDENCE_THRESHOLD:\n",
    "        validation_statistics['medium_confidence'] += 1\n",
    "    else:\n",
    "        validation_statistics['low_confidence'] += 1\n",
    "\n",
    "    # Format detection\n",
    "    if result.metadata.get('qa_format_detected', False):\n",
    "        validation_statistics['qa_format_detected'] += 1\n",
    "    else:\n",
    "        validation_statistics['traditional_format'] += 1\n",
    "\n",
    "    # Process based on confidence tier\n",
    "    if result.confidence_score >= VALIDATION_ACCEPTANCE_THRESHOLD:  # Accept high and medium confidence\n",
    "        cleaned_draft = {\n",
    "            'question': draft['question'],\n",
    "            'student_draft': result.cleaned_text,\n",
    "            'validation_metadata': {\n",
    "                'confidence_score': result.confidence_score,\n",
    "                'status': result.status.value,\n",
    "                'quality_metrics': result.metadata.get('quality_metrics', {}),\n",
    "                'errors': result.error_messages\n",
    "            }\n",
    "        }\n",
    "        cleaned_drafts.append(cleaned_draft)\n",
    "    # Note: Low confidence responses are rejected\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processed {i + 1}/{len(student_drafts)} drafts...\")\n",
    "\n",
    "# Calculate success rates\n",
    "total_processed = validation_statistics['total']\n",
    "success_rate = (validation_statistics['valid'] + validation_statistics['corrected']) / total_processed * 100\n",
    "average_confidence = sum(validation_statistics['quality_scores']) / len(validation_statistics['quality_scores'])\n",
    "\n",
    "print(f\"\\n=== VALIDATION RESULTS ===\")\n",
    "print(f\"Total processed: {total_processed}\")\n",
    "print(f\"✅ Valid: {validation_statistics['valid']} ({validation_statistics['valid']/total_processed*100:.1f}%)\")\n",
    "print(f\"🔧 Corrected: {validation_statistics['corrected']} ({validation_statistics['corrected']/total_processed*100:.1f}%)\")\n",
    "print(f\"❌ Invalid: {validation_statistics['invalid']} ({validation_statistics['invalid']/total_processed*100:.1f}%)\")\n",
    "print(f\"📊 Overall Success Rate: {success_rate:.1f}% (Target: 95%+)\")\n",
    "print(f\"🎯 Average Confidence: {average_confidence:.3f}\")\n",
    "\n",
    "print(f\"\\n=== CONFIDENCE DISTRIBUTION ===\")\n",
    "print(f\"High (≥0.8): {validation_statistics['high_confidence']} ({validation_statistics['high_confidence']/total_processed*100:.1f}%)\")\n",
    "print(f\"Medium (0.5-0.8): {validation_statistics['medium_confidence']} ({validation_statistics['medium_confidence']/total_processed*100:.1f}%)\")\n",
    "print(f\"Low (<0.5): {validation_statistics['low_confidence']} ({validation_statistics['low_confidence']/total_processed*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== FORMAT DETECTION ===\")\n",
    "print(f\"Q&A Interleaved: {validation_statistics['qa_format_detected']} ({validation_statistics['qa_format_detected']/total_processed*100:.1f}%)\")\n",
    "print(f\"Traditional: {validation_statistics['traditional_format']} ({validation_statistics['traditional_format']/total_processed*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== DATA RETENTION ===\")\n",
    "print(f\"Kept after validation: {len(cleaned_drafts)}/{total_processed} ({len(cleaned_drafts)/total_processed*100:.1f}%)\")\n",
    "\n",
    "# Save cleaned data with metadata\n",
    "cleaned_path_full = os.path.join(parent_dir, CLEANED_STUDENT_DRAFTS_PATH)\n",
    "with open(cleaned_path_full, 'w', encoding='utf-8') as f:\n",
    "    for draft in cleaned_drafts:\n",
    "        f.write(json.dumps(draft) + '\\n')\n",
    "\n",
    "print(f\"\\n✅ Enhanced cleaned student drafts saved to {cleaned_path_full}\")\n",
    "print(f\"🚀 Professional validation complete! Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "if success_rate >= 95:\n",
    "    print(\"🎯 SUCCESS: Achieved target validation rate of 95%+\")\n",
    "else:\n",
    "    print(f\"⚠️  Below target: {95 - success_rate:.1f}pp improvement needed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
