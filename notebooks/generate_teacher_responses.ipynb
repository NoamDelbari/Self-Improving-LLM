{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738efd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets transformers openai bitsandbytes accelerate python-dotenv huggingface_hub huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a77f4",
   "metadata": {},
   "source": [
    "## Setup ENV var config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19bfbc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Configuration ===\n",
      "Dataset: voidful/StrategyQA\n",
      "Model: microsoft/Phi-3.5-mini-instruct\n",
      "Batch size: 8\n",
      "4-bit quantization: True\n",
      "GPT-4 dry run: False\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Dataset parameters\n",
    "DATASET_NAME = os.getenv('DATASET_NAME', 'voidful/StrategyQA')\n",
    "TRAIN_SAMPLES = int(os.getenv('TRAIN_SAMPLES', '100'))\n",
    "RANDOM_SEED = int(os.getenv('RANDOM_SEED', '42'))\n",
    "USE_FULL_DATASET = os.getenv('USE_FULL_DATASET', 'False').lower() in ('true', '1', 't')\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "MODEL_NAME = os.getenv('MODEL_NAME', 'microsoft/phi-2')\n",
    "MAX_NEW_TOKENS = int(os.getenv('MAX_NEW_TOKENS', '35'))\n",
    "BATCH_SIZE = int(os.getenv('BATCH_SIZE', '8'))\n",
    "USE_4BIT = os.getenv('USE_4BIT', 'True').lower() in ('true', '1', 't')\n",
    "MAX_SEQ_LENGTH = int(os.getenv('MAX_SEQ_LENGTH', '512'))\n",
    "HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN', '')\n",
    "\n",
    "# Generation parameters\n",
    "DO_SAMPLE = os.getenv('DO_SAMPLE', 'False').lower() in ('true', '1', 't')\n",
    "TEMPERATURE = float(os.getenv('TEMPERATURE', '0.7'))\n",
    "\n",
    "# GPT-4 parameters\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')\n",
    "GPT4_MODEL = os.getenv('GPT4_MODEL', 'gpt-4')\n",
    "GPT4_MAX_TOKENS = int(os.getenv('GPT4_MAX_TOKENS', '150'))\n",
    "GPT4_TEMPERATURE = float(os.getenv('GPT4_TEMPERATURE', '0.3'))\n",
    "DRY_RUN = os.getenv('DRY_RUN', 'True').lower() in ('true', '1', 't')\n",
    "\n",
    "# Student Draft Generation\n",
    "STUDENT_MAX_TOKENS = int(os.getenv('STUDENT_MAX_TOKENS', '200'))\n",
    "STUDENT_TEMPERATURE = float(os.getenv('STUDENT_TEMPERATURE', '0.7'))\n",
    "STUDENT_BATCH_SIZE = int(os.getenv('STUDENT_BATCH_SIZE', '8'))\n",
    "\n",
    "# Enhanced Evaluation Generation\n",
    "EVAL_MAX_TOKENS = int(os.getenv('EVAL_MAX_TOKENS', '256'))\n",
    "EVAL_TEMPERATURE = float(os.getenv('EVAL_TEMPERATURE', '0.7'))\n",
    "EVAL_BATCH_SIZE = int(os.getenv('EVAL_BATCH_SIZE', '4'))\n",
    "\n",
    "# Quick Evaluation\n",
    "QUICK_EVAL_MAX_TOKENS = int(os.getenv('QUICK_EVAL_MAX_TOKENS', '5'))\n",
    "\n",
    "# Training Configuration\n",
    "# Phase A Training\n",
    "PHASE_A_EPOCHS = int(os.getenv('PHASE_A_EPOCHS', '3'))\n",
    "PHASE_A_BATCH_SIZE = int(os.getenv('PHASE_A_BATCH_SIZE', '1'))\n",
    "PHASE_A_LEARNING_RATE = float(os.getenv('PHASE_A_LEARNING_RATE', '1e-4'))\n",
    "PHASE_A_WARMUP_RATIO = float(os.getenv('PHASE_A_WARMUP_RATIO', '0.1'))\n",
    "PHASE_A_WEIGHT_DECAY = float(os.getenv('PHASE_A_WEIGHT_DECAY', '0.01'))\n",
    "\n",
    "# Phase B Training\n",
    "PHASE_B_EPOCHS = int(os.getenv('PHASE_B_EPOCHS', '3'))\n",
    "PHASE_B_BATCH_SIZE = int(os.getenv('PHASE_B_BATCH_SIZE', '4'))\n",
    "PHASE_B_LEARNING_RATE = float(os.getenv('PHASE_B_LEARNING_RATE', '5e-5'))\n",
    "PHASE_B_WARMUP_RATIO = float(os.getenv('PHASE_B_WARMUP_RATIO', '0.1'))\n",
    "PHASE_B_WEIGHT_DECAY = float(os.getenv('PHASE_B_WEIGHT_DECAY', '0.01'))\n",
    "\n",
    "# Progressive Curriculum Training\n",
    "CURRICULUM_STAGE1_EPOCHS = int(os.getenv('CURRICULUM_STAGE1_EPOCHS', '1'))\n",
    "CURRICULUM_STAGE1_LEARNING_RATE = float(os.getenv('CURRICULUM_STAGE1_LEARNING_RATE', '5e-5'))\n",
    "CURRICULUM_STAGE1_WARMUP_RATIO = float(os.getenv('CURRICULUM_STAGE1_WARMUP_RATIO', '0.1'))\n",
    "CURRICULUM_STAGE1_WEIGHT_DECAY = float(os.getenv('CURRICULUM_STAGE1_WEIGHT_DECAY', '0.01'))\n",
    "\n",
    "CURRICULUM_STAGE2_EPOCHS = int(os.getenv('CURRICULUM_STAGE2_EPOCHS', '2'))\n",
    "CURRICULUM_STAGE2_LEARNING_RATE = float(os.getenv('CURRICULUM_STAGE2_LEARNING_RATE', '3e-5'))\n",
    "CURRICULUM_STAGE2_WARMUP_RATIO = float(os.getenv('CURRICULUM_STAGE2_WARMUP_RATIO', '0.1'))\n",
    "CURRICULUM_STAGE2_WEIGHT_DECAY = float(os.getenv('CURRICULUM_STAGE2_WEIGHT_DECAY', '0.01'))\n",
    "\n",
    "# Validation Configuration\n",
    "HIGH_CONFIDENCE_THRESHOLD = float(os.getenv('HIGH_CONFIDENCE_THRESHOLD', '0.8'))\n",
    "MEDIUM_CONFIDENCE_THRESHOLD = float(os.getenv('MEDIUM_CONFIDENCE_THRESHOLD', '0.5'))\n",
    "LOW_CONFIDENCE_THRESHOLD = float(os.getenv('LOW_CONFIDENCE_THRESHOLD', '0.3'))\n",
    "VALIDATION_ACCEPTANCE_THRESHOLD = float(os.getenv('VALIDATION_ACCEPTANCE_THRESHOLD', '0.3'))\n",
    "\n",
    "# Quality Thresholds\n",
    "VALID_QUALITY_THRESHOLD = float(os.getenv('VALID_QUALITY_THRESHOLD', '0.5'))\n",
    "CORRECTED_QUALITY_THRESHOLD = float(os.getenv('CORRECTED_QUALITY_THRESHOLD', '0.3'))\n",
    "\n",
    "# Token Emphasis Configuration\n",
    "EMPHASIS_MULTIPLIER = float(os.getenv('EMPHASIS_MULTIPLIER', '2.5'))\n",
    "ADAPTIVE_EMPHASIS = os.getenv('ADAPTIVE_EMPHASIS', 'True').lower() in ('true', '1', 't')\n",
    "\n",
    "# Memory and Performance\n",
    "GRADIENT_CHECKPOINTING = os.getenv('GRADIENT_CHECKPOINTING', 'True').lower() in ('true', '1', 't')\n",
    "FP16 = os.getenv('FP16', 'False').lower() in ('true', '1', 't')\n",
    "BF16 = os.getenv('BF16', 'True').lower() in ('true', '1', 't')\n",
    "DATALOADER_NUM_WORKERS = int(os.getenv('DATALOADER_NUM_WORKERS', '0'))\n",
    "DATALOADER_PERSISTENT_WORKERS = os.getenv('DATALOADER_PERSISTENT_WORKERS', 'False').lower() in ('true', '1', 't')\n",
    "SKIP_MEMORY_METRICS = os.getenv('SKIP_MEMORY_METRICS', 'True').lower() in ('true', '1', 't')\n",
    "\n",
    "# Logging and Monitoring\n",
    "LOGGING_STEPS = int(os.getenv('LOGGING_STEPS', '10'))\n",
    "SAVE_STRATEGY = os.getenv('SAVE_STRATEGY', 'epoch')\n",
    "REPORT_TO = os.getenv('REPORT_TO', 'none')\n",
    "LOAD_BEST_MODEL_AT_END = os.getenv('LOAD_BEST_MODEL_AT_END', 'False').lower() in ('true', '1', 't')\n",
    "\n",
    "# Evaluation Configuration\n",
    "EVAL_SIZE = int(os.getenv('EVAL_SIZE', '100'))\n",
    "ENHANCED_EVAL_BATCH_SIZE = int(os.getenv('ENHANCED_EVAL_BATCH_SIZE', '4'))\n",
    "# File paths\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(parent_dir, os.getenv(\"DATA_DIR\", \"data\"))\n",
    "RAW_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "STUDENT_DIR = os.path.join(DATA_DIR, 'student')\n",
    "TEACHER_DIR = os.path.join(DATA_DIR, 'teacher')\n",
    "SAMPLE_TRAIN_PATH = os.path.join(TRAIN_DIR, 'sample_train.jsonl')\n",
    "STUDENT_DRAFTS_PATH = os.path.join(STUDENT_DIR, 'student_drafts.jsonl')\n",
    "CLEANED_STUDENT_DRAFTS_PATH = os.path.join(STUDENT_DIR, 'cleaned_student_drafts.jsonl')\n",
    "TEACHER_OUTPUTS_PATH = os.path.join(TEACHER_DIR, 'teacher_outputs.jsonl')\n",
    "BASELINE_PATH = os.path.join(TRAIN_DIR, 'train_baseline.jsonl')\n",
    "COT_PATH = os.path.join(TRAIN_DIR, 'train_cot.jsonl')\n",
    "COT_PATH_QA_COT = os.path.join(TEACHER_DIR, 'teacher_outputs_qa_cot.jsonl')\n",
    "SAMPLE_TEST_PATH = SAMPLE_TRAIN_PATH  # Alias for consistency with Build Training Corpora cell\n",
    "\n",
    "# Print configuration\n",
    "print(\"=== Configuration ===\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"4-bit quantization: {USE_4BIT}\")\n",
    "print(f\"GPT-4 dry run: {DRY_RUN}\")\n",
    "print(\"==\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b99bb",
   "metadata": {},
   "source": [
    "## Generate Teacher Responses\n",
    "\n",
    "We now call GPT‑4 to obtain chain‑of‑thought (CoT) reasoning and final yes/no answers for each question/draft pair.  The prompt format follows the plan:\n",
    "\n",
    "```\n",
    "Q: <original yes/no question>\n",
    "Student draft: <answer + clarifying questions>\n",
    "Teacher: Please think step-by-step and provide your thought process and final Yes/No answer.\n",
    "```\n",
    "\n",
    "To run the actual API calls, you must provide a valid OpenAI API key.  If you set `dry_run=True`, dummy responses will be generated for testing purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9861ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import logging\n",
    "from enum import Enum\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class ValidationStatus(Enum):\n",
    "    VALID = \"valid\"\n",
    "    INVALID = \"invalid\"\n",
    "    CORRECTED = \"corrected\"\n",
    "    FAILED = \"failed\"\n",
    "\n",
    "@dataclass\n",
    "class ValidationResult:\n",
    "    status: ValidationStatus\n",
    "    original_text: str\n",
    "    cleaned_text: Optional[str]\n",
    "    confidence_score: float  # 0.0 - 1.0\n",
    "    error_messages: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "    def is_valid(self) -> bool:\n",
    "        return self.status in [ValidationStatus.VALID, ValidationStatus.CORRECTED]\n",
    "\n",
    "\n",
    "class BaseResponseValidator(ABC):\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    @abstractmethod\n",
    "    def validate(self, response_text: str) -> ValidationResult:\n",
    "        pass\n",
    "\n",
    "    def _calculate_base_confidence(self, parsing_success: bool, content_quality: float) -> float:\n",
    "        \"\"\"Calculate base confidence score from parsing and content quality.\"\"\"\n",
    "        parsing_score = 1.0 if parsing_success else 0.3\n",
    "        return (parsing_score * 0.6) + (content_quality * 0.4)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TeacherResponse:\n",
    "    teaching_analysis: str\n",
    "    step_by_step_reasoning: str\n",
    "    final_assessment: str\n",
    "    extracted_answer: str  # YES/NO\n",
    "    confidence_score: float\n",
    "    quality_metrics: Dict[str, float]\n",
    "\n",
    "class TeacherResponseValidator(BaseResponseValidator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Support both old three-section and new Q&A formats\n",
    "        self.section_headers = [\n",
    "            r'## Teaching Analysis',\n",
    "            r'## Step-by-Step Reasoning',\n",
    "            r'## Final Assessment'\n",
    "        ]\n",
    "        # Enhanced patterns for Q&A-CoT format\n",
    "        self.final_answer_patterns = [\n",
    "            r'The answer is \\*\\*(Yes|No)\\*\\*',  # Primary Q&A-CoT pattern\n",
    "            r'## Final Assessment.*?\\*\\*(YES|NO)\\*\\*',  # Legacy pattern\n",
    "            r'Based on.*?the answer is.*?\\*\\*(YES|NO)\\*\\*',\n",
    "            r'\\*\\*(YES|NO)\\*\\*',\n",
    "            r'\\b(YES|NO)\\b(?=\\s*[.!]?\\s*$)'\n",
    "        ]\n",
    "        # Q&A format patterns\n",
    "        self.qa_patterns = [\n",
    "            r'Question\\s+(\\d+)\\s*:?\\s*(.+?)(?=Answer\\s+\\1|$)',\n",
    "            r'Answer\\s+(\\d+)\\s*:?\\s*(.+?)(?=Question\\s+\\d+|Therefore|The answer is|$)'\n",
    "        ]\n",
    "\n",
    "    def validate(self, response_text: str) -> ValidationResult:\n",
    "        \"\"\"Validate teacher response supporting both Q&A-CoT and legacy formats.\"\"\"\n",
    "        errors = []\n",
    "        metadata = {}\n",
    "\n",
    "        # Detect format type\n",
    "        has_qa_format = self._detect_qa_format(response_text)\n",
    "        has_three_section = self._detect_three_section_format(response_text)\n",
    "\n",
    "        metadata['format_detected'] = 'qa_cot' if has_qa_format else ('three_section' if has_three_section else 'unknown')\n",
    "\n",
    "        # Layer 1: Structural Parsing based on format\n",
    "        if has_qa_format:\n",
    "            parsed_data = self._parse_qa_cot_structure(response_text)\n",
    "            parsing_success = parsed_data is not None\n",
    "        elif has_three_section:\n",
    "            parsed_data = self._parse_three_section_structure(response_text)\n",
    "            parsing_success = parsed_data is not None\n",
    "        else:\n",
    "            parsed_data = None\n",
    "            parsing_success = False\n",
    "            errors.append(\"Unknown format: neither Q&A-CoT nor three-section structure detected\")\n",
    "\n",
    "        if not parsing_success and not errors:\n",
    "            errors.append(\"Failed to parse response structure\")\n",
    "            # Attempt correction\n",
    "            corrected = self._attempt_structure_correction(response_text)\n",
    "            if corrected:\n",
    "                if has_qa_format:\n",
    "                    parsed_data = self._parse_qa_cot_structure(corrected)\n",
    "                else:\n",
    "                    parsed_data = self._parse_three_section_structure(corrected)\n",
    "                if parsed_data:\n",
    "                    parsing_success = True\n",
    "                    errors.append(\"Structure corrected automatically\")\n",
    "\n",
    "        # Layer 2: Content Validation\n",
    "        if parsed_data:\n",
    "            if has_qa_format:\n",
    "                quality_metrics = self._assess_qa_cot_quality(parsed_data, response_text)\n",
    "            else:\n",
    "                quality_metrics = self._assess_teacher_quality(parsed_data)\n",
    "            metadata['quality_metrics'] = quality_metrics\n",
    "\n",
    "            # Extract final answer\n",
    "            final_answer = self._extract_final_answer(response_text, parsed_data)\n",
    "            metadata['extracted_answer'] = final_answer\n",
    "\n",
    "            if not final_answer or final_answer not in ['YES', 'NO']:\n",
    "                errors.append(\"Failed to extract valid final answer\")\n",
    "                quality_metrics['answer_extraction'] = 0.0\n",
    "            else:\n",
    "                quality_metrics['answer_extraction'] = 1.0\n",
    "        else:\n",
    "            quality_metrics = {\n",
    "                'overall': 0.0,\n",
    "                'structural_quality': 0.0,\n",
    "                'content_quality': 0.0,\n",
    "                'answer_extraction': 0.0\n",
    "            }\n",
    "            final_answer = None\n",
    "\n",
    "        # Layer 4: Confidence Scoring\n",
    "        confidence_score = self._calculate_teacher_confidence(\n",
    "            parsing_success, quality_metrics, len(errors), has_qa_format\n",
    "        )\n",
    "\n",
    "        # Determine final status\n",
    "        if parsing_success and quality_metrics['overall'] >= 0.7 and final_answer:\n",
    "            status = ValidationStatus.VALID\n",
    "            cleaned_text = self._format_response_output(parsed_data, final_answer, has_qa_format)\n",
    "        elif parsing_success and quality_metrics['overall'] >= 0.5 and final_answer:\n",
    "            status = ValidationStatus.CORRECTED\n",
    "            cleaned_text = self._format_response_output(parsed_data, final_answer, has_qa_format)\n",
    "        else:\n",
    "            status = ValidationStatus.INVALID\n",
    "            cleaned_text = None\n",
    "\n",
    "        return ValidationResult(\n",
    "            status=status,\n",
    "            original_text=response_text,\n",
    "            cleaned_text=cleaned_text,\n",
    "            confidence_score=confidence_score,\n",
    "            error_messages=errors,\n",
    "            metadata=metadata\n",
    "        )\n",
    "\n",
    "    def _detect_qa_format(self, text: str) -> bool:\n",
    "        \"\"\"Detect if text uses Q&A interleaved format.\"\"\"\n",
    "        return bool(re.search(r'(?:Question\\s+\\d+\\s*:.*?Answer\\s+\\d+\\s*:|Answer\\s*:.*?Questions?\\s*:)', text, re.DOTALL | re.IGNORECASE))\n",
    "\n",
    "    def _detect_three_section_format(self, text: str) -> bool:\n",
    "        \"\"\"Detect if text uses three-section format.\"\"\"\n",
    "        sections_found = sum(1 for header in self.section_headers\n",
    "                           if re.search(header, text, re.IGNORECASE))\n",
    "        return sections_found >= 2\n",
    "\n",
    "    def _parse_qa_cot_structure(self, text: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Parse Q&A-CoT format: Question 1: ... Answer 1: ... Therefore ...\"\"\"\n",
    "        # Extract Q&A pairs\n",
    "        qa_pairs = []\n",
    "        question_matches = re.findall(r'Question\\s+(\\d+)\\s*:?\\s*(.+?)(?=Answer\\s+\\1|$)', text, re.DOTALL | re.IGNORECASE)\n",
    "        answer_matches = re.findall(r'Answer\\s+(\\d+)\\s*:?\\s*(.+?)(?=Question\\s+\\d+|Therefore|The answer is|$)', text, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "        # Pair up questions and answers\n",
    "        for i, (q_num, question) in enumerate(question_matches):\n",
    "            answer = \"\"\n",
    "            for a_num, ans in answer_matches:\n",
    "                if a_num == q_num:\n",
    "                    answer = ans.strip()\n",
    "                    break\n",
    "            qa_pairs.append({\n",
    "                'number': q_num,\n",
    "                'question': question.strip(),\n",
    "                'answer': answer\n",
    "            })\n",
    "\n",
    "        # Extract therefore/conclusion\n",
    "        therefore_match = re.search(r'Therefore,?\\s*(.+?)(?=The answer is|$)', text, re.DOTALL | re.IGNORECASE)\n",
    "        therefore_text = therefore_match.group(1).strip() if therefore_match else \"\"\n",
    "\n",
    "        if qa_pairs:  # At least one Q&A pair found\n",
    "            return {\n",
    "                'qa_pairs': qa_pairs,\n",
    "                'therefore_reasoning': therefore_text,\n",
    "                'format_type': 'qa_cot',\n",
    "                'pair_count': len(qa_pairs)\n",
    "            }\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _parse_three_section_structure(self, text: str) -> Optional[Dict[str, str]]:\n",
    "        \"\"\"Parse the three-section teacher response format.\"\"\"\n",
    "        sections = {}\n",
    "\n",
    "        # Extract Teaching Analysis\n",
    "        teaching_match = re.search(\n",
    "            r'## Teaching Analysis\\s*\\n(.*?)(?=\\n## |$)',\n",
    "            text, re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        if teaching_match:\n",
    "            sections['teaching_analysis'] = teaching_match.group(1).strip()\n",
    "\n",
    "        # Extract Step-by-Step Reasoning\n",
    "        reasoning_match = re.search(\n",
    "            r'## Step-by-Step Reasoning\\s*\\n(.*?)(?=\\n## |$)',\n",
    "            text, re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        if reasoning_match:\n",
    "            sections['step_by_step_reasoning'] = reasoning_match.group(1).strip()\n",
    "\n",
    "        # Extract Final Assessment\n",
    "        assessment_match = re.search(\n",
    "            r'## Final Assessment\\s*\\n(.*?)(?=\\n## |$)',\n",
    "            text, re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        if assessment_match:\n",
    "            sections['final_assessment'] = assessment_match.group(1).strip()\n",
    "\n",
    "        # Require at least 2 of 3 sections\n",
    "        if len(sections) >= 2:\n",
    "            sections['format_type'] = 'three_section'\n",
    "            return sections\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _assess_qa_cot_quality(self, parsed_data: Dict[str, Any], full_text: str) -> Dict[str, float]:\n",
    "        \"\"\"Assess quality of Q&A-CoT format responses.\"\"\"\n",
    "        qa_pairs = parsed_data.get('qa_pairs', [])\n",
    "        therefore_text = parsed_data.get('therefore_reasoning', '')\n",
    "\n",
    "        # Structural quality - based on Q&A pairs\n",
    "        if len(qa_pairs) == 0:\n",
    "            structural_quality = 0.0\n",
    "        elif len(qa_pairs) == 1:\n",
    "            structural_quality = 0.7  # Single question is okay\n",
    "        elif len(qa_pairs) == 2:\n",
    "            structural_quality = 1.0  # Ideal: 2 questions\n",
    "        else:\n",
    "            structural_quality = 0.9  # More than 2 is still good but not ideal\n",
    "\n",
    "        # Content quality assessment\n",
    "        content_scores = []\n",
    "\n",
    "        # Question quality\n",
    "        if qa_pairs:\n",
    "            question_scores = []\n",
    "            for pair in qa_pairs:\n",
    "                question = pair.get('question', '')\n",
    "                answer = pair.get('answer', '')\n",
    "\n",
    "                # Question should be specific and end with ?\n",
    "                q_score = 0.0\n",
    "                if question:\n",
    "                    q_score += 0.3  # Has question\n",
    "                    if '?' in question:\n",
    "                        q_score += 0.3  # Proper question format\n",
    "                    if len(question.split()) >= 4:\n",
    "                        q_score += 0.2  # Reasonable length\n",
    "                    if any(word in question.lower() for word in ['what', 'when', 'where', 'who', 'how', 'why', 'did', 'does', 'is', 'are']):\n",
    "                        q_score += 0.2  # Contains question words\n",
    "\n",
    "                # Answer should be factual and relevant\n",
    "                a_score = 0.0\n",
    "                if answer:\n",
    "                    a_score += 0.4  # Has answer\n",
    "                    if len(answer.split()) >= 3:\n",
    "                        a_score += 0.3  # Substantial answer\n",
    "                    if len(answer.split()) <= 50:\n",
    "                        a_score += 0.3  # Not too verbose\n",
    "\n",
    "                pair_score = (q_score + a_score) / 2\n",
    "                question_scores.append(pair_score)\n",
    "\n",
    "            content_scores.append(sum(question_scores) / len(question_scores))\n",
    "        else:\n",
    "            content_scores.append(0.0)\n",
    "\n",
    "        # Therefore reasoning quality\n",
    "        if therefore_text:\n",
    "            therefore_score = 0.0\n",
    "            therefore_score += 0.4  # Has therefore reasoning\n",
    "            if len(therefore_text.split()) >= 5:\n",
    "                therefore_score += 0.3  # Reasonable length\n",
    "            if any(word in therefore_text.lower() for word in ['therefore', 'since', 'because', 'so', 'thus']):\n",
    "                therefore_score += 0.3  # Contains logical connectors\n",
    "            content_scores.append(therefore_score)\n",
    "        else:\n",
    "            content_scores.append(0.0)\n",
    "\n",
    "        content_quality = sum(content_scores) / len(content_scores) if content_scores else 0.0\n",
    "\n",
    "        # Overall quality\n",
    "        overall_quality = (structural_quality * 0.4) + (content_quality * 0.6)\n",
    "\n",
    "        return {\n",
    "            'overall': overall_quality,\n",
    "            'structural_quality': structural_quality,\n",
    "            'content_quality': content_quality,\n",
    "            'qa_pair_count': len(qa_pairs),\n",
    "            'has_therefore': bool(therefore_text)\n",
    "        }\n",
    "\n",
    "    def _extract_final_answer(self, text: str, parsed_data: Dict[str, Any]) -> Optional[str]:\n",
    "        \"\"\"Extract final YES/NO answer with enhanced Q&A-CoT patterns.\"\"\"\n",
    "        # Try patterns in order of preference (Q&A-CoT first)\n",
    "        for pattern in self.final_answer_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "            if match:\n",
    "                answer = match.group(1).upper()\n",
    "                # Normalize to YES/NO\n",
    "                if answer.lower() in ['yes', 'no']:\n",
    "                    return answer.upper()\n",
    "\n",
    "        # Fallback for three-section format\n",
    "        if parsed_data.get('format_type') == 'three_section' and 'final_assessment' in parsed_data:\n",
    "            final_section = parsed_data['final_assessment']\n",
    "            yes_count = len(re.findall(r'\\byes\\b', final_section, re.IGNORECASE))\n",
    "            no_count = len(re.findall(r'\\bno\\b', final_section, re.IGNORECASE))\n",
    "\n",
    "            if yes_count > no_count:\n",
    "                return 'YES'\n",
    "            elif no_count > yes_count:\n",
    "                return 'NO'\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _assess_teacher_quality(self, parsed_sections: Dict[str, str]) -> Dict[str, float]:\n",
    "        \"\"\"Assess quality of three-section teacher response.\"\"\"\n",
    "        # Structural quality (completeness)\n",
    "        section_completeness = len([k for k in parsed_sections.keys() if k != 'format_type']) / 3\n",
    "\n",
    "        # Content quality assessment\n",
    "        content_scores = []\n",
    "\n",
    "        for section_name, content in parsed_sections.items():\n",
    "            if section_name == 'format_type':\n",
    "                continue\n",
    "\n",
    "            if not content.strip():\n",
    "                content_scores.append(0.0)\n",
    "                continue\n",
    "\n",
    "            # Basic content quality heuristics\n",
    "            word_count = len(content.split())\n",
    "\n",
    "            if section_name == 'teaching_analysis':\n",
    "                ideal_length = 50\n",
    "                length_score = min(word_count / ideal_length, 1.0)\n",
    "            elif section_name == 'step_by_step_reasoning':\n",
    "                ideal_length = 100\n",
    "                length_score = min(word_count / ideal_length, 1.0)\n",
    "                # Bonus for structure\n",
    "                has_structure = bool(re.search(r'\\d+\\.|\\n-|Step \\d+', content))\n",
    "                length_score += 0.2 if has_structure else 0.0\n",
    "                length_score = min(length_score, 1.0)\n",
    "            elif section_name == 'final_assessment':\n",
    "                ideal_length = 30\n",
    "                length_score = min(word_count / ideal_length, 1.0)\n",
    "                # Bonus for conclusion language\n",
    "                has_conclusion = bool(re.search(r'based on|therefore|in conclusion', content, re.IGNORECASE))\n",
    "                length_score += 0.2 if has_conclusion else 0.0\n",
    "                length_score = min(length_score, 1.0)\n",
    "            else:\n",
    "                length_score = min(word_count / 50, 1.0)\n",
    "\n",
    "            content_scores.append(length_score)\n",
    "\n",
    "        content_quality = sum(content_scores) / len(content_scores) if content_scores else 0.0\n",
    "        overall_quality = (section_completeness * 0.4) + (content_quality * 0.6)\n",
    "\n",
    "        return {\n",
    "            'overall': overall_quality,\n",
    "            'structural_quality': section_completeness,\n",
    "            'content_quality': content_quality,\n",
    "            'section_count': len([k for k in parsed_sections.keys() if k != 'format_type'])\n",
    "        }\n",
    "\n",
    "    def _attempt_structure_correction(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Attempt to correct common structural issues.\"\"\"\n",
    "        corrected = text\n",
    "\n",
    "        # Q&A-CoT corrections\n",
    "        if 'question' in corrected.lower() and 'answer' in corrected.lower():\n",
    "            # Fix missing colons in Q&A format\n",
    "            corrected = re.sub(r'\\b(Question\\s+\\d+)\\b(?!\\s*:)', r'\\1:', corrected, flags=re.IGNORECASE)\n",
    "            corrected = re.sub(r'\\b(Answer\\s+\\d+)\\b(?!\\s*:)', r'\\1:', corrected, flags=re.IGNORECASE)\n",
    "\n",
    "        # Three-section corrections\n",
    "        if '## Teaching Analysis' not in corrected and 'Teaching' in corrected:\n",
    "            corrected = re.sub(r'^(Teaching.*?):', r'## Teaching Analysis\\n\\1:', corrected, flags=re.MULTILINE)\n",
    "\n",
    "        if '## Step-by-Step' not in corrected and ('Step' in corrected or 'reasoning' in corrected.lower()):\n",
    "            corrected = re.sub(r'^(.*reasoning.*?):', r'## Step-by-Step Reasoning\\n\\1:', corrected, flags=re.MULTILINE | re.IGNORECASE)\n",
    "\n",
    "        if '## Final Assessment' not in corrected and ('final' in corrected.lower() or 'assessment' in corrected.lower()):\n",
    "            corrected = re.sub(r'^(.*(?:final|assessment).*?):', r'## Final Assessment\\n\\1:', corrected, flags=re.MULTILINE | re.IGNORECASE)\n",
    "\n",
    "        return corrected if corrected != text else None\n",
    "\n",
    "    def _calculate_teacher_confidence(self, parsing_success: bool, quality_metrics: Dict[str, float], error_count: int, is_qa_format: bool) -> float:\n",
    "        \"\"\"Calculate confidence score for teacher responses.\"\"\"\n",
    "        base_confidence = self._calculate_base_confidence(parsing_success, quality_metrics['overall'])\n",
    "\n",
    "        # Format-specific adjustments\n",
    "        format_bonus = 0.1 if is_qa_format else 0.05  # Slight preference for Q&A format\n",
    "        structure_bonus = 0.1 if quality_metrics['structural_quality'] >= 0.8 else 0.0\n",
    "        answer_bonus = 0.1 if quality_metrics.get('answer_extraction', 0) >= 1.0 else 0.0\n",
    "        error_penalty = min(error_count * 0.15, 0.3)\n",
    "\n",
    "        return max(0.0, min(1.0, base_confidence + format_bonus + structure_bonus + answer_bonus - error_penalty))\n",
    "\n",
    "    def _format_response_output(self, parsed_data: Dict[str, Any], final_answer: str, is_qa_format: bool) -> str:\n",
    "        \"\"\"Format response output based on detected format.\"\"\"\n",
    "        if is_qa_format:\n",
    "            # Format Q&A-CoT output\n",
    "            output_parts = []\n",
    "            qa_pairs = parsed_data.get('qa_pairs', [])\n",
    "\n",
    "            for pair in qa_pairs:\n",
    "                output_parts.append(f\"Question {pair['number']}: {pair['question']}\")\n",
    "                output_parts.append(f\"Answer {pair['number']}: {pair['answer']}\")\n",
    "\n",
    "            therefore_text = parsed_data.get('therefore_reasoning', '')\n",
    "            if therefore_text:\n",
    "                output_parts.append(f\"Therefore, {therefore_text}\")\n",
    "\n",
    "            output_parts.append(f\"The answer is **{final_answer}**.\")\n",
    "\n",
    "            return '\\n'.join(output_parts)\n",
    "\n",
    "        else:\n",
    "            # Format three-section output\n",
    "            output_parts = []\n",
    "\n",
    "            if 'teaching_analysis' in parsed_data:\n",
    "                output_parts.append(f\"## Teaching Analysis\\n{parsed_data['teaching_analysis']}\")\n",
    "\n",
    "            if 'step_by_step_reasoning' in parsed_data:\n",
    "                output_parts.append(f\"## Step-by-Step Reasoning\\n{parsed_data['step_by_step_reasoning']}\")\n",
    "\n",
    "            if 'final_assessment' in parsed_data:\n",
    "                output_parts.append(f\"## Final Assessment\\n{parsed_data['final_assessment']}\")\n",
    "            else:\n",
    "                output_parts.append(f\"## Final Assessment\\nBased on this analysis, the answer is: **{final_answer}**\")\n",
    "\n",
    "            return '\\n\\n'.join(output_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd88039b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENHANCED Q&A-COT TEACHER GENERATION ===\n",
      "Implementing interleaved Q&A format from StrategyQA implementation plan...\n",
      "Loaded 200 questions from training data\n",
      "Facts available for 200 questions\n",
      "\n",
      "=== GENERATING Q&A-COT TEACHER RESPONSES ===\n",
      "Using enhanced GPT-4 prompts for interleaved self-questioning format...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 211\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    210\u001b[39m         \u001b[38;5;66;03m# Use enhanced Q&A-CoT generation with facts\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m         response_text = \u001b[43mcall_gpt4_qa_cot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfacts_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m         \u001b[38;5;66;03m# Validate it follows Q&A format\u001b[39;00m\n\u001b[32m    214\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re.search(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mQuestion\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*:\u001b[39m\u001b[33m'\u001b[39m, response_text):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mcall_gpt4_qa_cot\u001b[39m\u001b[34m(question, facts_list)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_gpt4_qa_cot\u001b[39m(question: \u001b[38;5;28mstr\u001b[39m, facts_list: Optional[List[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     71\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Enhanced GPT-4 call for Q&A-CoT generation.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOPENAI_API_KEY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     system_prompt = create_qa_cot_teacher_prompt()\n\u001b[32m     76\u001b[39m     user_prompt = create_qa_cot_user_prompt(question, facts_list)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\noham\\Desktop\\Self-Improving-LLM\\.venv\\Lib\\site-packages\\openai\\_client.py:154\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    152\u001b[39m     base_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://api.openai.com/v1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28mself\u001b[39m._default_stream_cls = Stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\noham\\Desktop\\Self-Improving-LLM\\.venv\\Lib\\site-packages\\openai\\_base_client.py:861\u001b[39m, in \u001b[36mSyncAPIClient.__init__\u001b[39m\u001b[34m(self, version, base_url, max_retries, timeout, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[39m\n\u001b[32m    847\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    848\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid `http_client` argument; Expected an instance of `httpx.Client` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(http_client)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    849\u001b[39m     )\n\u001b[32m    851\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    852\u001b[39m     version=version,\n\u001b[32m    853\u001b[39m     \u001b[38;5;66;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    859\u001b[39m     _strict_response_validation=_strict_response_validation,\n\u001b[32m    860\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m \u001b[38;5;28mself\u001b[39m._client = http_client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSyncHttpxClientWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;49;00m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\noham\\Desktop\\Self-Improving-LLM\\.venv\\Lib\\site-packages\\openai\\_base_client.py:791\u001b[39m, in \u001b[36m_DefaultHttpxClient.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    789\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mlimits\u001b[39m\u001b[33m\"\u001b[39m, DEFAULT_CONNECTION_LIMITS)\n\u001b[32m    790\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mfollow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\noham\\Desktop\\Self-Improving-LLM\\.venv\\Lib\\site-packages\\httpx\\_client.py:688\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, auth, params, headers, cookies, verify, cert, trust_env, http1, http2, proxy, mounts, timeout, follow_redirects, limits, max_redirects, event_hooks, base_url, transport, default_encoding)\u001b[39m\n\u001b[32m    685\u001b[39m allow_env_proxies = trust_env \u001b[38;5;129;01mand\u001b[39;00m transport \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    686\u001b[39m proxy_map = \u001b[38;5;28mself\u001b[39m._get_proxy_map(proxy, allow_env_proxies)\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m \u001b[38;5;28mself\u001b[39m._transport = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_transport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[38;5;28mself\u001b[39m._mounts: \u001b[38;5;28mdict\u001b[39m[URLPattern, BaseTransport | \u001b[38;5;28;01mNone\u001b[39;00m] = {\n\u001b[32m    698\u001b[39m     URLPattern(key): \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    699\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m proxy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    709\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, proxy \u001b[38;5;129;01min\u001b[39;00m proxy_map.items()\n\u001b[32m    710\u001b[39m }\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mounts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\noham\\Desktop\\Self-Improving-LLM\\.venv\\Lib\\site-packages\\httpx\\_client.py:731\u001b[39m, in \u001b[36mClient._init_transport\u001b[39m\u001b[34m(self, verify, cert, trust_env, http1, http2, limits, transport)\u001b[39m\n\u001b[32m    728\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transport \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    729\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m transport\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHTTPTransport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\noham\\Desktop\\Self-Improving-LLM\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:153\u001b[39m, in \u001b[36mHTTPTransport.__init__\u001b[39m\u001b[34m(self, verify, cert, trust_env, http1, http2, limits, proxy, uds, local_address, retries, socket_options)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    152\u001b[39m proxy = Proxy(url=proxy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proxy, (\u001b[38;5;28mstr\u001b[39m, URL)) \u001b[38;5;28;01melse\u001b[39;00m proxy\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m ssl_context = \u001b[43mcreate_ssl_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m proxy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28mself\u001b[39m._pool = httpcore.ConnectionPool(\n\u001b[32m    157\u001b[39m         ssl_context=ssl_context,\n\u001b[32m    158\u001b[39m         max_connections=limits.max_connections,\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m         socket_options=socket_options,\n\u001b[32m    167\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\noham\\Desktop\\Self-Improving-LLM\\.venv\\Lib\\site-packages\\httpx\\_config.py:40\u001b[39m, in \u001b[36mcreate_ssl_context\u001b[39m\u001b[34m(verify, cert, trust_env)\u001b[39m\n\u001b[32m     37\u001b[39m         ctx = ssl.create_default_context(capath=os.environ[\u001b[33m\"\u001b[39m\u001b[33mSSL_CERT_DIR\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;66;03m# Default case...\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m         ctx = \u001b[43mssl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_default_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcafile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcertifi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m verify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m     42\u001b[39m     ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:770\u001b[39m, in \u001b[36mcreate_default_context\u001b[39m\u001b[34m(purpose, cafile, capath, cadata)\u001b[39m\n\u001b[32m    767\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(purpose)\n\u001b[32m    769\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cafile \u001b[38;5;129;01mor\u001b[39;00m capath \u001b[38;5;129;01mor\u001b[39;00m cadata:\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcafile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m context.verify_mode != CERT_NONE:\n\u001b[32m    772\u001b[39m     \u001b[38;5;66;03m# no explicit cafile, capath or cadata but the verify mode is\u001b[39;00m\n\u001b[32m    773\u001b[39m     \u001b[38;5;66;03m# CERT_OPTIONAL or CERT_REQUIRED. Let's try to load default system\u001b[39;00m\n\u001b[32m    774\u001b[39m     \u001b[38;5;66;03m# root CA certificates for the given purpose. This may fail silently.\u001b[39;00m\n\u001b[32m    775\u001b[39m     context.load_default_certs(purpose)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENHANCED GPT-4 TEACHER PROMPTS FOR Q&A-COT GENERATION (NEW IMPLEMENTATION)\n",
    "# ============================================================================\n",
    "\n",
    "def create_qa_cot_teacher_prompt() -> str:\n",
    "    \"\"\"Create the enhanced GPT-4 teacher prompt for Q&A-CoT generation.\"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert teacher helping a 7B student AI model learn to reason through complex yes/no questions using self-questioning. Your role is to demonstrate the interleaved Q&A reasoning format that the student should learn to mimic.\n",
    "\n",
    "                        CRITICAL INSTRUCTION - INTERLEAVED Q&A FORMAT:\n",
    "                        You must generate responses that teach the student to ask itself clarifying sub-questions and answer them step-by-step. Use this EXACT format:\n",
    "\n",
    "                        Question 1: [Ask a relevant clarifying question the student should consider]\n",
    "                        Answer 1: [Provide a factual, concise answer to that question]\n",
    "                        Question 2: [Optional second question if needed for multi-step reasoning]\n",
    "                        Answer 2: [Answer to the second question]\n",
    "                        Therefore, [synthesize the answers into final reasoning]\n",
    "                        The answer is **[YES/NO]**.\n",
    "\n",
    "                        REQUIREMENTS:\n",
    "                        1. **Self-Questioning Strategy**: Generate 1-2 clarifying questions that break down implicit multi-hop reasoning\n",
    "                        2. **Factual Answers**: Provide correct, concise answers (1-2 sentences) that the student can learn from\n",
    "                        3. **Logical Synthesis**: Use \"Therefore\" to connect sub-answers to the final conclusion\n",
    "                        4. **Format Consistency**: Always use \"Question N:\", \"Answer N:\", \"Therefore\", and \"The answer is **[YES/NO]**\"\n",
    "                        5. **Educational Value**: Make implicit knowledge explicit (e.g., \"laptops didn't exist in ancient times\")\n",
    "\n",
    "                        EXAMPLES OF GOOD Q&A-COT FORMAT:\n",
    "\n",
    "                        Example 1:\n",
    "                        Question 1: Did laptops exist during Aristotle's time?\n",
    "                        Answer 1: No, laptops did not exist in Aristotle's era (they were invented much later).\n",
    "                        Therefore, since laptops didn't exist then, Aristotle could not have used one.\n",
    "                        The answer is **No**.\n",
    "\n",
    "                        Example 2:\n",
    "                        Question 1: What type of animal is a seahorse (fish or mammal)?\n",
    "                        Answer 1: A seahorse is a type of fish (not a mammal).\n",
    "                        Question 2: Do fish produce milk for their young?\n",
    "                        Answer 2: No, fish do not produce milk; only mammals do.\n",
    "                        Therefore, seahorses lack mammalian characteristics (like nursing their young), so a seahorse is not a mammal.\n",
    "                        The answer is **No**.\n",
    "\n",
    "                        STRATEGY GUIDELINES:\n",
    "                        - For single-hop questions: Use 1 question that addresses the key uncertainty\n",
    "                        - For multi-hop questions: Use 2 questions that cover the reasoning chain\n",
    "                        - Questions should be specific and factual (not abstract or philosophical)\n",
    "                        - Answers should provide knowledge the student needs to reach the conclusion\n",
    "                        - Keep total response under 150 words for 7B model capacity\n",
    "\n",
    "                        Your goal is to create training data where the student learns to break down complex reasoning into explicit Q&A steps.\"\"\"\n",
    "\n",
    "    return system_prompt\n",
    "\n",
    "def create_qa_cot_user_prompt(question: str, facts_list: Optional[List[str]] = None) -> str:\n",
    "    \"\"\"Create the user prompt for Q&A-CoT teacher generation.\"\"\"\n",
    "\n",
    "    base_prompt = f\"\"\"QUESTION: {question}\n",
    "\n",
    "                    Generate a Q&A-based chain-of-thought that demonstrates how the student should reason through this question. Follow the interleaved Q&A format specified in your instructions.\"\"\"\n",
    "\n",
    "    if facts_list:\n",
    "        facts_text = \" \".join(facts_list)\n",
    "        base_prompt += f\"\"\"\n",
    "\n",
    "                            HINT - Relevant facts to guide your reasoning: {facts_text}\n",
    "                            Use these facts to determine what questions to ask.\"\"\"\n",
    "\n",
    "    return base_prompt\n",
    "\n",
    "def call_gpt4_qa_cot(question: str, facts_list: Optional[List[str]] = None) -> str:\n",
    "    \"\"\"Enhanced GPT-4 call for Q&A-CoT generation.\"\"\"\n",
    "\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    system_prompt = create_qa_cot_teacher_prompt()\n",
    "    user_prompt = create_qa_cot_user_prompt(question, facts_list)\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=GPT4_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=300,  # Increased for Q&A format\n",
    "            temperature=0.1,  # Low for consistency\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0.1,  # Reduce repetitive phrasing\n",
    "            presence_penalty=0.1\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling GPT-4: {e}\")\n",
    "        # Fallback template for testing\n",
    "        return f\"\"\"Question 1: What is the key factor for answering this question?\n",
    "                    Answer 1: [Factual answer based on the question context]\n",
    "                    Therefore, based on this reasoning, the answer follows logically.\n",
    "                    The answer is **No**.\"\"\"\n",
    "\n",
    "def parse_facts_field(facts_string: str) -> List[str]:\n",
    "    \"\"\"Parse the facts field from training data into a list of facts.\"\"\"\n",
    "    if not facts_string or not isinstance(facts_string, str):\n",
    "        return []\n",
    "    \n",
    "    # The facts field contains sentences that should be split\n",
    "    # Split by periods and clean up\n",
    "    sentences = [s.strip() for s in facts_string.split('.') if s.strip()]\n",
    "    \n",
    "    # Further split by common delimiters and clean\n",
    "    facts = []\n",
    "    for sentence in sentences:\n",
    "        # Split by semicolons or other sentence-like breaks\n",
    "        sub_facts = [s.strip() for s in sentence.replace(';', '.').split('.') if s.strip()]\n",
    "        facts.extend(sub_facts)\n",
    "    \n",
    "    # Filter out very short fragments\n",
    "    facts = [fact for fact in facts if len(fact.split()) >= 3]\n",
    "    \n",
    "    return facts\n",
    "\n",
    "# Legacy wrapper for backward compatibility\n",
    "def extract_yes_no(text: str) -> str:\n",
    "    \"\"\"Extract a yes/no answer using professional validation.\"\"\"\n",
    "    validator = TeacherResponseValidator()\n",
    "    result = validator.validate(text)\n",
    "\n",
    "    if result.is_valid() and result.metadata.get('extracted_answer'):\n",
    "        return result.metadata['extracted_answer'].capitalize()\n",
    "\n",
    "    # Enhanced extraction for Q&A-CoT format\n",
    "    qa_answer_match = re.search(r'The answer is \\*\\*(Yes|No)\\*\\*', text, re.IGNORECASE)\n",
    "    if qa_answer_match:\n",
    "        return qa_answer_match.group(1).capitalize()\n",
    "\n",
    "    # Fallback to original patterns\n",
    "    final_assessment_match = re.search(r'## Final Assessment.*?\\*\\*(YES|NO)\\*\\*', text, re.IGNORECASE | re.DOTALL)\n",
    "    if final_assessment_match:\n",
    "        return final_assessment_match.group(1).capitalize()\n",
    "\n",
    "    match = re.search(r\"\\b(yes|no)\\b\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).capitalize()\n",
    "\n",
    "    return \"No\"  # Default fallback\n",
    "\n",
    "# Get API key from environment\n",
    "if not OPENAI_API_KEY and not DRY_RUN:\n",
    "    print(\"Warning: OPENAI_API_KEY not set. Set DRY_RUN=True or provide an API key.\")\n",
    "\n",
    "print(\"=== ENHANCED Q&A-COT TEACHER GENERATION ===\")\n",
    "print(\"Implementing interleaved Q&A format from StrategyQA implementation plan...\")\n",
    "\n",
    "# Load training data directly (no more student drafts dependency)\n",
    "with open(os.path.join(parent_dir, SAMPLE_TRAIN_PATH), 'r', encoding='utf-8') as f:\n",
    "    training_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Loaded {len(training_data)} questions from training data\")\n",
    "\n",
    "# Process facts field instead of decomposition\n",
    "facts_available_count = 0\n",
    "for item in training_data:\n",
    "    facts_field = item.get('facts', '')\n",
    "    if facts_field:\n",
    "        facts_available_count += 1\n",
    "\n",
    "print(f\"Facts available for {facts_available_count} questions\")\n",
    "\n",
    "# Enhanced teacher response generation with Q&A-CoT format\n",
    "teacher_data = []\n",
    "teacher_validation_stats = {\n",
    "    'total': 0,\n",
    "    'valid': 0,\n",
    "    'corrected': 0,\n",
    "    'invalid': 0,\n",
    "    'high_confidence': 0,\n",
    "    'medium_confidence': 0,\n",
    "    'low_confidence': 0,\n",
    "    'successful_extractions': 0,\n",
    "    'failed_extractions': 0,\n",
    "    'qa_format_detected': 0,\n",
    "    'traditional_format_detected': 0,\n",
    "    'quality_scores': []\n",
    "}\n",
    "\n",
    "# Initialize teacher validator\n",
    "teacher_validator = TeacherResponseValidator()\n",
    "\n",
    "print(\"\\n=== GENERATING Q&A-COT TEACHER RESPONSES ===\")\n",
    "print(\"Using enhanced GPT-4 prompts for interleaved self-questioning format...\")\n",
    "\n",
    "for i, item in enumerate(training_data):\n",
    "    question = item['question']\n",
    "    answer = item['answer']\n",
    "    \n",
    "    # Parse facts field instead of decomposition\n",
    "    facts_string = item.get('facts', '')\n",
    "    facts_list = parse_facts_field(facts_string) if facts_string else None\n",
    "\n",
    "    if DRY_RUN:\n",
    "        # Use mock Q&A-CoT response for testing\n",
    "        response_text = f\"\"\"Question 1: What is the key factor for determining if this is true?\n",
    "                            Answer 1: [Mock factual answer based on the question context]\n",
    "                            Therefore, based on this analysis, we can determine the answer logically.\n",
    "                            The answer is **No**.\"\"\"\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            # Use enhanced Q&A-CoT generation with facts\n",
    "            response_text = call_gpt4_qa_cot(question, facts_list)\n",
    "\n",
    "            # Validate it follows Q&A format\n",
    "            if not re.search(r'Question\\s+\\d+\\s*:', response_text):\n",
    "                print(f\"Warning: Response for question {i+1} doesn't follow Q&A format, retrying...\")\n",
    "                # Retry once with more explicit instruction\n",
    "                retry_prompt = f\"STRICT FORMAT REQUIRED - Generate EXACTLY this format:\\n\\nQuestion 1: [question]\\nAnswer 1: [answer]\\nTherefore, [reasoning]\\nThe answer is **[YES/NO]**.\\n\\nQUESTION: {question}\"\n",
    "                response_text = call_gpt4_qa_cot(question, facts_list)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating Q&A-CoT response for question {i+1}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Validate Q&A format detection\n",
    "    has_qa_format = bool(re.search(r'Question\\s+\\d+\\s*:.*?Answer\\s+\\d+\\s*:', response_text, re.DOTALL))\n",
    "    if has_qa_format:\n",
    "        teacher_validation_stats['qa_format_detected'] += 1\n",
    "    else:\n",
    "        teacher_validation_stats['traditional_format_detected'] += 1\n",
    "\n",
    "    # Apply professional validation\n",
    "    validation_result = teacher_validator.validate(response_text)\n",
    "    teacher_validation_stats['total'] += 1\n",
    "    teacher_validation_stats['quality_scores'].append(validation_result.confidence_score)\n",
    "\n",
    "    # Update validation statistics\n",
    "    if validation_result.status == ValidationStatus.VALID:\n",
    "        teacher_validation_stats['valid'] += 1\n",
    "    elif validation_result.status == ValidationStatus.CORRECTED:\n",
    "        teacher_validation_stats['corrected'] += 1\n",
    "    else:\n",
    "        teacher_validation_stats['invalid'] += 1\n",
    "\n",
    "    # Confidence tiers\n",
    "    if validation_result.confidence_score >= HIGH_CONFIDENCE_THRESHOLD:\n",
    "        teacher_validation_stats['high_confidence'] += 1\n",
    "    elif validation_result.confidence_score >= MEDIUM_CONFIDENCE_THRESHOLD:\n",
    "        teacher_validation_stats['medium_confidence'] += 1\n",
    "    else:\n",
    "        teacher_validation_stats['low_confidence'] += 1\n",
    "\n",
    "    # Extract answer with enhanced Q&A patterns\n",
    "    teacher_answer = extract_yes_no(response_text)\n",
    "    if teacher_answer and teacher_answer.lower() in ['yes', 'no']:\n",
    "        teacher_validation_stats['successful_extractions'] += 1\n",
    "    else:\n",
    "        teacher_validation_stats['failed_extractions'] += 1\n",
    "\n",
    "    # Only keep medium and high confidence responses\n",
    "    if validation_result.confidence_score >= MEDIUM_CONFIDENCE_THRESHOLD:\n",
    "        final_response_text = validation_result.cleaned_text if validation_result.cleaned_text else response_text\n",
    "\n",
    "        out_record = {\n",
    "            'question': question,\n",
    "            'teacher_thought': final_response_text,\n",
    "            'teacher_answer': teacher_answer,\n",
    "            'format_type': 'qa_interleaved' if has_qa_format else 'traditional',\n",
    "            'facts': facts_list if facts_list else [],\n",
    "            'ground_truth_answer': answer,\n",
    "            'validation_metadata': {\n",
    "                'confidence_score': validation_result.confidence_score,\n",
    "                'status': validation_result.status.value,\n",
    "                'quality_metrics': validation_result.metadata.get('quality_metrics', {}),\n",
    "                'errors': validation_result.error_messages,\n",
    "                'qa_format_detected': has_qa_format\n",
    "            }\n",
    "        }\n",
    "        teacher_data.append(out_record)\n",
    "\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"Processed {i + 1}/{len(training_data)} teacher responses...\")\n",
    "\n",
    "# Calculate comprehensive statistics\n",
    "total_teacher_processed = teacher_validation_stats['total']\n",
    "if total_teacher_processed > 0:\n",
    "    teacher_success_rate = (teacher_validation_stats['valid'] + teacher_validation_stats['corrected']) / total_teacher_processed * 100\n",
    "    avg_teacher_confidence = sum(teacher_validation_stats['quality_scores']) / len(teacher_validation_stats['quality_scores'])\n",
    "    extraction_success_rate = teacher_validation_stats['successful_extractions'] / total_teacher_processed * 100\n",
    "    qa_format_rate = teacher_validation_stats['qa_format_detected'] / total_teacher_processed * 100\n",
    "\n",
    "    print(f\"\\n=== Q&A-COT TEACHER GENERATION RESULTS ===\")\n",
    "    print(f\"📊 Total processed: {total_teacher_processed}\")\n",
    "    print(f\"✅ Valid: {teacher_validation_stats['valid']} ({teacher_validation_stats['valid']/total_teacher_processed*100:.1f}%)\")\n",
    "    print(f\"🔧 Corrected: {teacher_validation_stats['corrected']} ({teacher_validation_stats['corrected']/total_teacher_processed*100:.1f}%)\")\n",
    "    print(f\"❌ Invalid: {teacher_validation_stats['invalid']} ({teacher_validation_stats['invalid']/total_teacher_processed*100:.1f}%)\")\n",
    "    print(f\"📈 Success Rate: {teacher_success_rate:.1f}% (Target: 80-85%)\")\n",
    "    print(f\"🎯 Average Confidence: {avg_teacher_confidence:.3f}\")\n",
    "    print(f\"🔍 Answer Extraction: {extraction_success_rate:.1f}%\")\n",
    "\n",
    "    print(f\"\\n=== Q&A FORMAT ADOPTION ===\")\n",
    "    print(f\"🧠 Q&A Interleaved Format: {teacher_validation_stats['qa_format_detected']} ({qa_format_rate:.1f}%)\")\n",
    "    print(f\"📝 Traditional Format: {teacher_validation_stats['traditional_format_detected']} ({100-qa_format_rate:.1f}%)\")\n",
    "\n",
    "    print(f\"\\n=== CONFIDENCE DISTRIBUTION ===\")\n",
    "    print(f\"🔥 High (≥0.8): {teacher_validation_stats['high_confidence']} ({teacher_validation_stats['high_confidence']/total_teacher_processed*100:.1f}%)\")\n",
    "    print(f\"🟡 Medium (0.5-0.8): {teacher_validation_stats['medium_confidence']} ({teacher_validation_stats['medium_confidence']/total_teacher_processed*100:.1f}%)\")\n",
    "    print(f\"🔴 Low (<0.5): {teacher_validation_stats['low_confidence']} ({teacher_validation_stats['low_confidence']/total_teacher_processed*100:.1f}%)\")\n",
    "\n",
    "    print(f\"\\n=== DATA RETENTION ===\")\n",
    "    print(f\"Kept after validation: {len(teacher_data)}/{total_teacher_processed} ({len(teacher_data)/total_teacher_processed*100:.1f}%)\")\n",
    "\n",
    "# Save enhanced Q&A-CoT teacher outputs\n",
    "qa_cot_teacher_path = os.path.join(parent_dir, TEACHER_OUTPUTS_PATH.replace('.jsonl', '_qa_cot.jsonl'))\n",
    "with open(qa_cot_teacher_path, 'w', encoding='utf-8') as f:\n",
    "    for record in teacher_data:\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(f\"\\n✅ Enhanced Q&A-CoT teacher outputs saved to {qa_cot_teacher_path}\")\n",
    "print(f\"🚀 Q&A-CoT teacher generation complete!\")\n",
    "\n",
    "if total_teacher_processed > 0:\n",
    "    if teacher_success_rate >= 80:\n",
    "        print(\"🎯 SUCCESS: Achieved target teacher validation rate of 80%+\")\n",
    "    else:\n",
    "        print(f\"⚠️  Below target: {80 - teacher_success_rate:.1f}pp improvement needed\")\n",
    "\n",
    "    if qa_format_rate >= 90:\n",
    "        print(\"🎯 SUCCESS: High adoption of Q&A interleaved format (90%+)\")\n",
    "    else:\n",
    "        print(f\"⚠️  Q&A format adoption at {qa_format_rate:.1f}%, may need prompt refinement\")\n",
    "\n",
    "print(f\"\\n=== Q&A-COT IMPLEMENTATION SUMMARY ===\")\n",
    "print(f\"🧠 Format Innovation: Interleaved Q&A self-questioning implemented\")\n",
    "print(f\"📚 Training Examples: {len(teacher_data)} high-quality Q&A-CoT responses\")\n",
    "print(f\"🎯 Average Quality: {avg_teacher_confidence:.3f} confidence score\")\n",
    "print(f\"🔍 Format Consistency: {qa_format_rate:.1f}% Q&A format adoption\")\n",
    "print(f\"📈 Data Quality: {teacher_success_rate:.1f}% validation success rate\")\n",
    "\n",
    "print(f\"\\n🏁 Ready for Phase B training with Q&A-CoT supervision!\")\n",
    "\n",
    "# Update teacher outputs path for Phase B pipeline\n",
    "TEACHER_OUTPUTS_PATH_QA_COT = qa_cot_teacher_path.replace(parent_dir + os.sep, '')\n",
    "print(f\"\\n🔧 Updated path for Phase B:\")\n",
    "print(f\"- TEACHER_OUTPUTS_PATH_QA_COT = '{TEACHER_OUTPUTS_PATH_QA_COT}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
