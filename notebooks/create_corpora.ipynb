{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a546771c",
   "metadata": {},
   "source": [
    "## Build Training Corpora\n",
    "\n",
    "Finally, we build two parallel training corpora:\n",
    "\n",
    "1. **Baseline (Track¬†A)** ‚Äì pairs of `(question ‚Üí answer)` for training a basic model.\n",
    "2. **CoT (Track¬†B)** ‚Äì pairs of `(question + teacher chain-of-thought ‚Üí answer)` for CoT distillation.\n",
    "\n",
    "These files will be used in later steps for model fine‚Äëtuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcee85e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Configuration ===\n",
      "Dataset: voidful/StrategyQA\n",
      "Model: microsoft/Phi-3.5-mini-instruct\n",
      "Batch size: 8\n",
      "4-bit quantization: True\n",
      "GPT-4 dry run: False\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Dataset parameters\n",
    "DATASET_NAME = os.getenv('DATASET_NAME', 'voidful/StrategyQA')\n",
    "TRAIN_SAMPLES = int(os.getenv('TRAIN_SAMPLES', '100'))\n",
    "RANDOM_SEED = int(os.getenv('RANDOM_SEED', '42'))\n",
    "USE_FULL_DATASET = os.getenv('USE_FULL_DATASET', 'False').lower() in ('true', '1', 't')\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "MODEL_NAME = os.getenv('MODEL_NAME', 'microsoft/phi-2')\n",
    "MAX_NEW_TOKENS = int(os.getenv('MAX_NEW_TOKENS', '35'))\n",
    "BATCH_SIZE = int(os.getenv('BATCH_SIZE', '8'))\n",
    "USE_4BIT = os.getenv('USE_4BIT', 'True').lower() in ('true', '1', 't')\n",
    "MAX_SEQ_LENGTH = int(os.getenv('MAX_SEQ_LENGTH', '512'))\n",
    "HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN', '')\n",
    "\n",
    "# Generation parameters\n",
    "DO_SAMPLE = os.getenv('DO_SAMPLE', 'False').lower() in ('true', '1', 't')\n",
    "TEMPERATURE = float(os.getenv('TEMPERATURE', '0.7'))\n",
    "\n",
    "# GPT-4 parameters\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')\n",
    "GPT4_MODEL = os.getenv('GPT4_MODEL', 'gpt-4')\n",
    "GPT4_MAX_TOKENS = int(os.getenv('GPT4_MAX_TOKENS', '150'))\n",
    "GPT4_TEMPERATURE = float(os.getenv('GPT4_TEMPERATURE', '0.3'))\n",
    "DRY_RUN = os.getenv('DRY_RUN', 'True').lower() in ('true', '1', 't')\n",
    "\n",
    "# Student Draft Generation\n",
    "STUDENT_MAX_TOKENS = int(os.getenv('STUDENT_MAX_TOKENS', '200'))\n",
    "STUDENT_TEMPERATURE = float(os.getenv('STUDENT_TEMPERATURE', '0.7'))\n",
    "STUDENT_BATCH_SIZE = int(os.getenv('STUDENT_BATCH_SIZE', '8'))\n",
    "\n",
    "# Enhanced Evaluation Generation\n",
    "EVAL_MAX_TOKENS = int(os.getenv('EVAL_MAX_TOKENS', '256'))\n",
    "EVAL_TEMPERATURE = float(os.getenv('EVAL_TEMPERATURE', '0.7'))\n",
    "EVAL_BATCH_SIZE = int(os.getenv('EVAL_BATCH_SIZE', '4'))\n",
    "\n",
    "# Quick Evaluation\n",
    "QUICK_EVAL_MAX_TOKENS = int(os.getenv('QUICK_EVAL_MAX_TOKENS', '5'))\n",
    "\n",
    "# Training Configuration\n",
    "# Phase A Training\n",
    "PHASE_A_EPOCHS = int(os.getenv('PHASE_A_EPOCHS', '3'))\n",
    "PHASE_A_BATCH_SIZE = int(os.getenv('PHASE_A_BATCH_SIZE', '1'))\n",
    "PHASE_A_LEARNING_RATE = float(os.getenv('PHASE_A_LEARNING_RATE', '1e-4'))\n",
    "PHASE_A_WARMUP_RATIO = float(os.getenv('PHASE_A_WARMUP_RATIO', '0.1'))\n",
    "PHASE_A_WEIGHT_DECAY = float(os.getenv('PHASE_A_WEIGHT_DECAY', '0.01'))\n",
    "\n",
    "# Phase B Training\n",
    "PHASE_B_EPOCHS = int(os.getenv('PHASE_B_EPOCHS', '3'))\n",
    "PHASE_B_BATCH_SIZE = int(os.getenv('PHASE_B_BATCH_SIZE', '4'))\n",
    "PHASE_B_LEARNING_RATE = float(os.getenv('PHASE_B_LEARNING_RATE', '5e-5'))\n",
    "PHASE_B_WARMUP_RATIO = float(os.getenv('PHASE_B_WARMUP_RATIO', '0.1'))\n",
    "PHASE_B_WEIGHT_DECAY = float(os.getenv('PHASE_B_WEIGHT_DECAY', '0.01'))\n",
    "\n",
    "# Progressive Curriculum Training\n",
    "CURRICULUM_STAGE1_EPOCHS = int(os.getenv('CURRICULUM_STAGE1_EPOCHS', '1'))\n",
    "CURRICULUM_STAGE1_LEARNING_RATE = float(os.getenv('CURRICULUM_STAGE1_LEARNING_RATE', '5e-5'))\n",
    "CURRICULUM_STAGE1_WARMUP_RATIO = float(os.getenv('CURRICULUM_STAGE1_WARMUP_RATIO', '0.1'))\n",
    "CURRICULUM_STAGE1_WEIGHT_DECAY = float(os.getenv('CURRICULUM_STAGE1_WEIGHT_DECAY', '0.01'))\n",
    "\n",
    "CURRICULUM_STAGE2_EPOCHS = int(os.getenv('CURRICULUM_STAGE2_EPOCHS', '2'))\n",
    "CURRICULUM_STAGE2_LEARNING_RATE = float(os.getenv('CURRICULUM_STAGE2_LEARNING_RATE', '3e-5'))\n",
    "CURRICULUM_STAGE2_WARMUP_RATIO = float(os.getenv('CURRICULUM_STAGE2_WARMUP_RATIO', '0.1'))\n",
    "CURRICULUM_STAGE2_WEIGHT_DECAY = float(os.getenv('CURRICULUM_STAGE2_WEIGHT_DECAY', '0.01'))\n",
    "\n",
    "# Validation Configuration\n",
    "HIGH_CONFIDENCE_THRESHOLD = float(os.getenv('HIGH_CONFIDENCE_THRESHOLD', '0.8'))\n",
    "MEDIUM_CONFIDENCE_THRESHOLD = float(os.getenv('MEDIUM_CONFIDENCE_THRESHOLD', '0.5'))\n",
    "LOW_CONFIDENCE_THRESHOLD = float(os.getenv('LOW_CONFIDENCE_THRESHOLD', '0.3'))\n",
    "VALIDATION_ACCEPTANCE_THRESHOLD = float(os.getenv('VALIDATION_ACCEPTANCE_THRESHOLD', '0.3'))\n",
    "\n",
    "# Quality Thresholds\n",
    "VALID_QUALITY_THRESHOLD = float(os.getenv('VALID_QUALITY_THRESHOLD', '0.5'))\n",
    "CORRECTED_QUALITY_THRESHOLD = float(os.getenv('CORRECTED_QUALITY_THRESHOLD', '0.3'))\n",
    "\n",
    "# Token Emphasis Configuration\n",
    "EMPHASIS_MULTIPLIER = float(os.getenv('EMPHASIS_MULTIPLIER', '2.5'))\n",
    "ADAPTIVE_EMPHASIS = os.getenv('ADAPTIVE_EMPHASIS', 'True').lower() in ('true', '1', 't')\n",
    "\n",
    "# Memory and Performance\n",
    "GRADIENT_CHECKPOINTING = os.getenv('GRADIENT_CHECKPOINTING', 'True').lower() in ('true', '1', 't')\n",
    "FP16 = os.getenv('FP16', 'False').lower() in ('true', '1', 't')\n",
    "BF16 = os.getenv('BF16', 'True').lower() in ('true', '1', 't')\n",
    "DATALOADER_NUM_WORKERS = int(os.getenv('DATALOADER_NUM_WORKERS', '0'))\n",
    "DATALOADER_PERSISTENT_WORKERS = os.getenv('DATALOADER_PERSISTENT_WORKERS', 'False').lower() in ('true', '1', 't')\n",
    "SKIP_MEMORY_METRICS = os.getenv('SKIP_MEMORY_METRICS', 'True').lower() in ('true', '1', 't')\n",
    "\n",
    "# Logging and Monitoring\n",
    "LOGGING_STEPS = int(os.getenv('LOGGING_STEPS', '10'))\n",
    "SAVE_STRATEGY = os.getenv('SAVE_STRATEGY', 'epoch')\n",
    "REPORT_TO = os.getenv('REPORT_TO', 'none')\n",
    "LOAD_BEST_MODEL_AT_END = os.getenv('LOAD_BEST_MODEL_AT_END', 'False').lower() in ('true', '1', 't')\n",
    "\n",
    "# Evaluation Configuration\n",
    "EVAL_SIZE = int(os.getenv('EVAL_SIZE', '100'))\n",
    "ENHANCED_EVAL_BATCH_SIZE = int(os.getenv('ENHANCED_EVAL_BATCH_SIZE', '4'))\n",
    "# File paths\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(parent_dir, os.getenv(\"DATA_DIR\", \"data\"))\n",
    "RAW_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "STUDENT_DIR = os.path.join(DATA_DIR, 'student')\n",
    "TEACHER_DIR = os.path.join(DATA_DIR, 'teacher')\n",
    "SAMPLE_TRAIN_PATH = os.path.join(TRAIN_DIR, 'sample_train.jsonl')\n",
    "STUDENT_DRAFTS_PATH = os.path.join(STUDENT_DIR, 'student_drafts.jsonl')\n",
    "CLEANED_STUDENT_DRAFTS_PATH = os.path.join(STUDENT_DIR, 'cleaned_student_drafts.jsonl')\n",
    "TEACHER_OUTPUTS_PATH = os.path.join(TEACHER_DIR, 'teacher_outputs.jsonl')\n",
    "BASELINE_PATH = os.path.join(TRAIN_DIR, 'train_baseline.jsonl')\n",
    "COT_PATH = os.path.join(TRAIN_DIR, 'train_cot.jsonl')\n",
    "COT_PATH_QA_COT = os.path.join(TEACHER_DIR, 'teacher_outputs_qa_cot.jsonl')\n",
    "SAMPLE_TEST_PATH = SAMPLE_TRAIN_PATH  # Alias for consistency with Build Training Corpora cell\n",
    "STAGE1_PATH = os.path.join(TRAIN_DIR, 'stage1_train.jsonl.jsonl')\n",
    "STAGE2_PATH = os.path.join(TRAIN_DIR,'stage2_train.jsonl')\n",
    "\n",
    "\n",
    "# Print configuration\n",
    "print(\"=== Configuration ===\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"4-bit quantization: {USE_4BIT}\")\n",
    "print(f\"GPT-4 dry run: {DRY_RUN}\")\n",
    "print(\"==\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd335fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading teacher data for Progressive Curriculum Training...\n",
      "üìä Loaded 200 teacher responses\n",
      "üéØ Loaded 200 ground truth answers\n",
      "Processed 100 responses...\n",
      "Processed 200 responses...\n",
      "\n",
      "=== PROGRESSIVE CURRICULUM VALIDATION RESULTS ===\n",
      "üìä Total processed: 200\n",
      "‚úÖ Data kept: 200 (100.0%)\n",
      "üéØ Average confidence: 1.000\n",
      "\n",
      "=== DATASET GENERATION RESULTS ===\n",
      "üìù Baseline dataset: 200 examples\n",
      "üéØ Stage 1 dataset (Final Reasoning): 200 examples\n",
      "üß† Stage 2 dataset (Complete Q&A): 200 examples\n",
      "\n",
      "=== FORMAT PROCESSING RESULTS ===\n",
      "üß† Q&A-CoT format processed: 200 (100.0%)\n",
      "üìù Traditional format processed: 0 (0.0%)\n",
      "üìã Template fallbacks used: 0 (0.0%)\n",
      "\n",
      "=== CONFIDENCE TIER DISTRIBUTION ===\n",
      "üî• High (‚â•0.8): 200 (100.0%)\n",
      "üü° Medium (0.5-0.8): 0 (0.0%)\n",
      "üî¥ Low (<0.5): 0 (0.0%)\n",
      "\n",
      "=== ANSWER VALIDATION ===\n",
      "‚úÖ Valid teacher answers: 200\n",
      "‚ùå Invalid teacher answers: 0\n",
      "ü§ù Teacher-ground truth agreement: 91.5%\n",
      "\n",
      "=== ERROR ANALYSIS ===\n",
      "üîß Correction attempts: 0\n",
      "‚úÖ Successful corrections: 0\n",
      "\n",
      "=== DISTRIBUTION ANALYSIS (PRE-BALANCING) ===\n",
      "Baseline: Yes=90 (45.0%), No=110 (55.0%)\n",
      "Stage 1: Yes=90 (45.0%), No=110 (55.0%)\n",
      "Stage 2: Yes=90 (45.0%), No=110 (55.0%)\n",
      "\n",
      "=== APPLYING SMART CLASS BALANCING (15% THRESHOLD) ===\n",
      "Original distribution: Yes=90, No=110\n",
      "Imbalance ratio: 0.050 (threshold: 0.150)\n",
      "Imbalance within acceptable threshold, skipping oversampling\n",
      "Original distribution: Yes=90, No=110\n",
      "Imbalance ratio: 0.050 (threshold: 0.150)\n",
      "Imbalance within acceptable threshold, skipping oversampling\n",
      "Original distribution: Yes=90, No=110\n",
      "Imbalance ratio: 0.050 (threshold: 0.150)\n",
      "Imbalance within acceptable threshold, skipping oversampling\n",
      "\n",
      "=== DISTRIBUTION ANALYSIS (POST-BALANCING) ===\n",
      "Baseline: Yes=90 (45.0%), No=110 (55.0%)\n",
      "Stage 1: Yes=90 (45.0%), No=110 (55.0%)\n",
      "Stage 2: Yes=90 (45.0%), No=110 (55.0%)\n",
      "\n",
      "üöÄ Saving Progressive Curriculum Training datasets...\n",
      "üíæ Saved 200 baseline records to: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\train\\train_baseline.jsonl\n",
      "üéØ Saved 200 Stage 1 records to: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\train\\stage1_train.jsonl.jsonl\n",
      "üß† Saved 200 Stage 2 records to: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\train\\stage2_train.jsonl\n",
      "\n",
      "=== PROGRESSIVE CURRICULUM TRAINING DATA GENERATION SUMMARY ===\n",
      "üöÄ Pipeline Status: ‚úÖ COMPLETE\n",
      "üéØ Stage 1 Focus: Final reasoning after 'Therefore' phrase\n",
      "üß† Stage 2 Focus: Complete Q&A with final reasoning\n",
      "üìä Data Quality: 200 high/medium confidence examples\n",
      "‚öñÔ∏è  Class Balance: Smart oversampling (15% threshold)\n",
      "üéØ Confidence Score: 1.000 average\n",
      "üîç Validation Rate: 100.0% data retention\n",
      "üß† Q&A-CoT Adoption: 100.0% of training data uses interleaved Q&A format\n",
      "üéØ SUCCESS: Achieved high-confidence Progressive Curriculum pipeline\n",
      "üéØ SUCCESS: High Q&A-CoT format adoption (80%+)\n",
      "\n",
      "üèÅ Ready for Progressive Curriculum Training!\n",
      "\n",
      "üìÇ Updated training paths:\n",
      "- BASELINE_PATH_ENHANCED = 'c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\train\\train_baseline.jsonl'\n",
      "- STAGE1_PATH_ENHANCED = 'c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\train\\stage1_train.jsonl.jsonl'\n",
      "- STAGE2_PATH_ENHANCED = 'c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\train\\stage2_train.jsonl'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import re\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class ValidationStatus(Enum):\n",
    "    VALID = \"valid\"\n",
    "    CORRECTED = \"corrected\" \n",
    "    INVALID = \"invalid\"\n",
    "\n",
    "@dataclass\n",
    "class ValidationResult:\n",
    "    status: ValidationStatus\n",
    "    original_text: str\n",
    "    cleaned_text: Optional[str]\n",
    "    confidence_score: float  # 0.0 - 1.0\n",
    "    error_messages: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "    def is_valid(self) -> bool:\n",
    "        return self.status in [ValidationStatus.VALID, ValidationStatus.CORRECTED]\n",
    "\n",
    "\n",
    "class ResponseValidationPipeline:\n",
    "    \"\"\"Professional validation pipeline with multi-layered validation and confidence scoring.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize pipeline with professional standards.\"\"\"\n",
    "        self.teacher_validator = TeacherResponseValidator()\n",
    "        self.HIGH_CONFIDENCE_THRESHOLD = 0.8\n",
    "        self.MEDIUM_CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "    def process_responses_for_progressive_curriculum(self, teacher_data: List[Dict], ground_truth_map: Dict[str, bool]) -> Tuple[List[Dict], List[Dict], List[Dict], Dict[str, Any]]:\n",
    "        \"\"\"Process teacher responses for Progressive Curriculum Training (Stage 1 & Stage 2).\"\"\"\n",
    "\n",
    "        # Initialize comprehensive statistics\n",
    "        validation_stats = {\n",
    "            'total_processed': 0,\n",
    "            'ground_truth_missing': 0,\n",
    "            'data_kept': 0,\n",
    "\n",
    "            # Validation tiers\n",
    "            'high_confidence': 0,    # >= 0.8\n",
    "            'medium_confidence': 0,  # 0.5 - 0.8\n",
    "            'low_confidence': 0,     # < 0.5\n",
    "\n",
    "            # Answer validation\n",
    "            'valid_teacher_answers': 0,\n",
    "            'invalid_teacher_answers': 0,\n",
    "            'teacher_agrees': 0,\n",
    "            'teacher_disagrees': 0,\n",
    "\n",
    "            # Q&A-CoT format tracking\n",
    "            'qa_cot_format': 0,\n",
    "            'traditional_format': 0,\n",
    "\n",
    "            # Error tracking \n",
    "            'validation_errors': Counter(),\n",
    "            'correction_attempts': 0,\n",
    "            'successful_corrections': 0,\n",
    "            'confidence_scores': [],\n",
    "            \n",
    "            # Template fallback tracking\n",
    "            'template_fallbacks': 0,  # Track samples that used template fallback\n",
    "        }\n",
    "\n",
    "        baseline_records = []\n",
    "        stage1_records = []  # Final reasoning only\n",
    "        stage2_records = []  # Complete Q&A with final reasoning\n",
    "        HIGH_CONFIDENCE_THRESHOLD = self.HIGH_CONFIDENCE_THRESHOLD\n",
    "\n",
    "        for rec in teacher_data:\n",
    "            q = rec['question']\n",
    "            thought = rec['teacher_thought']\n",
    "            teacher_answer = rec['teacher_answer']\n",
    "            format_type = rec.get('format_type', 'unknown')\n",
    "\n",
    "            validation_stats['total_processed'] += 1\n",
    "\n",
    "            # Track format types\n",
    "            if format_type == 'qa_interleaved':\n",
    "                validation_stats['qa_cot_format'] += 1\n",
    "            else:\n",
    "                validation_stats['traditional_format'] += 1\n",
    "\n",
    "            # Check ground truth availability\n",
    "            if q not in ground_truth_map:\n",
    "                validation_stats['ground_truth_missing'] += 1\n",
    "                continue\n",
    "\n",
    "            ground_truth_answer = self._convert_to_yes_no(ground_truth_map[q])\n",
    "\n",
    "            # Use existing validation metadata if available\n",
    "            if 'validation_metadata' in rec:\n",
    "                validation_result = self._create_validation_result_from_metadata(rec['validation_metadata'], thought)\n",
    "            else:\n",
    "                # Apply professional validation\n",
    "                validation_result = self.teacher_validator.validate(thought)\n",
    "\n",
    "            validation_stats['confidence_scores'].append(validation_result.confidence_score)\n",
    "\n",
    "            # Track validation status\n",
    "            if validation_result.status == ValidationStatus.VALID:\n",
    "                validation_stats['high_confidence'] += 1 if validation_result.confidence_score >= HIGH_CONFIDENCE_THRESHOLD else 0\n",
    "                validation_stats['medium_confidence'] += 1 if 0.5 <= validation_result.confidence_score < 0.8 else 0\n",
    "            elif validation_result.status == ValidationStatus.CORRECTED:\n",
    "                validation_stats['successful_corrections'] += 1\n",
    "                validation_stats['medium_confidence'] += 1\n",
    "            else:\n",
    "                validation_stats['low_confidence'] += 1\n",
    "\n",
    "            # Track errors\n",
    "            for error in validation_result.error_messages:\n",
    "                validation_stats['validation_errors'][error] += 1\n",
    "\n",
    "            # Validate teacher answer format\n",
    "            if not self._is_valid_answer(teacher_answer):\n",
    "                validation_stats['invalid_teacher_answers'] += 1\n",
    "                continue\n",
    "            else:\n",
    "                validation_stats['valid_teacher_answers'] += 1\n",
    "\n",
    "            # Track teacher-ground truth agreement\n",
    "            teacher_answer_clean = teacher_answer.strip().capitalize()\n",
    "            if teacher_answer_clean == ground_truth_answer:\n",
    "                validation_stats['teacher_agrees'] += 1\n",
    "            else:\n",
    "                validation_stats['teacher_disagrees'] += 1\n",
    "\n",
    "            # Confidence-based processing decision\n",
    "            confidence_tier = self._get_confidence_tier(validation_result.confidence_score)\n",
    "\n",
    "            if confidence_tier in ['high', 'medium']:\n",
    "                # Accept high and medium confidence responses\n",
    "                validation_stats['data_kept'] += 1\n",
    "\n",
    "                # Use validated response if available\n",
    "                final_thought = validation_result.cleaned_text if validation_result.cleaned_text else thought\n",
    "\n",
    "                # Create baseline record (question ‚Üí teacher_answer)\n",
    "                baseline_record = {\n",
    "                    'prompt': q,\n",
    "                    'answer': teacher_answer_clean,\n",
    "                    'validation_metadata': {\n",
    "                        'confidence_score': validation_result.confidence_score,\n",
    "                        'validation_status': validation_result.status.value,\n",
    "                        'confidence_tier': confidence_tier,\n",
    "                        'format_type': format_type\n",
    "                    }\n",
    "                }\n",
    "                baseline_records.append(baseline_record)\n",
    "\n",
    "                # Create Progressive Curriculum datasets - CRITICAL FIX: Pass teacher_answer_clean instead of teacher_answer\n",
    "                stage1_prompt, stage1_answer = self._create_stage1_dataset_entry(q, final_thought, teacher_answer_clean)\n",
    "                stage2_prompt, stage2_answer = self._create_stage2_dataset_entry(q, final_thought, teacher_answer_clean)\n",
    "\n",
    "                # Stage 1: Final reasoning only\n",
    "                stage1_record = {\n",
    "                    'prompt': stage1_prompt,\n",
    "                    'answer': stage1_answer,\n",
    "                    'stage': 'stage_1',\n",
    "                    'original_teacher_answer': teacher_answer_clean,  # Track original answer for debugging\n",
    "                    'validation_metadata': {\n",
    "                        'confidence_score': validation_result.confidence_score,\n",
    "                        'validation_status': validation_result.status.value,\n",
    "                        'confidence_tier': confidence_tier,\n",
    "                        'format_type': format_type\n",
    "                    }\n",
    "                }\n",
    "                stage1_records.append(stage1_record)\n",
    "\n",
    "                # Stage 2: Complete Q&A with final reasoning\n",
    "                stage2_record = {\n",
    "                    'prompt': stage2_prompt,\n",
    "                    'answer': stage2_answer,\n",
    "                    'stage': 'stage_2',\n",
    "                    'original_teacher_answer': teacher_answer_clean,  # Track original answer for debugging\n",
    "                    'validation_metadata': {\n",
    "                        'confidence_score': validation_result.confidence_score,\n",
    "                        'validation_status': validation_result.status.value,\n",
    "                        'confidence_tier': confidence_tier,\n",
    "                        'format_type': format_type\n",
    "                    }\n",
    "                }\n",
    "                stage2_records.append(stage2_record)\n",
    "\n",
    "            # Progress reporting\n",
    "            if validation_stats['total_processed'] % 100 == 0:\n",
    "                print(f\"Processed {validation_stats['total_processed']} responses...\")\n",
    "\n",
    "        # Calculate final statistics\n",
    "        self._calculate_final_stats(validation_stats)\n",
    "\n",
    "        return baseline_records, stage1_records, stage2_records, validation_stats\n",
    "\n",
    "    def _create_stage1_dataset_entry(self, question: str, teacher_thought: str, teacher_answer: str) -> Tuple[str, str]:\n",
    "        \"\"\"Create Stage 1 entry: Focus on final reasoning after 'Therefore'.\"\"\"\n",
    "        \n",
    "        # Extract final reasoning (everything after \"Therefore\")\n",
    "        therefore_match = re.search(r'Therefore[,:]\\s*(.*?)(?=The answer is|$)', teacher_thought, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        if therefore_match:\n",
    "            final_reasoning = therefore_match.group(1).strip()\n",
    "        else:\n",
    "            # Fallback: use the last paragraph or sentence if no \"Therefore\" found\n",
    "            sentences = teacher_thought.split('.')\n",
    "            final_reasoning = sentences[-2].strip() if len(sentences) > 1 else teacher_thought.strip()\n",
    "        \n",
    "        # Create Stage 1 prompt (question only)\n",
    "        stage1_prompt = f\"Question: {question}\"\n",
    "        \n",
    "        # Create Stage 1 answer (final reasoning + answer) - PRESERVE original teacher_answer\n",
    "        stage1_answer = f\"{final_reasoning}. Therefore, the answer is **{teacher_answer}**.\"\n",
    "        \n",
    "        return stage1_prompt, stage1_answer\n",
    "\n",
    "    def _create_stage2_dataset_entry(self, question: str, teacher_thought: str, teacher_answer: str) -> Tuple[str, str]:\n",
    "        \"\"\"Create Stage 2 entry: Complete Q&A with final reasoning.\"\"\"\n",
    "        \n",
    "        # Create Stage 2 prompt (question only)\n",
    "        stage2_prompt = f\"Question: {question}\"\n",
    "        \n",
    "        # Create Stage 2 answer (complete teacher reasoning + final answer) - PRESERVE original teacher_answer\n",
    "        # Remove any existing \"The answer is\" conclusion to avoid duplication\n",
    "        clean_thought = re.sub(r'The answer is.*$', '', teacher_thought, flags=re.IGNORECASE).strip()\n",
    "        \n",
    "        stage2_answer = f\"{clean_thought}\\n\\nTherefore, the final answer is **{teacher_answer}**.\"\n",
    "        \n",
    "        return stage2_prompt, stage2_answer\n",
    "\n",
    "    def _create_validation_result_from_metadata(self, metadata: Dict[str, Any], original_text: str) -> 'ValidationResult':\n",
    "        \"\"\"Create ValidationResult from existing metadata.\"\"\"\n",
    "        return ValidationResult(\n",
    "            status=ValidationStatus(metadata.get('status', 'valid')),\n",
    "            original_text=original_text,\n",
    "            cleaned_text=original_text,  # Already processed\n",
    "            confidence_score=metadata.get('confidence_score', 0.5),\n",
    "            error_messages=metadata.get('errors', []),\n",
    "            metadata=metadata\n",
    "        )\n",
    "\n",
    "    def _create_qa_cot_training_prompt(self, question: str, teacher_thought: str) -> tuple:\n",
    "        \"\"\"Create Q&A-CoT training prompt using actual teacher reasoning.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (prompt_text, used_template_flag)\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract the Q&A structure from teacher thought\n",
    "        qa_match = re.search(r'(Question\\s+\\d+:.*?Answer\\s+\\d+:.*?)(?=Therefore|The answer is|$)', teacher_thought, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "        if qa_match:\n",
    "            qa_content = qa_match.group(1).strip()\n",
    "            \n",
    "            # Extract the conclusion part (Therefore...) from teacher_thought\n",
    "            conclusion_match = re.search(r'(Therefore.*?)(?=The answer is|$)', teacher_thought, re.DOTALL | re.IGNORECASE)\n",
    "            conclusion = conclusion_match.group(1).strip() if conclusion_match else \"Therefore, based on the analysis above\"\n",
    "            \n",
    "            # Create training prompt with actual teacher reasoning\n",
    "            prompt = f\"\"\"Question: {question}\n",
    "\n",
    "                    {qa_content}\n",
    "                    {conclusion}\"\"\"\n",
    "            \n",
    "            return prompt, False  # False = did not use template\n",
    "            \n",
    "        else:\n",
    "            # Fallback template if Q&A structure not found\n",
    "            prompt = f\"\"\"Question: {question}\n",
    "\n",
    "                        Question 1: [Ask a clarifying question about this topic]\n",
    "                        Answer 1: [Provide factual information]\n",
    "                        Therefore, [conclude based on the analysis]\"\"\"\n",
    "            \n",
    "            return prompt, True  # True = used template fallback\n",
    "\n",
    "    def _get_confidence_tier(self, score: float) -> str:\n",
    "        \"\"\"Classify confidence score into tier.\"\"\"\n",
    "        if score >= self.HIGH_CONFIDENCE_THRESHOLD:\n",
    "            return 'high'\n",
    "        elif score >= self.MEDIUM_CONFIDENCE_THRESHOLD:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'low'\n",
    "\n",
    "    def _create_enhanced_cot_prompt(self, question: str, draft: str, teacher_thought: str) -> str:\n",
    "        \"\"\"Create enhanced CoT prompt for traditional format.\"\"\"\n",
    "        return f\"\"\"Question: {question}\n",
    "\n",
    "                    Draft: {draft}\n",
    "\n",
    "                    Reasoning: {teacher_thought}\"\"\"\n",
    "\n",
    "    def _is_valid_answer(self, answer: str) -> bool:\n",
    "        \"\"\"Check if answer is valid yes/no format.\"\"\"\n",
    "        if not answer or not isinstance(answer, str):\n",
    "            return False\n",
    "        cleaned = answer.strip().lower()\n",
    "        return cleaned in ['yes', 'no']\n",
    "\n",
    "    def _convert_to_yes_no(self, ground_truth: bool) -> str:\n",
    "        \"\"\"Convert boolean ground truth to Yes/No string.\"\"\"\n",
    "        return \"Yes\" if ground_truth else \"No\"\n",
    "\n",
    "    def _calculate_final_stats(self, validation_stats: Dict[str, Any]) -> None:\n",
    "        \"\"\"Calculate final statistics.\"\"\"\n",
    "        total_processed = validation_stats['total_processed']\n",
    "        \n",
    "        if total_processed > 0:\n",
    "            # Calculate agreement rate\n",
    "            total_agreements = validation_stats['teacher_agrees'] + validation_stats['teacher_disagrees']\n",
    "            if total_agreements > 0:\n",
    "                validation_stats['teacher_agreement_rate'] = (validation_stats['teacher_agrees'] / total_agreements) * 100\n",
    "\n",
    "            # Calculate average confidence\n",
    "            if validation_stats['confidence_scores']:\n",
    "                validation_stats['avg_confidence'] = sum(validation_stats['confidence_scores']) / len(validation_stats['confidence_scores'])\n",
    "            else:\n",
    "                validation_stats['avg_confidence'] = 0.0\n",
    "\n",
    "\n",
    "class TeacherResponseValidator:\n",
    "    \"\"\"Validates teacher responses for quality and consistency.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Quality thresholds\n",
    "        self.MIN_LENGTH = 50\n",
    "        self.MAX_LENGTH = 2000\n",
    "        self.REQUIRED_PATTERNS = [\n",
    "            r'question\\s+\\d+:', r'answer\\s+\\d+:', r'therefore'\n",
    "        ]\n",
    "    \n",
    "    def validate(self, text: str) -> ValidationResult:\n",
    "        \"\"\"Comprehensive validation of teacher response.\"\"\"\n",
    "        errors = []\n",
    "        confidence_score = 1.0\n",
    "        \n",
    "        # Length check\n",
    "        if len(text) < self.MIN_LENGTH:\n",
    "            errors.append(\"Response too short\")\n",
    "            confidence_score *= 0.7\n",
    "        elif len(text) > self.MAX_LENGTH:\n",
    "            errors.append(\"Response too long\")\n",
    "            confidence_score *= 0.9\n",
    "            \n",
    "        # Pattern checks\n",
    "        patterns_found = 0\n",
    "        for pattern in self.REQUIRED_PATTERNS:\n",
    "            if re.search(pattern, text, re.IGNORECASE):\n",
    "                patterns_found += 1\n",
    "        \n",
    "        pattern_ratio = patterns_found / len(self.REQUIRED_PATTERNS)\n",
    "        confidence_score *= pattern_ratio\n",
    "        \n",
    "        if pattern_ratio < 0.5:\n",
    "            errors.append(\"Missing required Q&A structure\")\n",
    "            \n",
    "        # Determine status\n",
    "        if confidence_score >= 0.8:\n",
    "            status = ValidationStatus.VALID\n",
    "        elif confidence_score >= 0.5:\n",
    "            status = ValidationStatus.CORRECTED\n",
    "        else:\n",
    "            status = ValidationStatus.INVALID\n",
    "            \n",
    "        return ValidationResult(\n",
    "            status=status,\n",
    "            original_text=text,\n",
    "            cleaned_text=text,  # Could add cleaning logic here\n",
    "            confidence_score=confidence_score,\n",
    "            error_messages=errors,\n",
    "            metadata={'pattern_ratio': pattern_ratio}\n",
    "        )\n",
    "\n",
    "\n",
    "def analyze_distribution(records: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze answer distribution in dataset based on original_teacher_answer field.\"\"\"\n",
    "    if not records:\n",
    "        return {'yes_count': 0, 'no_count': 0, 'yes_percent': 0, 'no_percent': 0}\n",
    "    \n",
    "    # For Stage 1 and Stage 2, look at original_teacher_answer field if available\n",
    "    yes_count = 0\n",
    "    for r in records:\n",
    "        if 'original_teacher_answer' in r:\n",
    "            answer = r['original_teacher_answer'].lower()\n",
    "        else:\n",
    "            answer = r['answer'].lower()\n",
    "        \n",
    "        if 'yes' in answer:\n",
    "            yes_count += 1\n",
    "    \n",
    "    no_count = len(records) - yes_count\n",
    "    \n",
    "    return {\n",
    "        'yes_count': yes_count,\n",
    "        'no_count': no_count,\n",
    "        'yes_percent': (yes_count / len(records)) * 100,\n",
    "        'no_percent': (no_count / len(records)) * 100\n",
    "    }\n",
    "\n",
    "\n",
    "def balance_dataset_with_oversampling(records: List[Dict], target_ratio: float = 0.5, max_samples: Optional[int] = None, imbalance_threshold: float = 0.15) -> List[Dict]:\n",
    "    \"\"\"Balance dataset by oversampling the minority class only if imbalance > threshold.\"\"\"\n",
    "    if not records:\n",
    "        return records\n",
    "    \n",
    "    # Separate by answer based on original_teacher_answer if available\n",
    "    yes_records = []\n",
    "    no_records = []\n",
    "    \n",
    "    for r in records:\n",
    "        if 'original_teacher_answer' in r:\n",
    "            answer = r['original_teacher_answer'].lower()\n",
    "        else:\n",
    "            answer = r['answer'].lower()\n",
    "        \n",
    "        if 'yes' in answer:\n",
    "            yes_records.append(r)\n",
    "        else:\n",
    "            no_records.append(r)\n",
    "    \n",
    "    total_records = len(yes_records) + len(no_records)\n",
    "    if total_records == 0:\n",
    "        print(\"No valid records found for balancing\")\n",
    "        return records\n",
    "    \n",
    "    print(f\"Original distribution: Yes={len(yes_records)}, No={len(no_records)}\")\n",
    "    \n",
    "    # Calculate imbalance ratio\n",
    "    yes_ratio = len(yes_records) / total_records\n",
    "    no_ratio = len(no_records) / total_records\n",
    "    imbalance = abs(yes_ratio - 0.5)\n",
    "    \n",
    "    print(f\"Imbalance ratio: {imbalance:.3f} (threshold: {imbalance_threshold:.3f})\")\n",
    "    \n",
    "    # Only apply oversampling if imbalance > threshold\n",
    "    if imbalance <= imbalance_threshold:\n",
    "        print(\"Imbalance within acceptable threshold, skipping oversampling\")\n",
    "        if max_samples and len(records) > max_samples:\n",
    "            # Just sample down to max_samples if no balancing needed\n",
    "            random.shuffle(records)\n",
    "            return records[:max_samples]\n",
    "        return records\n",
    "    \n",
    "    # Apply oversampling for significant imbalance\n",
    "    print(\"Applying oversampling due to significant class imbalance\")\n",
    "    \n",
    "    # Determine majority and minority classes\n",
    "    if len(yes_records) > len(no_records):\n",
    "        majority_records = yes_records\n",
    "        minority_records = no_records\n",
    "        majority_class = 'yes'\n",
    "    else:\n",
    "        majority_records = no_records\n",
    "        minority_records = yes_records\n",
    "        majority_class = 'no'\n",
    "    \n",
    "    # Calculate target size for balanced dataset\n",
    "    if max_samples:\n",
    "        # If max_samples specified, use it as the total target\n",
    "        total_target = max_samples\n",
    "        target_per_class = total_target // 2\n",
    "        \n",
    "        # Sample majority class down to target\n",
    "        random.shuffle(majority_records)\n",
    "        majority_sampled = majority_records[:target_per_class]\n",
    "        \n",
    "        # Oversample minority class to match\n",
    "        minority_oversampled = []\n",
    "        if len(minority_records) > 0:  # Prevent infinite loop\n",
    "            cycles = target_per_class // len(minority_records)\n",
    "            remainder = target_per_class % len(minority_records)\n",
    "            \n",
    "            # Add complete cycles\n",
    "            for _ in range(cycles):\n",
    "                minority_oversampled.extend(minority_records[:])\n",
    "            \n",
    "            # Add remainder\n",
    "            if remainder > 0:\n",
    "                random.shuffle(minority_records)\n",
    "                minority_oversampled.extend(minority_records[:remainder])\n",
    "        else:\n",
    "            print(\"WARNING: No minority class records found, cannot oversample\")\n",
    "            minority_oversampled = []\n",
    "    else:\n",
    "        # Use majority class size as target for both classes\n",
    "        target_per_class = len(majority_records)\n",
    "        majority_sampled = majority_records[:]\n",
    "        \n",
    "        # Oversample minority class to match majority class size\n",
    "        minority_oversampled = []\n",
    "        if len(minority_records) > 0:  # Prevent infinite loop\n",
    "            cycles = target_per_class // len(minority_records)\n",
    "            remainder = target_per_class % len(minority_records)\n",
    "            \n",
    "            # Add complete cycles\n",
    "            for _ in range(cycles):\n",
    "                minority_oversampled.extend(minority_records[:])\n",
    "            \n",
    "            # Add remainder\n",
    "            if remainder > 0:\n",
    "                random.shuffle(minority_records)\n",
    "                minority_oversampled.extend(minority_records[:remainder])\n",
    "        else:\n",
    "            print(\"WARNING: No minority class records found, cannot oversample\")\n",
    "            minority_oversampled = []\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    if majority_class == 'yes':\n",
    "        balanced_records = majority_sampled + minority_oversampled\n",
    "    else:\n",
    "        balanced_records = minority_oversampled + majority_sampled\n",
    "    \n",
    "    random.shuffle(balanced_records)\n",
    "    \n",
    "    # Verify final distribution\n",
    "    final_yes = 0\n",
    "    for r in balanced_records:\n",
    "        if 'original_teacher_answer' in r:\n",
    "            answer = r['original_teacher_answer'].lower()\n",
    "        else:\n",
    "            answer = r['answer'].lower()\n",
    "        \n",
    "        if 'yes' in answer:\n",
    "            final_yes += 1\n",
    "    \n",
    "    final_no = len(balanced_records) - final_yes\n",
    "    print(f\"Balanced distribution: Yes={final_yes}, No={final_no}\")\n",
    "    \n",
    "    return balanced_records\n",
    "\n",
    "\n",
    "# Load and process teacher data for Progressive Curriculum Training\n",
    "print(\"üîç Loading teacher data for Progressive Curriculum Training...\")\n",
    "\n",
    "# Load the teacher data\n",
    "with open(COT_PATH_QA_COT, 'r') as f:\n",
    "    teacher_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"üìä Loaded {len(teacher_data)} teacher responses\")\n",
    "\n",
    "# Load ground truth\n",
    "ground_truth_map = {}\n",
    "with open(SAMPLE_TEST_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        ground_truth_map[item['question']] = item['answer']\n",
    "\n",
    "print(f\"üéØ Loaded {len(ground_truth_map)} ground truth answers\")\n",
    "\n",
    "# Initialize and run progressive curriculum pipeline\n",
    "pipeline = ResponseValidationPipeline()\n",
    "baseline_records, stage1_records, stage2_records, validation_stats = pipeline.process_responses_for_progressive_curriculum(teacher_data, ground_truth_map)\n",
    "\n",
    "# Display comprehensive validation results\n",
    "print(f\"\\n=== PROGRESSIVE CURRICULUM VALIDATION RESULTS ===\")\n",
    "print(f\"üìä Total processed: {validation_stats['total_processed']}\")\n",
    "print(f\"‚úÖ Data kept: {validation_stats['data_kept']} ({validation_stats['data_kept']/validation_stats['total_processed']*100:.1f}%)\")\n",
    "print(f\"üéØ Average confidence: {validation_stats['avg_confidence']:.3f}\")\n",
    "\n",
    "print(f\"\\n=== DATASET GENERATION RESULTS ===\")\n",
    "print(f\"üìù Baseline dataset: {len(baseline_records)} examples\")\n",
    "print(f\"üéØ Stage 1 dataset (Final Reasoning): {len(stage1_records)} examples\")\n",
    "print(f\"üß† Stage 2 dataset (Complete Q&A): {len(stage2_records)} examples\")\n",
    "\n",
    "print(f\"\\n=== FORMAT PROCESSING RESULTS ===\")\n",
    "print(f\"üß† Q&A-CoT format processed: {validation_stats['qa_cot_format']} ({validation_stats['qa_cot_format']/validation_stats['total_processed']*100:.1f}%)\")\n",
    "print(f\"üìù Traditional format processed: {validation_stats['traditional_format']} ({validation_stats['traditional_format']/validation_stats['total_processed']*100:.1f}%)\")\n",
    "print(f\"üìã Template fallbacks used: {validation_stats.get('template_fallbacks', 0)} ({validation_stats.get('template_fallbacks', 0)/validation_stats['total_processed']*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== CONFIDENCE TIER DISTRIBUTION ===\")\n",
    "total_validated = validation_stats['high_confidence'] + validation_stats['medium_confidence'] + validation_stats['low_confidence']\n",
    "if total_validated > 0:\n",
    "    print(f\"üî• High (‚â•0.8): {validation_stats['high_confidence']} ({validation_stats['high_confidence']/total_validated*100:.1f}%)\")\n",
    "    print(f\"üü° Medium (0.5-0.8): {validation_stats['medium_confidence']} ({validation_stats['medium_confidence']/total_validated*100:.1f}%)\")\n",
    "    print(f\"üî¥ Low (<0.5): {validation_stats['low_confidence']} ({validation_stats['low_confidence']/total_validated*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== ANSWER VALIDATION ===\")\n",
    "print(f\"‚úÖ Valid teacher answers: {validation_stats['valid_teacher_answers']}\")\n",
    "print(f\"‚ùå Invalid teacher answers: {validation_stats['invalid_teacher_answers']}\")\n",
    "if 'teacher_agreement_rate' in validation_stats:\n",
    "    print(f\"ü§ù Teacher-ground truth agreement: {validation_stats['teacher_agreement_rate']:.1f}%\")\n",
    "\n",
    "print(f\"\\n=== ERROR ANALYSIS ===\")\n",
    "if validation_stats['validation_errors']:\n",
    "    print(\"Most common validation errors:\")\n",
    "    for error, count in validation_stats['validation_errors'].most_common(3):\n",
    "        print(f\"  - {error}: {count} occurrences\")\n",
    "\n",
    "print(f\"üîß Correction attempts: {validation_stats['correction_attempts']}\")\n",
    "print(f\"‚úÖ Successful corrections: {validation_stats['successful_corrections']}\")\n",
    "\n",
    "# Analyze class distributions before balancing\n",
    "print(f\"\\n=== DISTRIBUTION ANALYSIS (PRE-BALANCING) ===\")\n",
    "baseline_dist = analyze_distribution(baseline_records)\n",
    "stage1_dist = analyze_distribution(stage1_records)\n",
    "stage2_dist = analyze_distribution(stage2_records)\n",
    "\n",
    "print(f\"Baseline: Yes={baseline_dist['yes_count']} ({baseline_dist['yes_percent']:.1f}%), No={baseline_dist['no_count']} ({baseline_dist['no_percent']:.1f}%)\")\n",
    "print(f\"Stage 1: Yes={stage1_dist['yes_count']} ({stage1_dist['yes_percent']:.1f}%), No={stage1_dist['no_count']} ({stage1_dist['no_percent']:.1f}%)\")\n",
    "print(f\"Stage 2: Yes={stage2_dist['yes_count']} ({stage2_dist['yes_percent']:.1f}%), No={stage2_dist['no_count']} ({stage2_dist['no_percent']:.1f}%)\")\n",
    "\n",
    "# Apply smart oversampling-based class balancing (only if imbalance > 15%)\n",
    "print(f\"\\n=== APPLYING SMART CLASS BALANCING (15% THRESHOLD) ===\")\n",
    "TARGET_TRAIN_SIZE = 1500  # Conservative target for quality\n",
    "IMBALANCE_THRESHOLD = 0.15  # Only apply oversampling if imbalance > 15%\n",
    "\n",
    "balanced_baseline_records = balance_dataset_with_oversampling(baseline_records, target_ratio=0.5, max_samples=TARGET_TRAIN_SIZE, imbalance_threshold=IMBALANCE_THRESHOLD)\n",
    "balanced_stage1_records = balance_dataset_with_oversampling(stage1_records, target_ratio=0.5, max_samples=TARGET_TRAIN_SIZE, imbalance_threshold=IMBALANCE_THRESHOLD)\n",
    "balanced_stage2_records = balance_dataset_with_oversampling(stage2_records, target_ratio=0.5, max_samples=TARGET_TRAIN_SIZE, imbalance_threshold=IMBALANCE_THRESHOLD)\n",
    "\n",
    "# Analyze post-balancing distributions\n",
    "balanced_baseline_dist = analyze_distribution(balanced_baseline_records)\n",
    "balanced_stage1_dist = analyze_distribution(balanced_stage1_records)\n",
    "balanced_stage2_dist = analyze_distribution(balanced_stage2_records)\n",
    "\n",
    "print(f\"\\n=== DISTRIBUTION ANALYSIS (POST-BALANCING) ===\")\n",
    "print(f\"Baseline: Yes={balanced_baseline_dist['yes_count']} ({balanced_baseline_dist['yes_percent']:.1f}%), No={balanced_baseline_dist['no_count']} ({balanced_baseline_dist['no_percent']:.1f}%)\")\n",
    "print(f\"Stage 1: Yes={balanced_stage1_dist['yes_count']} ({balanced_stage1_dist['yes_percent']:.1f}%), No={balanced_stage1_dist['no_count']} ({balanced_stage1_dist['no_percent']:.1f}%)\")\n",
    "print(f\"Stage 2: Yes={balanced_stage2_dist['yes_count']} ({balanced_stage2_dist['yes_percent']:.1f}%), No={balanced_stage2_dist['no_count']} ({balanced_stage2_dist['no_percent']:.1f}%)\")\n",
    "\n",
    "# Save balanced Progressive Curriculum Training datasets  \n",
    "print(f\"\\nüöÄ Saving Progressive Curriculum Training datasets...\")\n",
    "\n",
    "# Save baseline dataset (direct Q‚ÜíA)\n",
    "with open(BASELINE_PATH, 'w') as f:\n",
    "    for record in balanced_baseline_records:\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "# Save Stage 1 dataset (final reasoning focus)\n",
    "with open(STAGE1_PATH, 'w') as f:\n",
    "    for record in balanced_stage1_records:\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "# Save Stage 2 dataset (complete Q&A)\n",
    "with open(STAGE2_PATH, 'w') as f:\n",
    "    for record in balanced_stage2_records:\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(f\"üíæ Saved {len(balanced_baseline_records)} baseline records to: {BASELINE_PATH}\")\n",
    "print(f\"üéØ Saved {len(balanced_stage1_records)} Stage 1 records to: {STAGE1_PATH}\")\n",
    "print(f\"üß† Saved {len(balanced_stage2_records)} Stage 2 records to: {STAGE2_PATH}\")\n",
    "\n",
    "print(f\"\\n=== PROGRESSIVE CURRICULUM TRAINING DATA GENERATION SUMMARY ===\")\n",
    "print(f\"üöÄ Pipeline Status: ‚úÖ COMPLETE\")\n",
    "print(f\"üéØ Stage 1 Focus: Final reasoning after 'Therefore' phrase\")\n",
    "print(f\"üß† Stage 2 Focus: Complete Q&A with final reasoning\")\n",
    "print(f\"üìä Data Quality: {validation_stats['data_kept']} high/medium confidence examples\")\n",
    "print(f\"‚öñÔ∏è  Class Balance: Smart oversampling (15% threshold)\")\n",
    "print(f\"üéØ Confidence Score: {validation_stats['avg_confidence']:.3f} average\")\n",
    "print(f\"üîç Validation Rate: {(validation_stats['data_kept']/validation_stats['total_processed']*100):.1f}% data retention\")\n",
    "\n",
    "# Calculate Q&A-CoT adoption rate\n",
    "qa_cot_adoption = validation_stats['qa_cot_format'] / validation_stats['total_processed'] * 100\n",
    "print(f\"üß† Q&A-CoT Adoption: {qa_cot_adoption:.1f}% of training data uses interleaved Q&A format\")\n",
    "\n",
    "if validation_stats['avg_confidence'] >= 0.7:\n",
    "    print(\"üéØ SUCCESS: Achieved high-confidence Progressive Curriculum pipeline\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Moderate confidence - consider adjusting thresholds\")\n",
    "\n",
    "if qa_cot_adoption >= 80:\n",
    "    print(\"üéØ SUCCESS: High Q&A-CoT format adoption (80%+)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Q&A-CoT adoption at {qa_cot_adoption:.1f}% - consider improving prompt consistency\")\n",
    "\n",
    "print(f\"\\nüèÅ Ready for Progressive Curriculum Training!\")\n",
    "\n",
    "# Update paths for Progressive Curriculum Training\n",
    "BASELINE_PATH_ENHANCED = BASELINE_PATH\n",
    "STAGE1_PATH_ENHANCED = STAGE1_PATH\n",
    "STAGE2_PATH_ENHANCED = STAGE2_PATH\n",
    "\n",
    "print(f\"\\nüìÇ Updated training paths:\")\n",
    "print(f\"- BASELINE_PATH_ENHANCED = '{BASELINE_PATH_ENHANCED}'\")\n",
    "print(f\"- STAGE1_PATH_ENHANCED = '{STAGE1_PATH_ENHANCED}'\")\n",
    "print(f\"- STAGE2_PATH_ENHANCED = '{STAGE2_PATH_ENHANCED}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
