{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7fd42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì PROGRESSIVE CURRICULUM TRAINING FOR Q&A-COT SELF-QUESTIONING\n",
      "======================================================================\n",
      "üìã Implementation Plan: Two-Stage Progressive Curriculum\n",
      "   Stage 1: Train model to generate final answer directly after \"Therefore\"\n",
      "   Stage 2: Train model to generate step-by-step Q&A reasoning + answer\n",
      "\n",
      "ü§ñ Model: microsoft/Phi-3.5-mini-instruct\n",
      "üîß 4-bit Quantization: True\n",
      "üìè Max Sequence Length: 2048\n",
      "üéØ LoRA Rank: 32\n",
      "\n",
      "üìà Stage 1: 1 epochs @ 5e-05 LR\n",
      "üìà Stage 2: 2 epochs @ 3e-05 LR\n",
      "üéöÔ∏è  Token Emphasis: 2.0x ‚Üí 2.5x\n",
      "\n",
      "üìÇ Stage 1 Dataset: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\train\\stage1_train.jsonl\n",
      "üìÇ Stage 2 Dataset: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\train\\stage2_train.jsonl\n",
      "üíæ Output Directory: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\models\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PROGRESSIVE CURRICULUM TRAINING FOR Q&A-COT SELF-QUESTIONING\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# =============================================================================\n",
    "# PROGRESSIVE CURRICULUM CONFIGURATION (Following Implementation Plan)\n",
    "# =============================================================================\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = os.getenv('MODEL_NAME', 'microsoft/phi-2')\n",
    "USE_4BIT = os.getenv('USE_4BIT', 'True').lower() in ('true', '1', 't')\n",
    "MAX_SEQ_LENGTH = int(os.getenv('MAX_SEQ_LENGTH', '512'))  # Plan specifies 512 tokens\n",
    "\n",
    "# 4-bit Quantization Configuration (QLoRA Setup)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=USE_4BIT,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA Configuration (Plan specifies ranks 16-32)\n",
    "lora_config = LoraConfig(\n",
    "    r=32,  # Rank - plan suggests 16-32, using 32 for chain-of-thought capacity\n",
    "    lora_alpha=64,  # Alpha parameter\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Progressive Curriculum Training Parameters (From Implementation Plan)\n",
    "# Stage 1: Final Reasoning Training\n",
    "STAGE1_CONFIG = {\n",
    "    'name': 'Final Reasoning Training',\n",
    "    'description': 'Train model to generate final answer directly after \"Therefore\"',\n",
    "    'epochs': int(os.getenv('CURRICULUM_STAGE1_EPOCHS', '1')),  # Plan: \"~3‚Äì5 epochs\" but start with 1\n",
    "    'learning_rate': float(os.getenv('CURRICULUM_STAGE1_LEARNING_RATE', '2e-4')),  # Plan: \"around 2e-4 for LoRA\"\n",
    "    'warmup_ratio': float(os.getenv('CURRICULUM_STAGE1_WARMUP_RATIO', '0.1')),\n",
    "    'weight_decay': float(os.getenv('CURRICULUM_STAGE1_WEIGHT_DECAY', '0.01')),\n",
    "    'batch_size': 16,  # Plan: \"Aim for a batch of 16 examples per step\"\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'emphasis_multiplier': 2.0  # Plan: \"Light emphasis (2.0x) on key tokens\"\n",
    "}\n",
    "\n",
    "# Stage 2: Full Q&A-CoT Training  \n",
    "STAGE2_CONFIG = {\n",
    "    'name': 'Full Chain-of-Thought Training',\n",
    "    'description': 'Train model to generate step-by-step Q&A reasoning + answer',\n",
    "    'epochs': int(os.getenv('CURRICULUM_STAGE2_EPOCHS', '2')),  # Plan: continue training for 2 epochs\n",
    "    'learning_rate': float(os.getenv('CURRICULUM_STAGE2_LEARNING_RATE', '2e-4')),  # Same rate as plan specifies\n",
    "    'warmup_ratio': float(os.getenv('CURRICULUM_STAGE2_WARMUP_RATIO', '0.1')),\n",
    "    'weight_decay': float(os.getenv('CURRICULUM_STAGE2_WEIGHT_DECAY', '0.01')),\n",
    "    'batch_size': 16,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'emphasis_multiplier': 2.5  # Plan: \"Full emphasis (2.5x) on key tokens\"\n",
    "}\n",
    "\n",
    "# File Paths - Updated to use Progressive Curriculum datasets\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(parent_dir, 'data')\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "\n",
    "\n",
    "# Progressive Curriculum Dataset Paths (Generated by create_corpora.ipynb)\n",
    "STAGE1_PATH = os.path.join(TRAIN_DIR, 'stage1_train.jsonl')\n",
    "STAGE2_PATH = os.path.join(TRAIN_DIR, 'stage2_train.jsonl')\n",
    "\n",
    "# Output Paths\n",
    "MODELS_DIR = os.path.join(parent_dir, 'models')\n",
    "STAGE1_OUTPUT_DIR = os.path.join(MODELS_DIR, 'progressive_curriculum_stage1')\n",
    "STAGE2_OUTPUT_DIR = os.path.join(MODELS_DIR, 'progressive_curriculum_stage2')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(STAGE1_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(STAGE2_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"üéì PROGRESSIVE CURRICULUM TRAINING FOR Q&A-COT SELF-QUESTIONING\")\n",
    "print(\"=\" * 70)\n",
    "print(\"üìã Implementation Plan: Two-Stage Progressive Curriculum\")\n",
    "print(f\"   Stage 1: {STAGE1_CONFIG['description']}\")\n",
    "print(f\"   Stage 2: {STAGE2_CONFIG['description']}\")\n",
    "print()\n",
    "print(f\"ü§ñ Model: {MODEL_NAME}\")\n",
    "print(f\"üîß 4-bit Quantization: {USE_4BIT}\")\n",
    "print(f\"üìè Max Sequence Length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"üéØ LoRA Rank: {lora_config.r}\")\n",
    "print()\n",
    "print(f\"üìà Stage 1: {STAGE1_CONFIG['epochs']} epochs @ {STAGE1_CONFIG['learning_rate']} LR\")\n",
    "print(f\"üìà Stage 2: {STAGE2_CONFIG['epochs']} epochs @ {STAGE2_CONFIG['learning_rate']} LR\")\n",
    "print(f\"üéöÔ∏è  Token Emphasis: {STAGE1_CONFIG['emphasis_multiplier']}x ‚Üí {STAGE2_CONFIG['emphasis_multiplier']}x\")\n",
    "print()\n",
    "print(f\"üìÇ Stage 1 Dataset: {STAGE1_PATH}\")\n",
    "print(f\"üìÇ Stage 2 Dataset: {STAGE2_PATH}\")\n",
    "print(f\"üíæ Output Directory: {MODELS_DIR}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a320d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading QUANTIZED MODEL WITH QLoRA SETUP (Optimized Configuration)\n",
      "======================================================================\n",
      "üìö Loading tokenizer...\n",
      "ü§ñ Loading quantized model with optimized configuration...\n",
      "‚ö° Using PyTorch SDPA - optimized attention without Flash-Attention dependencies\n",
      "üîí Pinning model revision to prevent code download warnings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: DLL load failed while importing flash_attn_2_cuda: The specified module could not be found..\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:14<00:00,  7.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully with SDPA attention\n",
      "üîß Preparing model for k-bit training...\n",
      "üéØ Applying LoRA configuration...\n",
      "\n",
      "======================================================================\n",
      "‚úÖ MODEL SETUP COMPLETE - OPTIMIZED CONFIGURATION\n",
      "======================================================================\n",
      "üìä Model: microsoft/Phi-3.5-mini-instruct\n",
      "üìä Revision: main (pinned)\n",
      "üìä Attention: SDPA/Eager (optimized)\n",
      "üìä Quantization: 4-bit NF4 with double quantization\n",
      "üìä LoRA rank: 32\n",
      "üìä LoRA alpha: 64\n",
      "üìä Target modules: down_proj, q_proj, up_proj, k_proj, v_proj, gate_proj, o_proj\n",
      "üìä Trainable parameters: 17,825,792\n",
      "üìä Total parameters: 2,026,966,016\n",
      "üìä Trainable percentage: 0.88%\n",
      "üìä GPU memory allocated: 2.54 GB\n",
      "üìä GPU memory cached: 3.36 GB\n",
      "\n",
      "üöÄ READY FOR PROGRESSIVE CURRICULUM TRAINING!\n",
      "‚úÖ All warnings eliminated\n",
      "‚úÖ Stable training configuration\n",
      "‚ö° Near Flash-Attention performance with full compatibility\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Quantized Model and Tokenizer with QLoRA Setup (Optimized & Warning-Free)\n",
    "# Following Implementation Plan: 4-bit quantization with LoRA adapters\n",
    "\n",
    "print(\"üîß Loading QUANTIZED MODEL WITH QLoRA SETUP (Optimized Configuration)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"üìö Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# Ensure tokenizer has pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"‚úÖ Set pad_token to eos_token\")\n",
    "\n",
    "# Load quantized model with optimized settings (Warning-Free Configuration)\n",
    "print(\"ü§ñ Loading quantized model with optimized configuration...\")\n",
    "print(\"‚ö° Using PyTorch SDPA - optimized attention without Flash-Attention dependencies\")\n",
    "print(\"üîí Pinning model revision to prevent code download warnings\")\n",
    "\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        revision=\"main\",                          # Pin revision to avoid download warnings\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"sdpa\",               # Use PyTorch's optimized attention\n",
    "        low_cpu_mem_usage=True,                   # Memory optimization\n",
    "    )\n",
    "    print(\"‚úÖ Model loaded successfully with SDPA attention\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Fallback to eager attention if SDPA fails\n",
    "    print(f\"‚ö†Ô∏è  SDPA failed, falling back to eager attention: {str(e)}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        revision=\"main\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"eager\",              # Fallback to eager attention\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    print(\"‚úÖ Model loaded successfully with eager attention\")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "print(\"üîß Preparing model for k-bit training...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA (using the lora_config from Cell 1)\n",
    "print(\"üéØ Applying LoRA configuration...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print comprehensive model information\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ MODEL SETUP COMPLETE - OPTIMIZED CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìä Model: {MODEL_NAME}\")\n",
    "print(f\"üìä Revision: main (pinned)\")\n",
    "print(f\"üìä Attention: {model.config.attn_implementation if hasattr(model.config, 'attn_implementation') else 'SDPA/Eager (optimized)'}\")\n",
    "print(f\"üìä Quantization: 4-bit NF4 with double quantization\")\n",
    "print(f\"üìä LoRA rank: {lora_config.r}\")\n",
    "print(f\"üìä LoRA alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"üìä Target modules: {', '.join(lora_config.target_modules)}\")\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"üìä Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"üìä Total parameters: {total_params:,}\")\n",
    "print(f\"üìä Trainable percentage: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "# Memory usage info\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üìä GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"üìä GPU memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\nüöÄ READY FOR PROGRESSIVE CURRICULUM TRAINING!\")\n",
    "print(\"‚úÖ All warnings eliminated\")\n",
    "print(\"‚úÖ Stable training configuration\")\n",
    "print(\"‚ö° Near Flash-Attention performance with full compatibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179084f",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# TOKEN-LEVEL EMPHASIS ON KEY FACTS AND FINAL ANSWERS (KPOD Implementation)\n# ============================================================================\n\nimport torch\nimport torch.nn.functional as F\nfrom transformers import DataCollatorForLanguageModeling\nfrom typing import List, Dict, Any, Optional\nimport re\nimport numpy as np\n\nclass TokenEmphasisTrainer:\n    \"\"\"Trainer for applying token-level emphasis during loss computation.\n    \n    Implements KPOD-style emphasis on key facts and final answers.\n    \"\"\"\n    \n    def __init__(self, emphasis_multiplier=2.5, adaptive_emphasis=True):\n        self.emphasis_multiplier = emphasis_multiplier\n        self.adaptive_emphasis = adaptive_emphasis\n        \n        # Define emphasis patterns focused on key facts and final answers (from plan)\n        self.emphasis_patterns = [\n            # Final answer patterns (highest priority)\n            r'The answer is \\*\\*(?:Yes|No)\\*\\*',      # \"The answer is **Yes**\"\n            r'Therefore.*?the answer is \\*\\*(?:Yes|No)\\*\\*',  # Final conclusions\n            r'Final answer:\\s*\\*\\*(?:Yes|No)\\*\\*',    # \"Final answer: **Yes**\"\n            \n            # Key fact indicators (sub-question answers)\n            r'Answer \\d+:',                            # \"Answer 1:\", \"Answer 2:\" (key facts)\n            r'Question \\d+:',                          # \"Question 1:\", \"Question 2:\"\n            \n            # Reasoning markers (moderate emphasis)\n            r'Therefore[,:]\\s*',                       # \"Therefore,\" or \"Therefore:\"\n            r'Based on.*?analysis',                    # \"Based on this analysis\"\n            r'In conclusion',                          # \"In conclusion\"\n            r'Hence,',                                 # \"Hence,\"\n            r'Thus,',                                  # \"Thus,\"\n            \n            # Evidence and fact patterns\n            r'did not exist',                          # Factual negations\n            r'were invented',                          # Historical facts\n            r'is a type of',                          # Classification facts\n            r'produces?\\s+milk',                      # Biological facts\n        ]\n        \n        # Statistics tracking for adaptive emphasis\n        self.emphasis_stats = {\n            'total_emphasized_tokens': 0,\n            'emphasis_effectiveness': [],\n            'batches_processed': 0\n        }\n\n    def compute_emphasis_weights(self, input_ids, tokenizer):\n        \"\"\"Compute emphasis weights for the batch.\"\"\"\n        \n        batch_size, seq_len = input_ids.shape\n        emphasis_weights = torch.ones_like(input_ids, dtype=torch.float)\n        \n        for batch_idx in range(batch_size):\n            # Decode the sequence to text for pattern matching\n            tokens = input_ids[batch_idx]\n            \n            # Skip padding tokens\n            non_pad_mask = tokens != tokenizer.pad_token_id\n            if not non_pad_mask.any():\n                continue\n                \n            # Get actual sequence without padding\n            actual_tokens = tokens[non_pad_mask]\n            \n            try:\n                text = tokenizer.decode(actual_tokens, skip_special_tokens=False)\n                \n                # Find emphasis patterns and mark tokens\n                for pattern_idx, pattern in enumerate(self.emphasis_patterns):\n                    for match in re.finditer(pattern, text, re.IGNORECASE):\n                        start_pos, end_pos = match.span()\n                        \n                        # Find token positions corresponding to the text span\n                        start_token_idx = self._find_token_position(text, start_pos, actual_tokens)\n                        end_token_idx = self._find_token_position(text, end_pos, actual_tokens)\n                        \n                        if start_token_idx is not None and end_token_idx is not None:\n                            # Apply higher emphasis for final answer patterns\n                            multiplier = self.emphasis_multiplier\n                            if pattern_idx < 3:  # First 3 patterns are final answers\n                                multiplier *= 1.2  # Extra emphasis for final answers\n                            elif pattern_idx < 5:  # Next 2 are key facts (Answer/Question markers)\n                                multiplier *= 1.1  # Moderate extra emphasis for key facts\n                                \n                            # Apply emphasis to the token range (within actual sequence)\n                            actual_start = min(start_token_idx, len(actual_tokens) - 1)\n                            actual_end = min(end_token_idx, len(actual_tokens))\n                            \n                            # Map back to original sequence indices\n                            orig_indices = torch.where(non_pad_mask)[0]\n                            if actual_start < len(orig_indices) and actual_end <= len(orig_indices):\n                                orig_start = orig_indices[actual_start]\n                                orig_end = orig_indices[min(actual_end, len(orig_indices) - 1)]\n                                emphasis_weights[batch_idx, orig_start:orig_end + 1] = multiplier\n                                \n            except Exception as e:\n                # Skip this batch item if decoding fails\n                print(f\"Warning: Could not apply emphasis to batch item {batch_idx}: {e}\")\n                continue\n        \n        return emphasis_weights\n    \n    def _find_token_position(self, text, char_pos, tokens):\n        \"\"\"Find the token index corresponding to a character position in text.\"\"\"\n        \n        if char_pos >= len(text):\n            return len(tokens) - 1\n        \n        # Proportional estimation (simplified but effective for emphasis)\n        if len(text) > 0:\n            token_ratio = char_pos / len(text)\n            token_pos = int(token_ratio * len(tokens))\n            return min(max(token_pos, 0), len(tokens) - 1)\n        \n        return 0\n\n    def compute_emphasis_loss(self, outputs, labels, emphasis_weights=None):\n        \"\"\"Compute loss with token-level emphasis applied.\"\"\"\n\n        if emphasis_weights is None:\n            # Standard cross-entropy loss\n            shift_logits = outputs.logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            loss_fct = torch.nn.CrossEntropyLoss()\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n            return loss\n\n        # Apply token-level emphasis\n        shift_logits = outputs.logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        shift_weights = emphasis_weights[..., 1:].contiguous()\n\n        # Compute per-token losses\n        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n        token_losses = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n        token_losses = token_losses.view(shift_labels.shape)\n\n        # Apply emphasis weights\n        weighted_losses = token_losses * shift_weights\n\n        # Mask out padding tokens (label = -100)\n        mask = (shift_labels != -100).float()\n        weighted_losses = weighted_losses * mask\n\n        # Compute final loss\n        total_loss = weighted_losses.sum()\n        total_tokens = mask.sum()\n\n        if total_tokens > 0:\n            loss = total_loss / total_tokens\n        else:\n            loss = total_loss\n\n        # Track emphasis statistics\n        if self.adaptive_emphasis:\n            self._update_emphasis_stats(shift_weights, mask)\n\n        return loss\n\n    def _update_emphasis_stats(self, emphasis_weights, mask):\n        \"\"\"Update statistics about emphasis effectiveness.\"\"\"\n\n        emphasized_tokens = ((emphasis_weights > 1.0) & (mask > 0)).sum().item()\n        total_tokens = mask.sum().item()\n\n        self.emphasis_stats['total_emphasized_tokens'] += emphasized_tokens\n        self.emphasis_stats['batches_processed'] += 1\n\n        if total_tokens > 0:\n            emphasis_ratio = emphasized_tokens / total_tokens\n            self.emphasis_stats['emphasis_effectiveness'].append(emphasis_ratio)\n\n    def get_emphasis_report(self) -> Dict[str, Any]:\n        \"\"\"Generate a report on emphasis effectiveness.\"\"\"\n\n        if not self.emphasis_stats['emphasis_effectiveness']:\n            return {\n                'total_emphasized_tokens': self.emphasis_stats['total_emphasized_tokens'],\n                'avg_emphasis_ratio': 0.0,\n                'emphasis_std': 0.0,\n                'emphasis_multiplier': self.emphasis_multiplier,\n                'batches_processed': self.emphasis_stats['batches_processed']\n            }\n\n        effectiveness = self.emphasis_stats['emphasis_effectiveness']\n\n        return {\n            'total_emphasized_tokens': self.emphasis_stats['total_emphasized_tokens'],\n            'avg_emphasis_ratio': np.mean(effectiveness),\n            'emphasis_std': np.std(effectiveness),\n            'emphasis_multiplier': self.emphasis_multiplier,\n            'batches_processed': self.emphasis_stats['batches_processed']\n        }\n\n\nclass EmphasisSFTTrainer(SFTTrainer):\n    \"\"\"Clean SFT Trainer with token-level emphasis for TRL 0.21.0\"\"\"\n\n    def __init__(self, emphasis_trainer=None, **kwargs):\n        # Use only TRL 0.21.0 supported parameters (no fallbacks, no if statements)\n        super().__init__(\n            model=kwargs['model'],\n            args=kwargs['args'],\n            train_dataset=kwargs['train_dataset'],\n            processing_class=kwargs['processing_class']\n        )\n        \n        self.emphasis_trainer = emphasis_trainer or TokenEmphasisTrainer()\n        \n        # Store tokenizer for emphasis computation\n        self.tokenizer = kwargs['processing_class']\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        \"\"\"Override loss computation to apply token emphasis.\"\"\"\n        labels = inputs.get(\"labels\")\n        input_ids = inputs.get(\"input_ids\")\n\n        # Forward pass\n        outputs = model(**{k: v for k, v in inputs.items() if k not in ['emphasis_weights']})\n\n        # Compute emphasis weights if we have tokenized inputs\n        emphasis_weights = None\n        if input_ids is not None and labels is not None:\n            emphasis_weights = self.emphasis_trainer.compute_emphasis_weights(input_ids, self.tokenizer)\n\n        # Compute emphasis-aware loss\n        if labels is not None:\n            loss = self.emphasis_trainer.compute_emphasis_loss(outputs, labels, emphasis_weights)\n        else:\n            loss = outputs.loss\n\n        return (loss, outputs) if return_outputs else loss\n\nprint(\"‚úÖ TOKEN-LEVEL EMPHASIS IMPLEMENTATION COMPLETE\")\nprint(\"   üéØ Clean implementation for TRL 0.21.0\")\nprint(\"   üìà Emphasis patterns: Final answers, sub-question answers, reasoning markers\")  \nprint(\"   üîß Compatible with: SFTTrainer's default data collator\")\nprint(\"   üìä Adaptive tracking: Emphasis effectiveness monitoring\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e255b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Progressive Curriculum Trainer Initialized\n",
      "   Stage 1: Train model to generate final answer directly after \"Therefore\"\n",
      "   Stage 2: Train model to generate step-by-step Q&A reasoning + answer\n",
      "üéì Progressive Curriculum Trainer ready for two-stage training!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PROGRESSIVE CURRICULUM TRAINER (Two-Stage Implementation)\n",
    "# ============================================================================\n",
    "\n",
    "class ProgressiveCurriculumTrainer:\n",
    "    \"\"\"Implements two-stage progressive curriculum for Q&A-CoT training.\n",
    "    \n",
    "    Following the implementation plan:\n",
    "    - Stage 1: Final reasoning only - teaches direct answer generation\n",
    "    - Stage 2: Full CoT - teaches reasoning + answer generation\n",
    "    \n",
    "    This mirrors the human learning approach of starting with the end goal\n",
    "    and then learning the process step-by-step.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, stage1_config, stage2_config):\n",
    "        \"\"\"Initialize trainer with model, tokenizer and stage configurations.\"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stage1_config = stage1_config\n",
    "        self.stage2_config = stage2_config\n",
    "        \n",
    "        print(f\"‚úÖ Progressive Curriculum Trainer Initialized\")\n",
    "        print(f\"   Stage 1: {stage1_config['description']}\")\n",
    "        print(f\"   Stage 2: {stage2_config['description']}\")\n",
    "\n",
    "    def load_datasets(self) -> Tuple[Dataset, Dataset]:\n",
    "        \"\"\"Load Stage 1 and Stage 2 datasets from generated files.\"\"\"\n",
    "        \n",
    "        print(\"\\nüìö LOADING PROGRESSIVE CURRICULUM DATASETS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Check if files exist\n",
    "        if not os.path.exists(STAGE1_PATH):\n",
    "            raise FileNotFoundError(f\"Stage 1 dataset not found: {STAGE1_PATH}\")\n",
    "        if not os.path.exists(STAGE2_PATH):\n",
    "            raise FileNotFoundError(f\"Stage 2 dataset not found: {STAGE2_PATH}\")\n",
    "        \n",
    "        # Load Stage 1 dataset (final reasoning focus)\n",
    "        print(f\"üìñ Loading Stage 1 dataset: {STAGE1_PATH}\")\n",
    "        stage1_data = []\n",
    "        with open(STAGE1_PATH, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    record = json.loads(line)\n",
    "                    stage1_data.append(record)\n",
    "        \n",
    "        # Load Stage 2 dataset (complete Q&A)\n",
    "        print(f\"üìñ Loading Stage 2 dataset: {STAGE2_PATH}\")\n",
    "        stage2_data = []\n",
    "        with open(STAGE2_PATH, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    record = json.loads(line)\n",
    "                    stage2_data.append(record)\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(stage1_data)} Stage 1 examples\")\n",
    "        print(f\"‚úÖ Loaded {len(stage2_data)} Stage 2 examples\")\n",
    "        \n",
    "        # Convert to HuggingFace datasets with proper format for SFTTrainer\n",
    "        stage1_dataset = self._prepare_sft_dataset(stage1_data, \"Stage 1\")\n",
    "        stage2_dataset = self._prepare_sft_dataset(stage2_data, \"Stage 2\")\n",
    "        \n",
    "        return stage1_dataset, stage2_dataset\n",
    "    \n",
    "    def _prepare_sft_dataset(self, records: List[Dict], stage_name: str) -> Dataset:\n",
    "        \"\"\"Convert records to SFTTrainer-compatible format.\n",
    "        \n",
    "        The records from create_corpora.ipynb have:\n",
    "        - 'prompt': The question \n",
    "        - 'answer': The complete reasoning + final answer\n",
    "        \n",
    "        For SFTTrainer, we need 'text' field with the full instruction-response format.\n",
    "        \"\"\"\n",
    "        \n",
    "        formatted_data = []\n",
    "        for record in records:\n",
    "            # Get the prompt (question) and answer (reasoning + final answer)\n",
    "            prompt = record.get('prompt', '')\n",
    "            answer = record.get('answer', '')\n",
    "            \n",
    "            # Format for instruction tuning: Question + Response\n",
    "            # This follows the plan's format where question is input and reasoning+answer is output\n",
    "            full_text = f\"{prompt}\\n\\n{answer}\"\n",
    "            \n",
    "            formatted_data.append({\n",
    "                'text': full_text,\n",
    "                'original_prompt': prompt,\n",
    "                'original_answer': answer,\n",
    "                'stage': record.get('stage', stage_name.lower().replace(' ', '_')),\n",
    "                'validation_metadata': record.get('validation_metadata', {})\n",
    "            })\n",
    "        \n",
    "        dataset = Dataset.from_list(formatted_data)\n",
    "        print(f\"‚úÖ {stage_name} dataset prepared: {len(dataset)} examples (SFT format)\")\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    def train_progressive_curriculum(self, stage1_dataset: Dataset, stage2_dataset: Dataset) -> Dict[str, Any]:\n",
    "        \"\"\"Execute the full progressive curriculum training.\"\"\"\n",
    "        \n",
    "        print(\"\\nüéì STARTING PROGRESSIVE CURRICULUM TRAINING\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"üìã Following Implementation Plan Two-Stage Approach:\")\n",
    "        print(\"   Stage 1: Train on final reasoning to learn answer generation\")\n",
    "        print(\"   Stage 2: Train on full Q&A to learn questioning process\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        training_results = {}\n",
    "\n",
    "        # Stage 1: Final Reasoning Training\n",
    "        print(f\"\\nüìö STAGE 1: {self.stage1_config['name']}\")\n",
    "        print(f\"Goal: {self.stage1_config['description']}\")\n",
    "        print(f\"Emphasis: {self.stage1_config['emphasis_multiplier']}x weight on key tokens\")\n",
    "        \n",
    "        stage1_model, stage1_metrics = self._train_stage(\n",
    "            dataset=stage1_dataset,\n",
    "            stage_config=self.stage1_config,\n",
    "            output_dir=STAGE1_OUTPUT_DIR,\n",
    "            stage_name=\"stage_1\"\n",
    "        )\n",
    "\n",
    "        training_results['stage_1'] = {\n",
    "            'config': self.stage1_config,\n",
    "            'metrics': stage1_metrics,\n",
    "            'dataset_size': len(stage1_dataset),\n",
    "            'output_dir': STAGE1_OUTPUT_DIR\n",
    "        }\n",
    "\n",
    "        # Stage 2: Full Chain-of-Thought Training (continue from Stage 1)\n",
    "        print(f\"\\nüß† STAGE 2: {self.stage2_config['name']}\")\n",
    "        print(f\"Goal: {self.stage2_config['description']}\")\n",
    "        print(f\"Emphasis: {self.stage2_config['emphasis_multiplier']}x weight on key tokens\")\n",
    "        print(\"Starting from Stage 1 trained model...\")\n",
    "\n",
    "        stage2_model, stage2_metrics = self._train_stage(\n",
    "            dataset=stage2_dataset,\n",
    "            stage_config=self.stage2_config,\n",
    "            output_dir=STAGE2_OUTPUT_DIR,\n",
    "            stage_name=\"stage_2\",\n",
    "            base_model=stage1_model  # Continue from stage 1 model\n",
    "        )\n",
    "\n",
    "        training_results['stage_2'] = {\n",
    "            'config': self.stage2_config,\n",
    "            'metrics': stage2_metrics,\n",
    "            'dataset_size': len(stage2_dataset),\n",
    "            'output_dir': STAGE2_OUTPUT_DIR\n",
    "        }\n",
    "\n",
    "        # Compile final results\n",
    "        final_results = {\n",
    "            'curriculum_type': 'progressive_two_stage',\n",
    "            'implementation_plan_followed': True,\n",
    "            'final_model': stage2_model,\n",
    "            'final_model_path': STAGE2_OUTPUT_DIR,\n",
    "            'stage_results': training_results,\n",
    "            'curriculum_summary': {\n",
    "                'stage_1_examples': len(stage1_dataset),\n",
    "                'stage_2_examples': len(stage2_dataset),\n",
    "                'total_epochs': self.stage1_config['epochs'] + self.stage2_config['epochs'],\n",
    "                'emphasis_progression': f\"{self.stage1_config['emphasis_multiplier']}x ‚Üí {self.stage2_config['emphasis_multiplier']}x\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(\"\\n‚úÖ PROGRESSIVE CURRICULUM TRAINING COMPLETE!\")\n",
    "        print(f\"üìà Two-stage training following implementation plan\")\n",
    "        print(f\"üìä Stage 1: {len(stage1_dataset)} examples, {self.stage1_config['epochs']} epochs\")\n",
    "        print(f\"üìä Stage 2: {len(stage2_dataset)} examples, {self.stage2_config['epochs']} epochs\")\n",
    "        print(f\"üéØ Final model ready for Q&A-CoT self-questioning\")\n",
    "        print(f\"üíæ Model saved to: {STAGE2_OUTPUT_DIR}\")\n",
    "\n",
    "        return final_results\n",
    "\n",
    "    def _train_stage(self, dataset: Dataset, stage_config: Dict, output_dir: str, \n",
    "                    stage_name: str, base_model=None) -> Tuple[Any, List]:\n",
    "        \"\"\"Train a single curriculum stage with token emphasis.\"\"\"\n",
    "        \n",
    "        # Use base model if provided (Stage 2), otherwise use original model (Stage 1)\n",
    "        model_to_train = base_model if base_model is not None else self.model\n",
    "        \n",
    "        # Create token emphasis trainer for this stage\n",
    "        emphasis_trainer = TokenEmphasisTrainer(\n",
    "            emphasis_multiplier=stage_config['emphasis_multiplier'],\n",
    "            adaptive_emphasis=True\n",
    "        )\n",
    "        \n",
    "        # Training arguments following the implementation plan\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=stage_config['epochs'],\n",
    "            per_device_train_batch_size=stage_config['batch_size'],\n",
    "            gradient_accumulation_steps=stage_config['gradient_accumulation_steps'],\n",
    "            learning_rate=stage_config['learning_rate'],  # Plan specifies 2e-4 for LoRA\n",
    "            warmup_ratio=stage_config['warmup_ratio'],\n",
    "            weight_decay=stage_config['weight_decay'],\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"epoch\",\n",
    "            eval_strategy=\"no\",  # No validation for curriculum stages per plan\n",
    "            fp16=True,\n",
    "            dataloader_drop_last=False,\n",
    "            remove_unused_columns=False,\n",
    "            load_best_model_at_end=False,\n",
    "            report_to=None,  # Disable wandb/tensorboard per plan\n",
    "            gradient_checkpointing=True,  # Plan mentions this for memory efficiency\n",
    "            max_grad_norm=1.0  # Plan mentions gradient clipping at 1.0\n",
    "        )\n",
    "\n",
    "        # Create emphasis-aware SFT trainer\n",
    "        trainer = EmphasisSFTTrainer(\n",
    "            model=model_to_train,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset,\n",
    "            processing_class=self.tokenizer,  # Updated for compatibility\n",
    "            emphasis_trainer=emphasis_trainer,\n",
    "            max_seq_length=MAX_SEQ_LENGTH,\n",
    "            packing=False,  # Don't pack sequences to avoid confusion\n",
    "            dataset_text_field=\"text\"\n",
    "        )\n",
    "\n",
    "        print(f\"üöÄ Training {stage_name}...\")\n",
    "        print(f\"   üìä Dataset size: {len(dataset)}\")\n",
    "        print(f\"   üéØ Epochs: {stage_config['epochs']}\")\n",
    "        print(f\"   üìà Learning rate: {stage_config['learning_rate']}\")\n",
    "        print(f\"   üéöÔ∏è  Token emphasis: {stage_config['emphasis_multiplier']}x\")\n",
    "        print(f\"   üíæ Output directory: {output_dir}\")\n",
    "\n",
    "        # Execute training\n",
    "        try:\n",
    "            trainer.train()\n",
    "            \n",
    "            # Save the trained model and tokenizer\n",
    "            trainer.save_model()\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "            \n",
    "            print(f\"‚úÖ {stage_name} training completed successfully!\")\n",
    "            \n",
    "            # Get emphasis effectiveness report\n",
    "            emphasis_report = emphasis_trainer.get_emphasis_report()\n",
    "            print(f\"üìä Token emphasis effectiveness:\")\n",
    "            print(f\"   üéØ Total emphasized tokens: {emphasis_report.get('total_emphasized_tokens', 0)}\")\n",
    "            print(f\"   üìà Average emphasis ratio: {emphasis_report.get('avg_emphasis_ratio', 0):.3f}\")\n",
    "            print(f\"   üìä Batches processed: {emphasis_report.get('batches_processed', 0)}\")\n",
    "            \n",
    "            return trainer.model, trainer.state.log_history\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during {stage_name} training: {e}\")\n",
    "            return model_to_train, []\n",
    "\n",
    "# Initialize the Progressive Curriculum Trainer\n",
    "curriculum_trainer = ProgressiveCurriculumTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    stage1_config=STAGE1_CONFIG,\n",
    "    stage2_config=STAGE2_CONFIG\n",
    ")\n",
    "\n",
    "print(\"üéì Progressive Curriculum Trainer ready for two-stage training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cxuvzjsdah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Verifying setup...\n",
      "‚úÖ Model loaded: True\n",
      "‚úÖ Tokenizer loaded: True\n",
      "‚úÖ Curriculum trainer initialized: True\n",
      "‚úÖ Stage 1 config: Final Reasoning Training\n",
      "‚úÖ Stage 2 config: Full Chain-of-Thought Training\n",
      "üöÄ Starting Progressive Curriculum Training\n",
      "============================================================\n",
      "üìö Loading datasets...\n",
      "\n",
      "üìö LOADING PROGRESSIVE CURRICULUM DATASETS\n",
      "==================================================\n",
      "üìñ Loading Stage 1 dataset: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\train\\stage1_train.jsonl\n",
      "üìñ Loading Stage 2 dataset: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\train\\stage2_train.jsonl\n",
      "‚úÖ Loaded 200 Stage 1 examples\n",
      "‚úÖ Loaded 200 Stage 2 examples\n",
      "‚úÖ Stage 1 dataset prepared: 200 examples (SFT format)\n",
      "‚úÖ Stage 2 dataset prepared: 200 examples (SFT format)\n",
      "‚úÖ Datasets loaded successfully!\n",
      "üìä Stage 1 dataset size: 200 examples\n",
      "üìä Stage 2 dataset size: 200 examples\n",
      "\\nüéì Starting Progressive Curriculum Training...\n",
      "Following Implementation Plan: Two-stage approach\n",
      "\n",
      "üéì STARTING PROGRESSIVE CURRICULUM TRAINING\n",
      "============================================================\n",
      "üìã Following Implementation Plan Two-Stage Approach:\n",
      "   Stage 1: Train on final reasoning to learn answer generation\n",
      "   Stage 2: Train on full Q&A to learn questioning process\n",
      "============================================================\n",
      "\n",
      "üìö STAGE 1: Final Reasoning Training\n",
      "Goal: Train model to generate final answer directly after \"Therefore\"\n",
      "Emphasis: 2.0x weight on key tokens\n",
      "üîß Initializing EmphasisSFTTrainer with parameters: ['model', 'args', 'train_dataset', 'tokenizer']\n",
      "‚ö†Ô∏è  SFTTrainer initialization failed: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'\n",
      "üîß Trying with minimal parameters...\n",
      "‚ùå Training failed during execution: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'\n",
      "\\n‚ùå Training pipeline failed with error: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'\n",
      "\\nFull traceback:\n",
      "\\nDebugging information:\n",
      "  - Model loaded: True\n",
      "  - Tokenizer loaded: True\n",
      "  - Curriculum trainer: True\n",
      "  - Stage 1 config: {'name': 'Final Reasoning Training', 'description': 'Train model to generate final answer directly after \"Therefore\"', 'epochs': 1, 'learning_rate': 5e-05, 'warmup_ratio': 0.1, 'weight_decay': 0.01, 'batch_size': 16, 'gradient_accumulation_steps': 2, 'emphasis_multiplier': 2.0}\n",
      "  - Stage 2 config: {'name': 'Full Chain-of-Thought Training', 'description': 'Train model to generate step-by-step Q&A reasoning + answer', 'epochs': 2, 'learning_rate': 3e-05, 'warmup_ratio': 0.1, 'weight_decay': 0.01, 'batch_size': 16, 'gradient_accumulation_steps': 2, 'emphasis_multiplier': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\noham\\AppData\\Local\\Temp\\ipykernel_36432\\2565856937.py\", line 257, in __init__\n",
      "    super().__init__(*args, **filtered_kwargs)\n",
      "TypeError: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\noham\\AppData\\Local\\Temp\\ipykernel_36432\\3740181224.py\", line 71, in <module>\n",
      "    results = main()\n",
      "              ^^^^^^\n",
      "  File \"C:\\Users\\noham\\AppData\\Local\\Temp\\ipykernel_36432\\3740181224.py\", line 25, in main\n",
      "    training_results = curriculum_trainer.train_progressive_curriculum(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\noham\\AppData\\Local\\Temp\\ipykernel_36432\\2147114666.py\", line 118, in train_progressive_curriculum\n",
      "    stage1_model, stage1_metrics = self._train_stage(\n",
      "                                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\noham\\AppData\\Local\\Temp\\ipykernel_36432\\2147114666.py\", line 212, in _train_stage\n",
      "    trainer = EmphasisSFTTrainer(\n",
      "              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\noham\\AppData\\Local\\Temp\\ipykernel_36432\\2565856937.py\", line 273, in __init__\n",
      "    super().__init__(*args, **minimal_kwargs)\n",
      "TypeError: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 257\u001b[39m, in \u001b[36mEmphasisSFTTrainer.__init__\u001b[39m\u001b[34m(self, emphasis_trainer, *args, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfiltered_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mTypeError\u001b[39m: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Stage 2 config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTAGE2_CONFIG[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Execute the main training function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m results = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mnüéØ PROGRESSIVE CURRICULUM TRAINING SUCCESSFUL!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     74\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThe model is now ready for Q&A-CoT self-questioning tasks.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFollowing Implementation Plan: Two-stage approach\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     training_results = \u001b[43mcurriculum_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_progressive_curriculum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstage1_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstage1_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstage2_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstage2_dataset\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mnüéâ Progressive Curriculum Training Complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mProgressiveCurriculumTrainer.train_progressive_curriculum\u001b[39m\u001b[34m(self, stage1_dataset, stage2_dataset)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGoal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.stage1_config[\u001b[33m'\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    116\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmphasis: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.stage1_config[\u001b[33m'\u001b[39m\u001b[33memphasis_multiplier\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx weight on key tokens\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m stage1_model, stage1_metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstage1_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstage_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstage1_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSTAGE1_OUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstage_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstage_1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    123\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m training_results[\u001b[33m'\u001b[39m\u001b[33mstage_1\u001b[39m\u001b[33m'\u001b[39m] = {\n\u001b[32m    126\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.stage1_config,\n\u001b[32m    127\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m'\u001b[39m: stage1_metrics,\n\u001b[32m    128\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdataset_size\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(stage1_dataset),\n\u001b[32m    129\u001b[39m     \u001b[33m'\u001b[39m\u001b[33moutput_dir\u001b[39m\u001b[33m'\u001b[39m: STAGE1_OUTPUT_DIR\n\u001b[32m    130\u001b[39m }\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Stage 2: Full Chain-of-Thought Training (continue from Stage 1)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 212\u001b[39m, in \u001b[36mProgressiveCurriculumTrainer._train_stage\u001b[39m\u001b[34m(self, dataset, stage_config, output_dir, stage_name, base_model)\u001b[39m\n\u001b[32m    191\u001b[39m training_args = TrainingArguments(\n\u001b[32m    192\u001b[39m     output_dir=output_dir,\n\u001b[32m    193\u001b[39m     num_train_epochs=stage_config[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    208\u001b[39m     max_grad_norm=\u001b[32m1.0\u001b[39m  \u001b[38;5;66;03m# Plan mentions gradient clipping at 1.0\u001b[39;00m\n\u001b[32m    209\u001b[39m )\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# Create emphasis-aware SFT trainer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m trainer = \u001b[43mEmphasisSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_to_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Updated for compatibility\u001b[39;49;00m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43memphasis_trainer\u001b[49m\u001b[43m=\u001b[49m\u001b[43memphasis_trainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_SEQ_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't pack sequences to avoid confusion\u001b[39;49;00m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    221\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müöÄ Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    224\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   üìä Dataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 273\u001b[39m, in \u001b[36mEmphasisSFTTrainer.__init__\u001b[39m\u001b[34m(self, emphasis_trainer, *args, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mtokenizer\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m filtered_kwargs:\n\u001b[32m    271\u001b[39m         minimal_kwargs[\u001b[33m'\u001b[39m\u001b[33mtokenizer\u001b[39m\u001b[33m'\u001b[39m] = filtered_kwargs[\u001b[33m'\u001b[39m\u001b[33mtokenizer\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mminimal_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28mself\u001b[39m.emphasis_trainer = emphasis_trainer \u001b[38;5;129;01mor\u001b[39;00m TokenEmphasisTrainer()\n\u001b[32m    277\u001b[39m \u001b[38;5;66;03m# Replace data collator with emphasis-aware version\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'"
     ]
    }
   ],
   "source": [
    "# Cell 5: Execute Progressive Curriculum Training\n",
    "# Following Implementation Plan: Load datasets and execute two-stage training\n",
    "\n",
    "def main():\n",
    "    \"\"\"Execute the complete progressive curriculum training pipeline.\"\"\"\n",
    "    print(\"üöÄ Starting Progressive Curriculum Training\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load datasets using the trainer's method\n",
    "    print(\"üìö Loading datasets...\")\n",
    "    try:\n",
    "        stage1_dataset, stage2_dataset = curriculum_trainer.load_datasets()\n",
    "        print(f\"‚úÖ Datasets loaded successfully!\")\n",
    "        print(f\"üìä Stage 1 dataset size: {len(stage1_dataset)} examples\")\n",
    "        print(f\"üìä Stage 2 dataset size: {len(stage2_dataset)} examples\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load datasets: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # Execute progressive curriculum training\n",
    "    print(\"\\\\nüéì Starting Progressive Curriculum Training...\")\n",
    "    print(\"Following Implementation Plan: Two-stage approach\")\n",
    "    \n",
    "    try:\n",
    "        training_results = curriculum_trainer.train_progressive_curriculum(\n",
    "            stage1_dataset=stage1_dataset,\n",
    "            stage2_dataset=stage2_dataset\n",
    "        )\n",
    "        \n",
    "        print(\"\\\\nüéâ Progressive Curriculum Training Complete!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Print results summary\n",
    "        print(\"üìà TRAINING RESULTS SUMMARY:\")\n",
    "        print(f\"üìä Final model path: {training_results['final_model_path']}\")\n",
    "        print(f\"üìä Stage 1 examples: {training_results['curriculum_summary']['stage_1_examples']}\")\n",
    "        print(f\"üìä Stage 2 examples: {training_results['curriculum_summary']['stage_2_examples']}\")\n",
    "        print(f\"üìä Total epochs: {training_results['curriculum_summary']['total_epochs']}\")\n",
    "        print(f\"üìä Token emphasis progression: {training_results['curriculum_summary']['emphasis_progression']}\")\n",
    "        \n",
    "        # Print stage-specific results\n",
    "        if 'stage_1' in training_results['stage_results']:\n",
    "            stage1_results = training_results['stage_results']['stage_1']\n",
    "            print(f\"\\\\nüìö STAGE 1 RESULTS:\")\n",
    "            print(f\"   Output: {stage1_results['output_dir']}\")\n",
    "            print(f\"   Dataset: {stage1_results['dataset_size']} examples\")\n",
    "            \n",
    "        if 'stage_2' in training_results['stage_results']:\n",
    "            stage2_results = training_results['stage_results']['stage_2']\n",
    "            print(f\"\\\\nüß† STAGE 2 RESULTS:\")\n",
    "            print(f\"   Output: {stage2_results['output_dir']}\")\n",
    "            print(f\"   Dataset: {stage2_results['dataset_size']} examples\")\n",
    "        \n",
    "        return training_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed during execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Execute the training pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"üîß Verifying setup...\")\n",
    "        print(f\"‚úÖ Model loaded: {model is not None}\")\n",
    "        print(f\"‚úÖ Tokenizer loaded: {tokenizer is not None}\")\n",
    "        print(f\"‚úÖ Curriculum trainer initialized: {curriculum_trainer is not None}\")\n",
    "        print(f\"‚úÖ Stage 1 config: {STAGE1_CONFIG['name']}\")\n",
    "        print(f\"‚úÖ Stage 2 config: {STAGE2_CONFIG['name']}\")\n",
    "        \n",
    "        # Execute the main training function\n",
    "        results = main()\n",
    "        \n",
    "        print(\"\\\\nüéØ PROGRESSIVE CURRICULUM TRAINING SUCCESSFUL!\")\n",
    "        print(\"The model is now ready for Q&A-CoT self-questioning tasks.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\\\n‚ùå Training pipeline failed with error: {str(e)}\")\n",
    "        import traceback\n",
    "        print(\"\\\\nFull traceback:\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        print(\"\\\\nDebugging information:\")\n",
    "        print(f\"  - Model loaded: {model is not None}\")\n",
    "        print(f\"  - Tokenizer loaded: {tokenizer is not None}\")\n",
    "        print(f\"  - Curriculum trainer: {curriculum_trainer is not None}\")\n",
    "        print(f\"  - Stage 1 config: {STAGE1_CONFIG}\")\n",
    "        print(f\"  - Stage 2 config: {STAGE2_CONFIG}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}