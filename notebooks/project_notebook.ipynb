{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0675efc",
   "metadata": {},
   "source": [
    "# Self‑Improving LLM Project\n",
    "\n",
    "This notebook implements Parts 2 and 3 of the project plan for the **Self‑Improving LLM** final project.  Specifically, it covers:\n",
    "\n",
    "- **Dataset Acquisition & Sampling:** download the StrategyQA dataset, sample ~2 000 training examples as recommended, and save them to disk for subsequent processing.\n",
    "- **Prompt Engineering & Teacher Generation:** generate a baseline *student draft* for each question, compose prompts according to the plan (question, student draft, and a teacher instruction), call GPT‑4 (or run in dry‑run mode), and build two parallel corpora for baseline and CoT training.\n",
    "\n",
    "The plan specifies a data‑generation loop where each question is paired with a student draft and a teacher chain‑of‑thought, resulting in two training tracks.  The baseline model is trained on `(Q → answer)` pairs, while the CoT model is trained on `(Q + teacher CoT → answer)` pair.\n",
    "\n",
    "> **Note:** Running the full pipeline (especially calling GPT‑4) requires an OpenAI API key and may incur costs.  A dry‑run mode is provided for testing the notebook without external API calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6bd4dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets transformers openai bitsandbytes accelerate python-dotenv huggingface_hub huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4ae3796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Configuration ===\n",
      "Dataset: voidful/StrategyQA\n",
      "Model: microsoft/phi-2\n",
      "Batch size: 8\n",
      "4-bit quantization: True\n",
      "GPT-4 dry run: False\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Dataset parameters\n",
    "DATASET_NAME = os.getenv('DATASET_NAME', 'voidful/StrategyQA')\n",
    "TRAIN_SAMPLES = int(os.getenv('TRAIN_SAMPLES', '100'))\n",
    "RANDOM_SEED = int(os.getenv('RANDOM_SEED', '42'))\n",
    "\n",
    "# Model parameters\n",
    "MODEL_NAME = os.getenv('MODEL_NAME', 'microsoft/phi-2')\n",
    "MAX_NEW_TOKENS = int(os.getenv('MAX_NEW_TOKENS', '35'))\n",
    "BATCH_SIZE = int(os.getenv('BATCH_SIZE', '8'))\n",
    "USE_4BIT = os.getenv('USE_4BIT', 'True').lower() in ('true', '1', 't')\n",
    "MAX_SEQ_LENGTH = int(os.getenv('MAX_SEQ_LENGTH', '512'))\n",
    "HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN', '')\n",
    "\n",
    "# Generation parameters\n",
    "DO_SAMPLE = os.getenv('DO_SAMPLE', 'False').lower() in ('true', '1', 't')\n",
    "TEMPERATURE = float(os.getenv('TEMPERATURE', '0.7'))\n",
    "\n",
    "# GPT-4 parameters\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')\n",
    "GPT4_MODEL = os.getenv('GPT4_MODEL', 'gpt-4')\n",
    "GPT4_MAX_TOKENS = int(os.getenv('GPT4_MAX_TOKENS', '150'))\n",
    "GPT4_TEMPERATURE = float(os.getenv('GPT4_TEMPERATURE', '0.3'))\n",
    "DRY_RUN = os.getenv('DRY_RUN', 'True').lower() in ('true', '1', 't')\n",
    "\n",
    "# File paths\n",
    "DATA_DIR = os.getenv('DATA_DIR', 'data')\n",
    "RAW_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "SAMPLE_TRAIN_PATH = os.path.join(DATA_DIR, 'sample_train.jsonl')\n",
    "STUDENT_DRAFTS_PATH = os.path.join(DATA_DIR, 'student_drafts.jsonl')\n",
    "TEACHER_OUTPUTS_PATH = os.path.join(DATA_DIR, 'teacher_outputs.jsonl')\n",
    "BASELINE_PATH = os.path.join(DATA_DIR, 'train_baseline.jsonl')\n",
    "COT_PATH = os.path.join(DATA_DIR, 'train_cot.jsonl')\n",
    "\n",
    "# Print configuration\n",
    "print(\"=== Configuration ===\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"4-bit quantization: {USE_4BIT}\")\n",
    "print(f\"GPT-4 dry run: {DRY_RUN}\")\n",
    "print(\"====================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login, notebook_login\n",
    "\n",
    "# def smart_hf_login():\n",
    "#     \"\"\"Use HF_TOKEN env/secret if present, else fall back to interactive login.\"\"\"\n",
    "#     if HUGGINGFACE_TOKEN:         # works for Colab secrets, CI, docker, …\n",
    "#         login(HUGGINGFACE_TOKEN)\n",
    "#     elif 'google.colab' in sys.modules:   # inside a Colab kernel but no secret set\n",
    "#         notebook_login()\n",
    "#     else:                                 # local Jupyter; will prompt only once\n",
    "#         login()\n",
    "\n",
    "# smart_hf_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "149b7c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files in:\n",
      "- Train: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\\strategyqa_train.jsonl\n",
      "- Val: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\\strategyqa_validation.jsonl\n",
      "- Test: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\\strategyqa_test.jsonl\n",
      "Created directory: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\n",
      "Files exist: True\n",
      "Loading dataset from local files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1038 examples [00:00, 61042.70 examples/s]\n",
      "Generating validation split: 565 examples [00:00, 51368.47 examples/s]\n",
      "Generating test split: 687 examples [00:00, 57247.32 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 200 examples with seed 42\n",
      "Full training set saved to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\\strategyqa_train.jsonl\n",
      "Validation set saved to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\\strategyqa_validation.jsonl\n",
      "Sampled train set (≈200 entries) saved to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\sample_train.jsonl\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Check if we need to download the dataset\n",
    "# Fix: Use paths relative to the parent directory (project root)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "raw_dir = os.path.join(parent_dir, DATA_DIR, 'raw')\n",
    "train_path = os.path.join(raw_dir, 'strategyqa_train.jsonl')\n",
    "val_path = os.path.join(raw_dir, 'strategyqa_validation.jsonl')  # Changed from dev to validation to match download script\n",
    "test_path = os.path.join(raw_dir, 'strategyqa_test.jsonl')\n",
    "\n",
    "print(f\"Looking for files in:\")\n",
    "print(f\"- Train: {train_path}\")\n",
    "print(f\"- Val: {val_path}\")\n",
    "print(f\"- Test: {test_path}\")\n",
    "\n",
    "# Create raw directory if it doesn't exist\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "print(f\"Created directory: {raw_dir}\")\n",
    "\n",
    "# Check if files exist\n",
    "files_exist = all(os.path.exists(p) for p in [train_path, val_path])\n",
    "print(f\"Files exist: {files_exist}\")\n",
    "\n",
    "if not files_exist:\n",
    "    print(\"Dataset files not found. Running download script...\")\n",
    "    script_path = os.path.join(parent_dir, 'scripts', 'download_strategyqa.py')\n",
    "    print(f\"Running: {sys.executable} {script_path} --output-dir {raw_dir}\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, script_path, '--output-dir', raw_dir],\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(\"Download script output:\")\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"Errors:\")\n",
    "        print(result.stderr)\n",
    "    \n",
    "    # Verify files were created\n",
    "    print(\"\\nChecking if files were created:\")\n",
    "    for path in [train_path, val_path, test_path]:\n",
    "        exists = os.path.exists(path)\n",
    "        print(f\"- {path}: {'✓' if exists else '✗'}\")\n",
    "        if exists:\n",
    "            size = os.path.getsize(path)\n",
    "            print(f\"  Size: {size:,} bytes\")\n",
    "\n",
    "# Load the dataset from local JSONL files\n",
    "print(\"Loading dataset from local files...\")\n",
    "data_files = {\n",
    "    'train': train_path,\n",
    "    'validation': val_path,\n",
    "}\n",
    "if os.path.exists(test_path):\n",
    "    data_files['test'] = test_path\n",
    "\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "train = dataset['train']\n",
    "validation = dataset['validation']\n",
    "\n",
    "def sample_train_set(train_dataset, n_samples=TRAIN_SAMPLES, seed=RANDOM_SEED):\n",
    "    '''Return a random sample of the training set.'''\n",
    "    shuffled = train_dataset.shuffle(seed=seed)\n",
    "    return shuffled.select(range(min(n_samples, len(shuffled))))\n",
    "\n",
    "# Sample examples from the training set\n",
    "print(f\"Sampling {TRAIN_SAMPLES} examples with seed {RANDOM_SEED}\")\n",
    "target_train = sample_train_set(train)\n",
    "\n",
    "# Create output directories (also fix these paths)\n",
    "data_dir_full = os.path.join(parent_dir, DATA_DIR)\n",
    "os.makedirs(data_dir_full, exist_ok=True)\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "# Save the full dev/test sets and the sampled train set\n",
    "sample_train_path = os.path.join(parent_dir, SAMPLE_TRAIN_PATH)\n",
    "\n",
    "def save_jsonl(dataset_split, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset_split:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Save splits\n",
    "save_jsonl(train, train_path)\n",
    "save_jsonl(validation, val_path)\n",
    "if 'test' in dataset:\n",
    "    save_jsonl(dataset['test'], test_path)\n",
    "save_jsonl(target_train, sample_train_path)\n",
    "\n",
    "print(f\"Full training set saved to {train_path}\")\n",
    "print(f\"Validation set saved to {val_path}\")\n",
    "print(f\"Sampled train set (≈{TRAIN_SAMPLES} entries) saved to {sample_train_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d2250a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model: microsoft/Phi-3.5-mini-instruct\n",
      "Loading model in 4-bit quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory after model load: 2.26 GB\n",
      "Loading dataset from c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\sample_train.jsonl with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 200 examples [00:00, 17865.21 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 1177.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def setup_dataset(input_path: str, tokenizer, batch_size: int = BATCH_SIZE):\n",
    "    \"\"\"Load and prepare dataset for GPU processing.\"\"\"\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset('json', data_files=input_path, split='train')\n",
    "    \n",
    "    # Keep the original questions for reference\n",
    "    original_questions = dataset['question']\n",
    "    \n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['question'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            return_tensors=None  # Return as list, not tensors\n",
    "        )\n",
    "    \n",
    "    # Apply tokenization\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Create a custom dataset that includes both tokenized data and original questions\n",
    "    class QADataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, tokenized_data, original_questions):\n",
    "            self.tokenized_data = tokenized_data\n",
    "            self.original_questions = original_questions\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.tokenized_data)\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            item = {\n",
    "                'input_ids': torch.tensor(self.tokenized_data[idx]['input_ids']),\n",
    "                'attention_mask': torch.tensor(self.tokenized_data[idx]['attention_mask']),\n",
    "                'question': self.original_questions[idx]\n",
    "            }\n",
    "            return item\n",
    "    \n",
    "    # Create custom dataset\n",
    "    custom_dataset = QADataset(tokenized_dataset, original_questions)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    loader = DataLoader(\n",
    "        custom_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False  # Keep order for output matching\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model setup\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Use 4-bit quantization if enabled and on GPU\n",
    "if device.type == 'cuda' and USE_4BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    print(\"Loading model in 4-bit quantization...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading model in standard precision...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# Ensure the tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Memory after model load: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# Load and prepare dataset (fix path)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sample_train_path_full = os.path.join(parent_dir, SAMPLE_TRAIN_PATH)\n",
    "print(f\"Loading dataset from {sample_train_path_full} with batch size {BATCH_SIZE}\")\n",
    "train_loader = setup_dataset(sample_train_path_full, tokenizer, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09e8cc",
   "metadata": {},
   "source": [
    "## Generate Student Drafts\n",
    "\n",
    "In this section we load a base language model (e.g. `meta-llama/Llama-2-7b-hf` or `gpt2`) and generate a short *student draft* for each question in the sampled training set.  A draft consists of a yes/no answer followed by one or two clarifying questions, as specified in the data‑generation loop.  Adjust the model name based on your available hardware and licences.\n",
    "\n",
    "> **Tip:** On Colab, you can enable a GPU via *Runtime → Change runtime type → GPU* and use half‑precision weights to reduce memory usage.  For demonstration, we use `gpt2` (which is small) to keep the example runnable on CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aab9c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages(q: str):\n",
    "    return [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a reasoning assistant that provides initial analysis for complex yes/no questions.\\n\\n\"\n",
    "                   \"RESPONSE FORMAT - EXACTLY two lines:\\n\"\n",
    "                   \"Line 1: Answer: <Yes/No> - <brief reasoning>\\n\"\n",
    "                   \"Line 2: Questions: <focused question>? <key uncertainty>?\\n\\n\"\n",
    "                   \"REASONING APPROACH:\\n\"\n",
    "                   \"- Consider key facts and logical connections\\n\"\n",
    "                   \"- When uncertain, lean toward the more likely answer with caveats\\n\"\n",
    "                   \"- Identify the most critical missing information\\n\"\n",
    "                   \"- Focus on specific, actionable clarifying questions\\n\\n\"\n",
    "                   \"QUALITY CRITERIA:\\n\"\n",
    "                   \"- Your brief reasoning should capture the main logical path\\n\"\n",
    "                   \"- Questions should target genuine uncertainties, not obvious facts\\n\"\n",
    "                   \"- Be specific rather than generic in your inquiries\\n\"\n",
    "                   \"- Consider context, timing, and domain-specific knowledge\"},\n",
    "        \n",
    "        # Improved few-shot examples\n",
    "        {\"role\": \"user\", \"content\": \"Is the sky blue?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Answer: Yes - electromagnetic scattering favors blue wavelengths\\n\"\n",
    "                                        \"Questions: Under what atmospheric conditions? At what time of day?\"},\n",
    "        \n",
    "        {\"role\": \"user\", \"content\": \"Can koalas digest meat?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Answer: No - specialized herbivore digestive system\\n\"\n",
    "                                        \"Questions: What about small amounts accidentally? In emergency situations?\"},\n",
    "        \n",
    "        # Your actual question\n",
    "        {\"role\": \"user\", \"content\": q},\n",
    "    ]\n",
    "\n",
    "def generate_batch_drafts(batch):\n",
    "    \"\"\"Generate drafts for a batch of questions.\"\"\"\n",
    "    # Set padding side to left for generation (decoder-only models need this)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    # Create prompts for each question\n",
    "    prompts = [\n",
    "        tokenizer.apply_chat_template(build_messages(q),\n",
    "                                    tokenize=False,\n",
    "                                    add_generation_prompt=True)\n",
    "        for q in batch[\"question\"]\n",
    "    ]\n",
    "    inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Track memory usage before generation\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"GPU Memory before generation: {torch.cuda.memory_allocated()/1e9:.2f} GB\", end='\\r')\n",
    "    \n",
    "    # Generate responses\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            temperature=TEMPERATURE if DO_SAMPLE else 1.0,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "    prompt_lens = inputs[\"attention_mask\"].sum(dim=1)\n",
    "    decoded = [tokenizer.decode(seq[p_len:], skip_special_tokens=True).strip()\n",
    "            for seq, p_len in zip(outputs.sequences, prompt_lens)]\n",
    "    drafts = [ans[ans.find(\"Answer:\"):] for ans in decoded]\n",
    "    return drafts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3433bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tqdm for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create output directory if it doesn't exist (fix path)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "student_drafts_path_full = os.path.join(parent_dir, STUDENT_DRAFTS_PATH)\n",
    "os.makedirs(os.path.dirname(student_drafts_path_full), exist_ok=True)\n",
    "\n",
    "# Process batches and write outputs\n",
    "print(f\"Writing student drafts to {student_drafts_path_full}\")\n",
    "with open(student_drafts_path_full, 'w', encoding='utf-8') as out_f:\n",
    "    for batch in tqdm(train_loader, desc='Generating drafts'):\n",
    "        # Get original questions directly from the batch\n",
    "        questions = batch['question']\n",
    "        \n",
    "        # Move input_ids and attention_mask to device\n",
    "        input_batch = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        \n",
    "        # Generate drafts for the batch\n",
    "        drafts = generate_batch_drafts({'question': questions})\n",
    "        \n",
    "        # Write results\n",
    "        for q, draft in zip(questions, drafts):\n",
    "            out_rec = {\n",
    "                'question': q,\n",
    "                'student_draft': draft\n",
    "            }\n",
    "            out_f.write(json.dumps(out_rec, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        # Print memory usage periodically\n",
    "        if device.type == 'cuda':\n",
    "            print(f\"Current GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\", end='\\r')\n",
    "\n",
    "print(f\"Student drafts written to {student_drafts_path_full}\")\n",
    "print(\"Using optimized student prompts for better reasoning and question quality!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hac9jegin1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading student drafts from c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\student_drafts.jsonl\n",
      "Cleaning 200 student drafts...\n",
      "Saving cleaned drafts to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\student_drafts_cleaned.jsonl\n",
      "Updating original file c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\student_drafts.jsonl\n",
      "\n",
      "=== CLEANING STATISTICS ===\n",
      "Total drafts processed: 200\n",
      "Already clean: 183\n",
      "Required major cleaning: 17\n",
      "Required extraction: 0\n",
      "Cleaning success rate: 91.5%\n",
      "\n",
      "=== SAMPLE CLEANED DRAFTS ===\n",
      "Example 1:\n",
      "Question: Did Disney get most of Rudyard Kipling's The Jungle Book pro...\n",
      "Draft: Answer: Uncertain - distribution rights and profits depend on agreements\n",
      "Questions: What was the nature of Disney's agreement with Kipling's estate? How are profits typically divided in such cases?\n",
      "\n",
      "Example 2:\n",
      "Question: Did Robert Downey Jr. possess same caliber gun as Resident E...\n",
      "Draft: Answer: No - Robert Downey Jr. is an actor, not a gun manufacturer or user\n",
      "Questions: Is there a movie or context where both are featured together? What is the nature of their comparison in that context?\n",
      "\n",
      "Example 3:\n",
      "Question: Can you get a ride on Amtrak to the Underworld?...\n",
      "Draft: Answer: No - Amtrak is a passenger rail service and does not have access to the Underworld\n",
      "Questions: What is the Underworld typically referred to in popular culture? Are there any fictional transportation services that offer access to the Underworld?\n",
      "\n",
      "✅ Student drafts cleaned successfully!\n",
      "All drafts now contain only Answer and Questions lines without extra reasoning.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def clean_student_draft(draft_text):\n",
    "    \"\"\"\n",
    "    Clean student draft to keep only Answer and Questions lines.\n",
    "    Removes extra reasoning, paragraphs, and explanatory text.\n",
    "    \"\"\"\n",
    "    lines = draft_text.strip().split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Keep lines that start with \"Answer:\" or \"Questions:\"\n",
    "        if line.startswith('Answer:') or line.startswith('Questions:'):\n",
    "            cleaned_lines.append(line)\n",
    "        # Stop processing if we hit any reasoning sections\n",
    "        elif any(keyword in line.upper() for keyword in ['REASONING APPROACH:', 'QUALITY CRITERIA:', 'TO CLARIFY', 'IN THIS CASE']):\n",
    "            break\n",
    "    \n",
    "    # If we have both Answer and Questions, return them\n",
    "    if len(cleaned_lines) >= 2:\n",
    "        return '\\n'.join(cleaned_lines[:2])  # Keep only first Answer and Questions lines\n",
    "    elif len(cleaned_lines) == 1 and cleaned_lines[0].startswith('Answer:'):\n",
    "        # If only Answer line, add a generic Questions line\n",
    "        return cleaned_lines[0] + '\\nQuestions: What additional context is needed?'\n",
    "    else:\n",
    "        # Fallback: extract from the full text\n",
    "        return extract_answer_and_questions(draft_text)\n",
    "\n",
    "def extract_answer_and_questions(text):\n",
    "    \"\"\"\n",
    "    Extract Answer and Questions from messy text using regex patterns.\n",
    "    \"\"\"\n",
    "    # Look for Answer pattern\n",
    "    answer_match = re.search(r'Answer:\\s*([^\\n]+)', text, re.IGNORECASE)\n",
    "    answer = answer_match.group(0) if answer_match else \"Answer: Unable to determine\"\n",
    "    \n",
    "    # Look for Questions pattern\n",
    "    questions_match = re.search(r'Questions?:\\s*([^\\n]+(?:\\?[^\\n]*)*)', text, re.IGNORECASE | re.MULTILINE)\n",
    "    if questions_match:\n",
    "        questions = f\"Questions: {questions_match.group(1)}\"\n",
    "    else:\n",
    "        # Look for question marks in the text as a fallback\n",
    "        question_lines = [line.strip() for line in text.split('\\n') if '?' in line and not line.startswith('Answer:')]\n",
    "        if question_lines:\n",
    "            questions = f\"Questions: {question_lines[0]}\"\n",
    "        else:\n",
    "            questions = \"Questions: What additional context is needed?\"\n",
    "    \n",
    "    return f\"{answer}\\n{questions}\"\n",
    "\n",
    "# Load the existing student drafts\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "student_drafts_path_full = os.path.join(parent_dir, STUDENT_DRAFTS_PATH)\n",
    "cleaned_drafts_path = os.path.join(parent_dir, DATA_DIR, 'student_drafts_cleaned.jsonl')\n",
    "\n",
    "print(f\"Loading student drafts from {student_drafts_path_full}\")\n",
    "with open(student_drafts_path_full, 'r', encoding='utf-8') as f:\n",
    "    drafts = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Cleaning {len(drafts)} student drafts...\")\n",
    "cleaned_count = 0\n",
    "stats = {\n",
    "    'total': len(drafts),\n",
    "    'cleaned': 0,\n",
    "    'already_clean': 0,\n",
    "    'extracted': 0\n",
    "}\n",
    "\n",
    "# Clean each draft\n",
    "cleaned_drafts = []\n",
    "for draft in drafts:\n",
    "    original_text = draft['student_draft']\n",
    "    \n",
    "    # Check if already clean (only has Answer and Questions lines)\n",
    "    lines = [line.strip() for line in original_text.split('\\n') if line.strip()]\n",
    "    if (len(lines) == 2 and \n",
    "        lines[0].startswith('Answer:') and \n",
    "        lines[1].startswith('Questions:')):\n",
    "        # Already clean\n",
    "        cleaned_drafts.append(draft)\n",
    "        stats['already_clean'] += 1\n",
    "    else:\n",
    "        # Needs cleaning\n",
    "        cleaned_text = clean_student_draft(original_text)\n",
    "        cleaned_draft = {\n",
    "            'question': draft['question'],\n",
    "            'student_draft': cleaned_text\n",
    "        }\n",
    "        cleaned_drafts.append(cleaned_draft)\n",
    "        \n",
    "        # Count the type of cleaning performed\n",
    "        if len(original_text) > len(cleaned_text) * 2:  # Significant reduction\n",
    "            stats['cleaned'] += 1\n",
    "        else:\n",
    "            stats['extracted'] += 1\n",
    "\n",
    "# Save cleaned drafts\n",
    "print(f\"Saving cleaned drafts to {cleaned_drafts_path}\")\n",
    "with open(cleaned_drafts_path, 'w', encoding='utf-8') as f:\n",
    "    for draft in cleaned_drafts:\n",
    "        f.write(json.dumps(draft, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Also update the original file\n",
    "print(f\"Updating original file {student_drafts_path_full}\")\n",
    "with open(student_drafts_path_full, 'w', encoding='utf-8') as f:\n",
    "    for draft in cleaned_drafts:\n",
    "        f.write(json.dumps(draft, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"\\n=== CLEANING STATISTICS ===\")\n",
    "print(f\"Total drafts processed: {stats['total']}\")\n",
    "print(f\"Already clean: {stats['already_clean']}\")\n",
    "print(f\"Required major cleaning: {stats['cleaned']}\")\n",
    "print(f\"Required extraction: {stats['extracted']}\")\n",
    "print(f\"Cleaning success rate: {((stats['total'] - (stats['cleaned'] + stats['extracted']))/stats['total']*100):.1f}%\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\n=== SAMPLE CLEANED DRAFTS ===\")\n",
    "for i, draft in enumerate(cleaned_drafts[:3]):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Question: {draft['question'][:60]}...\")\n",
    "    print(f\"Draft: {draft['student_draft']}\")\n",
    "    print()\n",
    "\n",
    "print(\"✅ Student drafts cleaned successfully!\")\n",
    "print(\"All drafts now contain only Answer and Questions lines without extra reasoning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105e4488",
   "metadata": {},
   "source": [
    "## Generate Teacher Responses\n",
    "\n",
    "We now call GPT‑4 to obtain chain‑of‑thought (CoT) reasoning and final yes/no answers for each question/draft pair.  The prompt format follows the plan:\n",
    "\n",
    "```\n",
    "Q: <original yes/no question>\n",
    "Student draft: <answer + clarifying questions>\n",
    "Teacher: Please think step-by-step and provide your thought process and final Yes/No answer.\n",
    "```\n",
    "\n",
    "To run the actual API calls, you must provide a valid OpenAI API key.  If you set `dry_run=True`, dummy responses will be generated for testing purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8082d0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading student drafts from c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\student_drafts.jsonl\n",
      "Generating teacher responses with improved prompts - model: gpt-4.1-nano-2025-04-14 (dry_run: False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating teacher responses: 100%|██████████| 200/200 [09:26<00:00,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher outputs written to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\teacher_outputs.jsonl\n",
      "Improved prompts now provide structured responses for reliable parsing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load student drafts (fix path)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "student_drafts_path_full = os.path.join(parent_dir, STUDENT_DRAFTS_PATH)\n",
    "print(f\"Loading student drafts from {student_drafts_path_full}\")\n",
    "with open(student_drafts_path_full, 'r', encoding='utf-8') as f:\n",
    "    drafts = [json.loads(line) for line in f]\n",
    "\n",
    "# Get API key from environment\n",
    "if not OPENAI_API_KEY and not DRY_RUN:\n",
    "    print(\"Warning: OPENAI_API_KEY not set. Set DRY_RUN=True or provide an API key.\")\n",
    "\n",
    "def extract_yes_no(text: str) -> str:\n",
    "    \"\"\"Extract a yes/no answer from the teacher's structured response.\n",
    "    \n",
    "    Looks for the Final Assessment section with **YES** or **NO** in bold.\n",
    "    Falls back to searching for yes/no in the text if structured format not found.\n",
    "    \"\"\"\n",
    "    # First, try to find the structured final assessment\n",
    "    final_assessment_match = re.search(r'## Final Assessment.*?\\*\\*(YES|NO)\\*\\*', text, re.IGNORECASE | re.DOTALL)\n",
    "    if final_assessment_match:\n",
    "        return final_assessment_match.group(1).capitalize()\n",
    "    \n",
    "    # Fallback to original method for backwards compatibility\n",
    "    match = re.search(r\"\\b(yes|no)\\b\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).capitalize()\n",
    "    \n",
    "    return \"No\"  # Default fallback instead of returning full text\n",
    "\n",
    "def call_gpt4(q: str, draft: str) -> str:\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert teacher helping a student AI model learn to reason through complex yes/no questions. Your role is to provide educational reasoning that demonstrates good thinking patterns.\n",
    "\n",
    "CRITICAL: You must follow this exact response format:\n",
    "\n",
    "## Teaching Analysis\n",
    "[Provide 2-3 sentences acknowledging the student's approach and identifying key reasoning steps needed]\n",
    "\n",
    "## Step-by-Step Reasoning\n",
    "[Provide clear, educational reasoning in numbered steps that the student can learn from and apply to similar questions]\n",
    "\n",
    "## Final Assessment\n",
    "Based on this analysis, the answer is: **[YES/NO]**\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Always use the exact format above with the specified headers\n",
    "- Keep your reasoning educational and transferable\n",
    "- Address the student's specific concerns when they raise valid points\n",
    "- The Final Assessment section must contain exactly \"**YES**\" or \"**NO**\" in bold\n",
    "- Be concise but thorough in your explanations\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"QUESTION: {q}\n",
    "\n",
    "STUDENT DRAFT: {draft}\n",
    "\n",
    "Provide your teaching analysis following the required format.\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT4_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        max_tokens=GPT4_MAX_TOKENS,\n",
    "        temperature=GPT4_TEMPERATURE\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Create output directory if it doesn't exist (fix path)\n",
    "teacher_outputs_path_full = os.path.join(parent_dir, TEACHER_OUTPUTS_PATH)\n",
    "os.makedirs(os.path.dirname(teacher_outputs_path_full), exist_ok=True)\n",
    "\n",
    "print(f\"Generating teacher responses with improved prompts - model: {GPT4_MODEL} (dry_run: {DRY_RUN})\")\n",
    "with open(teacher_outputs_path_full, 'w', encoding='utf-8') as out_f:\n",
    "    for rec in tqdm(drafts, desc='Generating teacher responses'):\n",
    "        q = rec['question']\n",
    "        draft = rec['student_draft']\n",
    "        \n",
    "        if DRY_RUN or not OPENAI_API_KEY:\n",
    "            response_text = '''## Teaching Analysis\n",
    "Good approach with clarifying questions. Let me analyze this systematically.\n",
    "\n",
    "## Step-by-Step Reasoning\n",
    "1. This is a placeholder reasoning step for dry run mode\n",
    "2. Replace DRY_RUN with False for real GPT-4 calls\n",
    "3. The structured format ensures easy parsing\n",
    "\n",
    "## Final Assessment\n",
    "Based on this analysis, the answer is: **NO**'''\n",
    "        else:\n",
    "            response_text = call_gpt4(q, draft)\n",
    "            \n",
    "        answer = extract_yes_no(response_text)\n",
    "        out_record = {\n",
    "            'question': q,\n",
    "            'student_draft': draft,\n",
    "            'teacher_thought': response_text,\n",
    "            'teacher_answer': answer\n",
    "        }\n",
    "        out_f.write(json.dumps(out_record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Teacher outputs written to {teacher_outputs_path_full}\")\n",
    "print(\"Improved prompts now provide structured responses for reliable parsing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3c20c",
   "metadata": {},
   "source": [
    "## Build Training Corpora\n",
    "\n",
    "Finally, we build two parallel training corpora:\n",
    "\n",
    "1. **Baseline (Track A)** – pairs of `(question → answer)` for training a basic model.\n",
    "2. **CoT (Track B)** – pairs of `(question + teacher chain-of-thought → answer)` for CoT distillation.\n",
    "\n",
    "These files will be used in later steps for model fine‑tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dec8f3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading teacher outputs from c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\teacher_outputs.jsonl\n",
      "Loading ground truth data from c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\sample_train.jsonl\n",
      "Processing and validating teacher responses...\n",
      "Writing validated baseline corpus to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\train_baseline.jsonl\n",
      "Writing validated CoT corpus to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\train_cot.jsonl\n",
      "Baseline corpus saved to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\train_baseline.jsonl\n",
      "CoT corpus saved to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\train_cot.jsonl\n",
      "\n",
      "=== VALIDATION STATISTICS ===\n",
      "Total processed: 200\n",
      "Ground truth missing: 0\n",
      "Invalid teacher answers (not Yes/No): 0\n",
      "Wrong teacher answers (vs ground truth): 63\n",
      "Valid records kept: 137\n",
      "Data quality rate: 68.5%\n",
      "\n",
      "Final training corpora:\n",
      "- Baseline: 137 validated examples\n",
      "- CoT: 137 validated examples\n",
      "\n",
      "All training data is now validated against ground truth and contains only clean Yes/No answers!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load teacher outputs (fix path)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "teacher_outputs_path_full = os.path.join(parent_dir, TEACHER_OUTPUTS_PATH)\n",
    "print(f\"Loading teacher outputs from {teacher_outputs_path_full}\")\n",
    "with open(teacher_outputs_path_full, 'r', encoding='utf-8') as f:\n",
    "    teacher_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Load original training data with ground truth answers\n",
    "sample_train_path_full = os.path.join(parent_dir, SAMPLE_TRAIN_PATH)\n",
    "print(f\"Loading ground truth data from {sample_train_path_full}\")\n",
    "with open(sample_train_path_full, 'r', encoding='utf-8') as f:\n",
    "    ground_truth_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Create a mapping from question to ground truth answer\n",
    "ground_truth_map = {item['question']: item['answer'] for item in ground_truth_data}\n",
    "\n",
    "def is_valid_answer(answer: str) -> bool:\n",
    "    \"\"\"Check if the answer is a valid Yes/No response.\"\"\"\n",
    "    return answer.strip().lower() in ['yes', 'no']\n",
    "\n",
    "def convert_to_yes_no(boolean_answer: bool) -> str:\n",
    "    \"\"\"Convert boolean ground truth to Yes/No string.\"\"\"\n",
    "    return \"Yes\" if boolean_answer else \"No\"\n",
    "\n",
    "# Create baseline and CoT records with validation\n",
    "baseline_records = []\n",
    "cot_records = []\n",
    "validation_stats = {\n",
    "    'total_processed': 0,\n",
    "    'invalid_teacher_answers': 0,\n",
    "    'teacher_wrong_answers': 0,\n",
    "    'valid_records': 0,\n",
    "    'ground_truth_missing': 0\n",
    "}\n",
    "\n",
    "print(\"Processing and validating teacher responses...\")\n",
    "for rec in teacher_data:\n",
    "    q = rec['question']\n",
    "    draft = rec['student_draft']\n",
    "    thought = rec['teacher_thought']\n",
    "    teacher_answer = rec['teacher_answer']\n",
    "    \n",
    "    validation_stats['total_processed'] += 1\n",
    "    \n",
    "    # Check if we have ground truth for this question\n",
    "    if q not in ground_truth_map:\n",
    "        validation_stats['ground_truth_missing'] += 1\n",
    "        print(f\"Warning: No ground truth found for question: {q[:50]}...\")\n",
    "        continue\n",
    "    \n",
    "    ground_truth_answer = convert_to_yes_no(ground_truth_map[q])\n",
    "    \n",
    "    # Validate teacher answer format\n",
    "    if not is_valid_answer(teacher_answer):\n",
    "        validation_stats['invalid_teacher_answers'] += 1\n",
    "        print(f\"Skipping invalid teacher answer: '{teacher_answer}' for question: {q[:50]}...\")\n",
    "        continue\n",
    "    \n",
    "    # Check if teacher answer matches ground truth\n",
    "    if teacher_answer.strip().capitalize() != ground_truth_answer:\n",
    "        validation_stats['teacher_wrong_answers'] += 1\n",
    "        # print(f\"Skipping incorrect teacher answer: Teacher='{teacher_answer}', Truth='{ground_truth_answer}' for question: {q[:50]}...\")\n",
    "        continue\n",
    "    \n",
    "    # If we reach here, the data point is valid\n",
    "    validation_stats['valid_records'] += 1\n",
    "    \n",
    "    # Track A: Baseline (question → answer)\n",
    "    baseline_records.append({'prompt': q, 'answer': teacher_answer})\n",
    "    \n",
    "    # Track B: CoT with student draft context (self-improvement format)\n",
    "    cot_prompt = f\"Question: {q}\\nStudent draft: {draft}\\nTeacher reasoning: {thought}\"\n",
    "    cot_records.append({'prompt': cot_prompt, 'answer': teacher_answer})\n",
    "\n",
    "# Create output directories if they don't exist (fix paths)\n",
    "baseline_path_full = os.path.join(parent_dir, BASELINE_PATH)\n",
    "cot_path_full = os.path.join(parent_dir, COT_PATH)\n",
    "os.makedirs(os.path.dirname(baseline_path_full), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(cot_path_full), exist_ok=True)\n",
    "\n",
    "# Write output files\n",
    "print(f\"Writing validated baseline corpus to {baseline_path_full}\")\n",
    "with open(baseline_path_full, 'w', encoding='utf-8') as f:\n",
    "    for r in baseline_records:\n",
    "        f.write(json.dumps(r) + '\\n')\n",
    "\n",
    "print(f\"Writing validated CoT corpus to {cot_path_full}\")\n",
    "with open(cot_path_full, 'w', encoding='utf-8') as f:\n",
    "    for r in cot_records:\n",
    "        f.write(json.dumps(r) + '\\n')\n",
    "\n",
    "print(f\"Baseline corpus saved to {baseline_path_full}\")\n",
    "print(f\"CoT corpus saved to {cot_path_full}\")\n",
    "\n",
    "# Print validation statistics\n",
    "print(f\"\\n=== VALIDATION STATISTICS ===\")\n",
    "print(f\"Total processed: {validation_stats['total_processed']}\")\n",
    "print(f\"Ground truth missing: {validation_stats['ground_truth_missing']}\")\n",
    "print(f\"Invalid teacher answers (not Yes/No): {validation_stats['invalid_teacher_answers']}\")\n",
    "print(f\"Wrong teacher answers (vs ground truth): {validation_stats['teacher_wrong_answers']}\")\n",
    "print(f\"Valid records kept: {validation_stats['valid_records']}\")\n",
    "print(f\"Data quality rate: {validation_stats['valid_records']/validation_stats['total_processed']*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nFinal training corpora:\")\n",
    "print(f\"- Baseline: {len(baseline_records)} validated examples\")\n",
    "print(f\"- CoT: {len(cot_records)} validated examples\")\n",
    "print(f\"\\nAll training data is now validated against ground truth and contains only clean Yes/No answers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sprmg3jr9n",
   "metadata": {},
   "source": [
    "## Phase A: Baseline Training\n",
    "\n",
    "Phase A trains the student model on direct question-answer pairs without CoT reasoning. This establishes a baseline performance before implementing self-improvement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f338to89gaj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase A: Baseline Training ===\n",
      "Model: microsoft/phi-2\n",
      "Training file: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\train_baseline.jsonl\n",
      "Output directory: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\models\\baseline_phaseA\n",
      "Max sequence length: 2048\n",
      "Training for 3 epochs\n",
      "Loading model for training: microsoft/phi-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\noham\\.cache\\huggingface\\hub\\models--microsoft--phi-2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 2 files: 100%|██████████| 2/2 [01:58<00:00, 59.18s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prepared for 4-bit training\n",
      "trainable params: 7,864,320 || all params: 2,787,548,160 || trainable%: 0.2821\n",
      "✅ LoRA adapter successfully attached\n",
      "GPU Memory after model setup: 2.38 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,      \n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Phase A Configuration\n",
    "PHASE_A_CONFIG = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'train_file': os.path.join(parent_dir, BASELINE_PATH),\n",
    "    'output_dir': os.path.join(parent_dir, 'models', \n",
    "'baseline_phaseA'),\n",
    "    'max_length': MAX_SEQ_LENGTH,\n",
    "    'num_epochs': 3,\n",
    "    'batch_size': 4,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'learning_rate': 2e-4,\n",
    "    'use_4bit': USE_4BIT\n",
    "}\n",
    "\n",
    "print(\"=== Phase A: Baseline Training ===\")   \n",
    "print(f\"Model: {PHASE_A_CONFIG['model_name']}\")\n",
    "print(f\"Training file: {PHASE_A_CONFIG['train_file']}\")\n",
    "print(f\"Output directory: {PHASE_A_CONFIG['output_dir']}\")\n",
    "print(f\"Max sequence length: {PHASE_A_CONFIG['max_length']}\")\n",
    "print(f\"Training for {PHASE_A_CONFIG['num_epochs']} epochs\")\n",
    "\n",
    "# Clear previous model from memory\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "if 'tokenizer' in locals():\n",
    "    del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load fresh model for training\n",
    "print(f\"Loading model for training: {PHASE_A_CONFIG['model_name']}\")\n",
    "\n",
    "if PHASE_A_CONFIG['use_4bit']:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,       \n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16  \n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        PHASE_A_CONFIG['model_name'],\n",
    "        quantization_config=bnb_config,       \n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # Prepare model for k-bit training first  \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    print(\"Model prepared for 4-bit training\")\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        PHASE_A_CONFIG['model_name'],\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PHASE_A_CONFIG['model_name'])\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "# LoRA Configuration - Compatible with 4-bit quantization\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],  # Phi-3.5 specific modules\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "try:\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()        \n",
    "    print(\"✅ LoRA adapter successfully attached\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error applying LoRA: {e}\")     \n",
    "    print(\"Troubleshooting: Using alternative LoRA configuration...\")\n",
    "    \n",
    "    # Alternative LoRA config for compatibility\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,  # Smaller rank\n",
    "        lora_alpha=16,\n",
    "        target_modules=\"all-linear\",  # Auto-detect linear layers\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()        \n",
    "    print(\"✅ Alternative LoRA configuration applied successfully\")\n",
    "\n",
    "print(f\"GPU Memory after model setup: {torch.cuda.memory_allocated()/1e9:.2f} GB\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bxmk7ue4cw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baseline training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 137/137 [00:00<00:00, 1108.54 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 137\n",
      "Adjusted batch size: 2, grad accumulation: 16\n",
      "Starting Phase A training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 24:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.749100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.615900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Phase A training completed!\n",
      "Model saved to: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\models\\baseline_phaseA\n",
      "Final GPU Memory: 2.46 GB\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare training data\n",
    "print(\"Loading baseline training data...\")\n",
    "train_dataset = load_dataset('json', data_files=PHASE_A_CONFIG['train_file'], split='train')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Format: \"Question: {prompt}\\nAnswer: {answer}\"\n",
    "    texts = [f\"Question: {prompt}\\nAnswer: {answer}\" for prompt, answer in zip(examples['prompt'], examples['answer'])]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',  # Fix: Use consistent padding\n",
    "        max_length=PHASE_A_CONFIG['max_length'],\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "\n",
    "# Data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "print(f\"Training dataset size: {len(tokenized_dataset)}\")\n",
    "\n",
    "# Adjust batch size for small dataset\n",
    "effective_batch_size = min(32, len(tokenized_dataset))\n",
    "batch_size = 2\n",
    "gradient_accumulation_steps = max(1, effective_batch_size // batch_size)\n",
    "\n",
    "print(f\"Adjusted batch size: {batch_size}, grad accumulation: {gradient_accumulation_steps}\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=PHASE_A_CONFIG['output_dir'],\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=PHASE_A_CONFIG['num_epochs'],\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=PHASE_A_CONFIG['learning_rate'],\n",
    "    warmup_steps=5,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    fp16=True,\n",
    "    report_to=None,\n",
    "    dataloader_num_workers=0,\n",
    ")\n",
    "\n",
    "# Create and run trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting Phase A training...\")\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "\n",
    "print(\"✅ Phase A training completed!\")\n",
    "print(f\"Model saved to: {PHASE_A_CONFIG['output_dir']}\")\n",
    "print(f\"Final GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79082cymru",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     69\u001b[39m parent_dir = os.path.dirname(os.getcwd())\n\u001b[32m     70\u001b[39m test_file = os.path.join(parent_dir, \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstrategyqa_test.jsonl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m baseline_accuracy = evaluate_model_on_test(model, tokenizer, test_file, \u001b[43mdevice\u001b[49m)\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Save results for comparison with later phases\u001b[39;00m\n\u001b[32m     75\u001b[39m results = {\n\u001b[32m     76\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mphase_a_baseline_accuracy\u001b[39m\u001b[33m'\u001b[39m: baseline_accuracy,\n\u001b[32m     77\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel_path\u001b[39m\u001b[33m'\u001b[39m: PHASE_A_CONFIG[\u001b[33m'\u001b[39m\u001b[33moutput_dir\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     78\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdataset_size\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(tokenized_dataset),\n\u001b[32m     79\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtraining_epochs\u001b[39m\u001b[33m'\u001b[39m: PHASE_A_CONFIG[\u001b[33m'\u001b[39m\u001b[33mnum_epochs\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     80\u001b[39m }\n",
      "\u001b[31mNameError\u001b[39m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def evaluate_model_on_test(model, tokenizer, test_file, device):\n",
    "    \"\"\"Evaluate the model on test set and return accuracy.\"\"\"\n",
    "    \n",
    "    # Load test data\n",
    "    test_dataset = load_dataset('json', data_files=test_file, split='train')\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(f\"Evaluating on {len(test_dataset)} test examples...\")\n",
    "    \n",
    "    for example in tqdm(test_dataset, desc=\"Evaluating\"):\n",
    "        question = example['question']\n",
    "        ground_truth = \"Yes\" if example['answer'] else \"No\"\n",
    "        \n",
    "        # Format prompt same as training\n",
    "        prompt = f\"Question: {question}\\nAnswer:\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=5,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        # Extract generated answer\n",
    "        generated_text = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "        \n",
    "        # Parse Yes/No answer\n",
    "        predicted_answer = \"No\"  # Default\n",
    "        if re.search(r'\\byes\\b', generated_text.lower()):\n",
    "            predicted_answer = \"Yes\"\n",
    "        elif re.search(r'\\bno\\b', generated_text.lower()):\n",
    "            predicted_answer = \"No\"\n",
    "        \n",
    "        # Check correctness\n",
    "        if predicted_answer == ground_truth:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        # Debug first few examples\n",
    "        if total <= 3:\n",
    "            print(f\"Q: {question[:50]}...\")\n",
    "            print(f\"Generated: '{generated_text.strip()}'\")\n",
    "            print(f\"Predicted: {predicted_answer}, Ground truth: {ground_truth}\")\n",
    "            print(\"---\")\n",
    "    \n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"\\n=== Phase A Baseline Results ===\")\n",
    "    print(f\"Test Accuracy: {accuracy:.1f}% ({correct}/{total})\")\n",
    "    print(f\"Target was ~60%, {'✅ SUCCESS' if accuracy >= 55 else '⚠️ BELOW TARGET'}\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Run evaluation on test set\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "test_file = os.path.join(parent_dir, 'data', 'raw', 'strategyqa_test.jsonl')\n",
    "\n",
    "baseline_accuracy = evaluate_model_on_test(model, tokenizer, test_file, device)\n",
    "\n",
    "# Save results for comparison with later phases\n",
    "results = {\n",
    "    'phase_a_baseline_accuracy': baseline_accuracy,\n",
    "    'model_path': PHASE_A_CONFIG['output_dir'],\n",
    "    'dataset_size': len(tokenized_dataset),\n",
    "    'training_epochs': PHASE_A_CONFIG['num_epochs']\n",
    "}\n",
    "\n",
    "results_file = os.path.join(parent_dir, 'results_phase_a.json')\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623zbqyjbqx",
   "source": "## Phase A Evaluation\n\nNow we evaluate the baseline model on the test set to establish our performance baseline. According to the training plan, we expect around 60% accuracy from the baseline model.\n\n**What this evaluation does:**\n- Loads the StrategyQA test set (687 examples)\n- Generates Yes/No answers using the trained baseline model\n- Compares predictions against ground truth labels\n- Calculates accuracy and saves results for comparison with future phases\n\nThis baseline accuracy is crucial for measuring the effectiveness of Phase B (CoT distillation) and Phase C (DPO alignment), which should achieve +7-10pp and +10pp improvements respectively.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}