{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0675efc",
   "metadata": {},
   "source": [
    "# Self‑Improving LLM Project\n",
    "\n",
    "This notebook implements Parts 2 and 3 of the project plan for the **Self‑Improving LLM** final project.  Specifically, it covers:\n",
    "\n",
    "- **Dataset Acquisition & Sampling:** download the StrategyQA dataset, sample ~2 000 training examples as recommended, and save them to disk for subsequent processing.\n",
    "- **Prompt Engineering & Teacher Generation:** generate a baseline *student draft* for each question, compose prompts according to the plan (question, student draft, and a teacher instruction), call GPT‑4 (or run in dry‑run mode), and build two parallel corpora for baseline and CoT training.\n",
    "\n",
    "The plan specifies a data‑generation loop where each question is paired with a student draft and a teacher chain‑of‑thought, resulting in two training tracks.  The baseline model is trained on `(Q → answer)` pairs, while the CoT model is trained on `(Q + teacher CoT → answer)` pair.\n",
    "\n",
    "> **Note:** Running the full pipeline (especially calling GPT‑4) requires an OpenAI API key and may incur costs.  A dry‑run mode is provided for testing the notebook without external API calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers openai bitsandbytes accelerate python-dotenv huggingface_hub huggingface_hub[hf_xet]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae3796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Dataset parameters\n",
    "DATASET_NAME = os.getenv('DATASET_NAME', 'voidful/StrategyQA')\n",
    "TRAIN_SAMPLES = int(os.getenv('TRAIN_SAMPLES', '100'))\n",
    "RANDOM_SEED = int(os.getenv('RANDOM_SEED', '42'))\n",
    "\n",
    "# Model parameters\n",
    "MODEL_NAME = os.getenv('MODEL_NAME', 'microsoft/phi-2')\n",
    "MAX_NEW_TOKENS = int(os.getenv('MAX_NEW_TOKENS', '35'))\n",
    "BATCH_SIZE = int(os.getenv('BATCH_SIZE', '8'))\n",
    "USE_4BIT = os.getenv('USE_4BIT', 'True').lower() in ('true', '1', 't')\n",
    "MAX_SEQ_LENGTH = int(os.getenv('MAX_SEQ_LENGTH', '512'))\n",
    "HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN', '')\n",
    "\n",
    "# Generation parameters\n",
    "DO_SAMPLE = os.getenv('DO_SAMPLE', 'False').lower() in ('true', '1', 't')\n",
    "TEMPERATURE = float(os.getenv('TEMPERATURE', '0.7'))\n",
    "\n",
    "# GPT-4 parameters\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')\n",
    "GPT4_MODEL = os.getenv('GPT4_MODEL', 'gpt-4')\n",
    "GPT4_MAX_TOKENS = int(os.getenv('GPT4_MAX_TOKENS', '150'))\n",
    "GPT4_TEMPERATURE = float(os.getenv('GPT4_TEMPERATURE', '0.3'))\n",
    "DRY_RUN = os.getenv('DRY_RUN', 'True').lower() in ('true', '1', 't')\n",
    "\n",
    "# File paths\n",
    "DATA_DIR = os.getenv('DATA_DIR', 'data')\n",
    "RAW_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "SAMPLE_TRAIN_PATH = os.path.join(DATA_DIR, 'sample_train.jsonl')\n",
    "STUDENT_DRAFTS_PATH = os.path.join(DATA_DIR, 'student_drafts.jsonl')\n",
    "TEACHER_OUTPUTS_PATH = os.path.join(DATA_DIR, 'teacher_outputs.jsonl')\n",
    "BASELINE_PATH = os.path.join(DATA_DIR, 'train_baseline.jsonl')\n",
    "COT_PATH = os.path.join(DATA_DIR, 'train_cot.jsonl')\n",
    "\n",
    "# Print configuration\n",
    "print(\"=== Configuration ===\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"4-bit quantization: {USE_4BIT}\")\n",
    "print(f\"GPT-4 dry run: {DRY_RUN}\")\n",
    "print(\"====================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login, notebook_login\n",
    "\n",
    "# def smart_hf_login():\n",
    "#     \"\"\"Use HF_TOKEN env/secret if present, else fall back to interactive login.\"\"\"\n",
    "#     if HUGGINGFACE_TOKEN:         # works for Colab secrets, CI, docker, …\n",
    "#         login(HUGGINGFACE_TOKEN)\n",
    "#     elif 'google.colab' in sys.modules:   # inside a Colab kernel but no secret set\n",
    "#         notebook_login()\n",
    "#     else:                                 # local Jupyter; will prompt only once\n",
    "#         login()\n",
    "\n",
    "# smart_hf_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Check if we need to download the dataset\n",
    "raw_dir = os.path.join(DATA_DIR, 'raw')\n",
    "train_path = os.path.join(raw_dir, 'strategyqa_train.jsonl')\n",
    "val_path = os.path.join(raw_dir, 'strategyqa_validation.jsonl')  # Changed from dev to validation to match download script\n",
    "test_path = os.path.join(raw_dir, 'strategyqa_test.jsonl')\n",
    "\n",
    "print(f\"Looking for files in:\")\n",
    "print(f\"- Train: {train_path}\")\n",
    "print(f\"- Val: {val_path}\")\n",
    "print(f\"- Test: {test_path}\")\n",
    "\n",
    "# Create raw directory if it doesn't exist\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "print(f\"Created directory: {raw_dir}\")\n",
    "\n",
    "# Check if files exist\n",
    "files_exist = all(os.path.exists(p) for p in [train_path, val_path])\n",
    "print(f\"Files exist: {files_exist}\")\n",
    "\n",
    "if not files_exist:\n",
    "    print(\"Dataset files not found. Running download script...\")\n",
    "    script_path = os.path.join('scripts', 'download_strategyqa.py')\n",
    "    print(f\"Running: {sys.executable} {script_path} --output-dir {raw_dir}\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, script_path, '--output-dir', raw_dir],\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(\"Download script output:\")\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"Errors:\")\n",
    "        print(result.stderr)\n",
    "    \n",
    "    # Verify files were created\n",
    "    print(\"\\nChecking if files were created:\")\n",
    "    for path in [train_path, val_path, test_path]:\n",
    "        exists = os.path.exists(path)\n",
    "        print(f\"- {path}: {'✓' if exists else '✗'}\")\n",
    "        if exists:\n",
    "            size = os.path.getsize(path)\n",
    "            print(f\"  Size: {size:,} bytes\")\n",
    "\n",
    "# Load the dataset from local JSONL files\n",
    "print(\"Loading dataset from local files...\")\n",
    "data_files = {\n",
    "    'train': train_path,\n",
    "    'validation': val_path,\n",
    "}\n",
    "if os.path.exists(test_path):\n",
    "    data_files['test'] = test_path\n",
    "\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "train = dataset['train']\n",
    "validation = dataset['validation']\n",
    "\n",
    "def sample_train_set(train_dataset, n_samples=TRAIN_SAMPLES, seed=RANDOM_SEED):\n",
    "    '''Return a random sample of the training set.'''\n",
    "    shuffled = train_dataset.shuffle(seed=seed)\n",
    "    return shuffled.select(range(min(n_samples, len(shuffled))))\n",
    "\n",
    "# Sample examples from the training set\n",
    "print(f\"Sampling {TRAIN_SAMPLES} examples with seed {RANDOM_SEED}\")\n",
    "target_train = sample_train_set(train)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "# Save the full dev/test sets and the sampled train set\n",
    "train_path = os.path.join(raw_dir, 'strategyqa_train.jsonl')\n",
    "val_path = os.path.join(raw_dir, 'strategyqa_validation.jsonl')  # Changed from dev to validation to match download script\n",
    "test_path = os.path.join(raw_dir, 'strategyqa_test.jsonl')\n",
    "sample_train_path = SAMPLE_TRAIN_PATH\n",
    "\n",
    "def save_jsonl(dataset_split, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset_split:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Save splits\n",
    "save_jsonl(train, train_path)\n",
    "save_jsonl(validation, val_path)\n",
    "if 'test' in dataset:\n",
    "    save_jsonl(dataset['test'], test_path)\n",
    "save_jsonl(target_train, sample_train_path)\n",
    "\n",
    "print(f\"Full training set saved to {train_path}\")\n",
    "print(f\"Validation set saved to {val_path}\")\n",
    "print(f\"Sampled train set (≈{TRAIN_SAMPLES} entries) saved to {sample_train_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2250a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def setup_dataset(input_path: str, tokenizer, batch_size: int = BATCH_SIZE):\n",
    "    \"\"\"Load and prepare dataset for GPU processing.\"\"\"\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset('json', data_files=input_path, split='train')\n",
    "    \n",
    "    # Keep the original questions for reference\n",
    "    original_questions = dataset['question']\n",
    "    \n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['question'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            return_tensors=None  # Return as list, not tensors\n",
    "        )\n",
    "    \n",
    "    # Apply tokenization\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Create a custom dataset that includes both tokenized data and original questions\n",
    "    class QADataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, tokenized_data, original_questions):\n",
    "            self.tokenized_data = tokenized_data\n",
    "            self.original_questions = original_questions\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.tokenized_data)\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            item = {\n",
    "                'input_ids': torch.tensor(self.tokenized_data[idx]['input_ids']),\n",
    "                'attention_mask': torch.tensor(self.tokenized_data[idx]['attention_mask']),\n",
    "                'question': self.original_questions[idx]\n",
    "            }\n",
    "            return item\n",
    "    \n",
    "    # Create custom dataset\n",
    "    custom_dataset = QADataset(tokenized_dataset, original_questions)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    loader = DataLoader(\n",
    "        custom_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False  # Keep order for output matching\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model setup\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Use 4-bit quantization if enabled and on GPU\n",
    "if device.type == 'cuda' and USE_4BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    print(\"Loading model in 4-bit quantization...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading model in standard precision...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# Ensure the tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Memory after model load: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "# Load and prepare dataset\n",
    "print(f\"Loading dataset from {SAMPLE_TRAIN_PATH} with batch size {BATCH_SIZE}\")\n",
    "train_loader = setup_dataset(SAMPLE_TRAIN_PATH, tokenizer, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09e8cc",
   "metadata": {},
   "source": [
    "## Generate Student Drafts\n",
    "\n",
    "In this section we load a base language model (e.g. `meta-llama/Llama-2-7b-hf` or `gpt2`) and generate a short *student draft* for each question in the sampled training set.  A draft consists of a yes/no answer followed by one or two clarifying questions, as specified in the data‑generation loop.  Adjust the model name based on your available hardware and licences.\n",
    "\n",
    "> **Tip:** On Colab, you can enable a GPU via *Runtime → Change runtime type → GPU* and use half‑precision weights to reduce memory usage.  For demonstration, we use `gpt2` (which is small) to keep the example runnable on CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are an assistant that ALWAYS replies in exactly two lines.\\n\"\n",
    "    \"Line 1: Answer: <Yes/No>\\n\"\n",
    "    \"Line 2: Clarifying questions: <Q1>? <Q2>?\\n\"\n",
    "    \"Never repeat the user's question or the instructions.\"\n",
    ")\n",
    "\n",
    "DEMO = (\n",
    "    \"Example\\n\"\n",
    "    \"Question: Is the sky blue?\\n\"\n",
    "    \"Answer: Yes\\n\"\n",
    "    \"Clarifying questions: At what altitude? Under clear-sky conditions?\\n\"\n",
    ")\n",
    "\n",
    "def build_messages(q: str):\n",
    "    return [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"Answer ONLY in two lines.\\n\"\n",
    "                    \"Line 1: Answer: <Yes/No>\\n\"\n",
    "                    \"Line 2: Clarifying questions: <Q1>? <Q2>?\"},\n",
    "        # worked example (assistant answer **must** be its own turn)\n",
    "        {\"role\": \"user\",      \"content\": \"Is the sky blue?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Answer: Yes\\n\"\n",
    "                                        \"Clarifying questions: \"\n",
    "                                        \"At what altitude? Under clear-sky conditions?\"},\n",
    "        # your real question\n",
    "        {\"role\": \"user\", \"content\": q},\n",
    "    ]\n",
    "\n",
    "def generate_batch_drafts(batch):\n",
    "    \"\"\"Generate drafts for a batch of questions.\"\"\"\n",
    "    # Set padding side to left for generation (decoder-only models need this)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    # Create prompts for each question\n",
    "    prompts = [\n",
    "        tokenizer.apply_chat_template(build_messages(q),\n",
    "                                    tokenize=False,\n",
    "                                    add_generation_prompt=True)\n",
    "        for q in batch[\"question\"]\n",
    "    ]\n",
    "    inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Track memory usage before generation\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"GPU Memory before generation: {torch.cuda.memory_allocated()/1e9:.2f} GB\", end='\\r')\n",
    "    \n",
    "    # Generate responses\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            temperature=TEMPERATURE if DO_SAMPLE else 1.0,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "    prompt_lens = inputs[\"attention_mask\"].sum(dim=1)\n",
    "    decoded = [tokenizer.decode(seq[p_len:], skip_special_tokens=True).strip()\n",
    "            for seq, p_len in zip(outputs.sequences, prompt_lens)]\n",
    "    drafts = [ans[ans.find(\"Answer:\"):] for ans in decoded]\n",
    "    return drafts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3433bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create output directory if it doesn't exist\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(STUDENT_DRAFTS_PATH), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Process batches and write outputs\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting student drafts to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTUDENT_DRAFTS_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(STUDENT_DRAFTS_PATH), exist_ok=True)\n",
    "\n",
    "# Process batches and write outputs\n",
    "print(f\"Writing student drafts to {STUDENT_DRAFTS_PATH}\")\n",
    "with open(STUDENT_DRAFTS_PATH, 'w', encoding='utf-8') as out_f:\n",
    "    for batch in tqdm(train_loader, desc='Generating drafts'):\n",
    "        # Get original questions directly from the batch\n",
    "        questions = batch['question']\n",
    "        \n",
    "        # Move input_ids and attention_mask to device\n",
    "        input_batch = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        \n",
    "        # Generate drafts for the batch\n",
    "        drafts = generate_batch_drafts({'question': questions})\n",
    "        \n",
    "        # Write results\n",
    "        for q, draft in zip(questions, drafts):\n",
    "            out_rec = {\n",
    "                'question': q,\n",
    "                'student_draft': draft\n",
    "            }\n",
    "            out_f.write(json.dumps(out_rec, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        # Print memory usage periodically\n",
    "        if device.type == 'cuda':\n",
    "            print(f\"Current GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\", end='\\r')\n",
    "\n",
    "print(f\"Student drafts written to {STUDENT_DRAFTS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105e4488",
   "metadata": {},
   "source": [
    "## Generate Teacher Responses\n",
    "\n",
    "We now call GPT‑4 to obtain chain‑of‑thought (CoT) reasoning and final yes/no answers for each question/draft pair.  The prompt format follows the plan:\n",
    "\n",
    "```\n",
    "Q: <original yes/no question>\n",
    "Student draft: <answer + clarifying questions>\n",
    "Teacher: Please think step-by-step and provide your thought process and final Yes/No answer.\n",
    "```\n",
    "\n",
    "To run the actual API calls, you must provide a valid OpenAI API key.  If you set `dry_run=True`, dummy responses will be generated for testing purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load student drafts\n",
    "print(f\"Loading student drafts from {STUDENT_DRAFTS_PATH}\")\n",
    "with open(STUDENT_DRAFTS_PATH, 'r', encoding='utf-8') as f:\n",
    "    drafts = [json.loads(line) for line in f]\n",
    "\n",
    "# Get API key from environment\n",
    "if not OPENAI_API_KEY and not DRY_RUN:\n",
    "    print(\"Warning: OPENAI_API_KEY not set. Set DRY_RUN=True or provide an API key.\")\n",
    "\n",
    "def extract_yes_no(text: str) -> str:\n",
    "    m = re.search(r'(yes|no)', text, re.IGNORECASE)\n",
    "    return m.group(1).capitalize() if m else text.strip()\n",
    "\n",
    "def call_gpt4(prompt: str) -> str:\n",
    "    openai.api_key = OPENAI_API_KEY\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT4_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert teacher providing chain-of-thought reasoning and final yes/no answers.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=GPT4_MAX_TOKENS,\n",
    "        temperature=GPT4_TEMPERATURE\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(TEACHER_OUTPUTS_PATH), exist_ok=True)\n",
    "\n",
    "print(f\"Generating teacher responses with model: {GPT4_MODEL} (dry_run: {DRY_RUN})\")\n",
    "with open(TEACHER_OUTPUTS_PATH, 'w', encoding='utf-8') as out_f:\n",
    "    for rec in tqdm(drafts, desc='Generating teacher responses'):\n",
    "        q = rec['question']\n",
    "        draft = rec['student_draft']\n",
    "        prompt = f\"Q: {q} \\nStudent draft: {draft} \\nTeacher: Please think step-by-step and provide your thought process and final Yes/No answer.\"\n",
    "        \n",
    "        if DRY_RUN or not OPENAI_API_KEY:\n",
    "            response_text = '[Dummy CoT] This is a placeholder reasoning; replace DRY_RUN with False for real calls.'\n",
    "        else:\n",
    "            response_text = call_gpt4(prompt)\n",
    "            \n",
    "        answer = extract_yes_no(response_text)\n",
    "        out_record = {\n",
    "            'question': q,\n",
    "            'student_draft': draft,\n",
    "            'teacher_thought': response_text,\n",
    "            'teacher_answer': answer\n",
    "        }\n",
    "        out_f.write(json.dumps(out_record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Teacher outputs written to {TEACHER_OUTPUTS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8f3eb",
   "metadata": {},
   "outputs": [],
   "source": "import json\n\n# Load teacher outputs (fix path)\nparent_dir = os.path.dirname(os.getcwd())\nteacher_outputs_path_full = os.path.join(parent_dir, TEACHER_OUTPUTS_PATH)\nprint(f\"Loading teacher outputs from {teacher_outputs_path_full}\")\nwith open(teacher_outputs_path_full, 'r', encoding='utf-8') as f:\n    teacher_data = [json.loads(line) for line in f]\n\n# Create baseline and CoT records\nbaseline_records = []\ncot_records = []\nfor rec in teacher_data:\n    q = rec['question']\n    draft = rec['student_draft']\n    thought = rec['teacher_thought']\n    answer = rec['teacher_answer']\n    \n    # Track A: Baseline (question → answer)\n    baseline_records.append({'prompt': q, 'answer': answer})\n    \n    # Track B: CoT with student draft context (self-improvement format)\n    cot_prompt = f\"Question: {q}\\nStudent draft: {draft}\\nTeacher reasoning: {thought}\"\n    cot_records.append({'prompt': cot_prompt, 'answer': answer})\n\n# Create output directories if they don't exist (fix paths)\nbaseline_path_full = os.path.join(parent_dir, BASELINE_PATH)\ncot_path_full = os.path.join(parent_dir, COT_PATH)\nos.makedirs(os.path.dirname(baseline_path_full), exist_ok=True)\nos.makedirs(os.path.dirname(cot_path_full), exist_ok=True)\n\n# Write output files\nprint(f\"Writing baseline corpus to {baseline_path_full}\")\nwith open(baseline_path_full, 'w', encoding='utf-8') as f:\n    for r in baseline_records:\n        f.write(json.dumps(r) + '\\n')\n\nprint(f\"Writing CoT corpus to {cot_path_full}\")\nwith open(cot_path_full, 'w', encoding='utf-8') as f:\n    for r in cot_records:\n        f.write(json.dumps(r) + '\\n')\n\nprint(f\"Baseline corpus saved to {baseline_path_full}\")\nprint(f\"CoT corpus saved to {cot_path_full}\")\nprint(f\"\\nTraining data generation complete!\")\nprint(f\"Summary:\")\nprint(f\"- {len(baseline_records)} examples in baseline corpus\")\nprint(f\"- {len(cot_records)} examples in CoT corpus\")\nprint(f\"\\nCoT training format preview:\")\nprint(\"Input:\", cot_records[0]['prompt'][:200] + \"...\")\nprint(\"Target:\", cot_records[0]['answer'])"
  },
  {
   "cell_type": "markdown",
   "id": "caa3c20c",
   "metadata": {},
   "source": [
    "## Build Training Corpora\n",
    "\n",
    "Finally, we build two parallel training corpora:\n",
    "\n",
    "1. **Baseline (Track A)** – pairs of `(question → answer)` for training a basic model.\n",
    "2. **CoT (Track B)** – pairs of `(question + teacher chain-of-thought → answer)` for CoT distillation【777585631172426†L42-L45】.\n",
    "\n",
    "These files will be used in later steps for model fine‑tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318d2267",
   "metadata": {},
   "outputs": [],
   "source": "import json\n\n# Fix paths for notebook execution from notebooks/ directory\nparent_dir = os.path.dirname(os.getcwd())\nteacher_outputs_path = os.path.join(parent_dir, 'data', 'teacher_outputs.jsonl')\n\nwith open(teacher_outputs_path, 'r', encoding='utf-8') as f:\n    teacher_data = [json.loads(line) for line in f]\n\nbaseline_records = []\ncot_records = []\nfor rec in teacher_data:\n    q = rec['question']\n    draft = rec['student_draft']\n    thought = rec['teacher_thought']\n    answer = rec['teacher_answer']\n    \n    # Track A: Baseline (question → answer)\n    baseline_records.append({'prompt': q, 'answer': answer})\n    \n    # Track B: CoT with student draft context (self-improvement format)\n    cot_prompt = f\"Question: {q}\\nStudent draft: {draft}\\nTeacher reasoning: {thought}\"\n    cot_records.append({'prompt': cot_prompt, 'answer': answer})\n\nbaseline_path = os.path.join(parent_dir, 'data', 'train_baseline.jsonl')\ncot_path = os.path.join(parent_dir, 'data', 'train_cot.jsonl')\n\nwith open(baseline_path, 'w', encoding='utf-8') as f:\n    for r in baseline_records:\n        f.write(json.dumps(r) + '\\n')\n        \nwith open(cot_path, 'w', encoding='utf-8') as f:\n    for r in cot_records:\n        f.write(json.dumps(r) + '\\n')\n\nprint(f\"Baseline corpus saved to {baseline_path}\")\nprint(f\"CoT corpus saved to {cot_path}\")\nprint(f\"\\nSelf-improvement CoT format:\")\nprint(\"The student model will learn to improve its own drafts by seeing:\")\nprint(\"Input: Question + Student draft + Teacher reasoning\")\nprint(\"Target: Final answer\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
