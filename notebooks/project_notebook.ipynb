{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0675efc",
   "metadata": {},
   "source": [
    "# Self‑Improving LLM Project\n",
    "\n",
    "This notebook implements Parts 2 and 3 of the project plan for the **Self‑Improving LLM** final project.  Specifically, it covers:\n",
    "\n",
    "- **Dataset Acquisition & Sampling:** download the StrategyQA dataset, sample ~2 000 training examples as recommended, and save them to disk for subsequent processing.\n",
    "- **Prompt Engineering & Teacher Generation:** generate a baseline *student draft* for each question, compose prompts according to the plan (question, student draft, and a teacher instruction), call GPT‑4 (or run in dry‑run mode), and build two parallel corpora for baseline and CoT training.\n",
    "\n",
    "The plan specifies a data‑generation loop where each question is paired with a student draft and a teacher chain‑of‑thought, resulting in two training tracks.  The baseline model is trained on `(Q → answer)` pairs, while the CoT model is trained on `(Q + teacher CoT → answer)` pair.\n",
    "\n",
    "> **Note:** Running the full pipeline (especially calling GPT‑4) requires an OpenAI API key and may incur costs.  A dry‑run mode is provided for testing the notebook without external API calls.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "id": "c6bd4dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "c6bd4dbd",
   "metadata": {},
   "outputs": [],
>>>>>>> origin/main
   "source": [
    "!pip install -q datasets transformers openai bitsandbytes accelerate python-dotenv huggingface_hub huggingface_hub[hf_xet]\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "id": "f4ae3796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Configuration ===\n",
      "Dataset: voidful/StrategyQA\n",
      "Model: microsoft/phi-3-mini-4k-instruct\n",
      "Batch size: 8\n",
      "4-bit quantization: True\n",
      "GPT-4 dry run: True\n",
      "====================\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "f4ae3796",
   "metadata": {},
   "outputs": [],
>>>>>>> origin/main
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Dataset parameters\n",
    "DATASET_NAME = os.getenv('DATASET_NAME', 'voidful/StrategyQA')\n",
    "TRAIN_SAMPLES = int(os.getenv('TRAIN_SAMPLES', '100'))\n",
    "RANDOM_SEED = int(os.getenv('RANDOM_SEED', '42'))\n",
    "\n",
    "# Model parameters\n",
    "MODEL_NAME = os.getenv('MODEL_NAME', 'microsoft/phi-2')\n",
    "MAX_NEW_TOKENS = int(os.getenv('MAX_NEW_TOKENS', '35'))\n",
    "BATCH_SIZE = int(os.getenv('BATCH_SIZE', '8'))\n",
    "USE_4BIT = os.getenv('USE_4BIT', 'True').lower() in ('true', '1', 't')\n",
    "MAX_SEQ_LENGTH = int(os.getenv('MAX_SEQ_LENGTH', '512'))\n",
    "HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN', '')\n",
    "\n",
    "# Generation parameters\n",
    "DO_SAMPLE = os.getenv('DO_SAMPLE', 'False').lower() in ('true', '1', 't')\n",
    "TEMPERATURE = float(os.getenv('TEMPERATURE', '0.7'))\n",
    "\n",
    "# GPT-4 parameters\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')\n",
    "GPT4_MODEL = os.getenv('GPT4_MODEL', 'gpt-4')\n",
    "GPT4_MAX_TOKENS = int(os.getenv('GPT4_MAX_TOKENS', '150'))\n",
    "GPT4_TEMPERATURE = float(os.getenv('GPT4_TEMPERATURE', '0.3'))\n",
    "DRY_RUN = os.getenv('DRY_RUN', 'True').lower() in ('true', '1', 't')\n",
    "\n",
    "# File paths\n",
    "DATA_DIR = os.getenv('DATA_DIR', 'data')\n",
    "RAW_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "SAMPLE_TRAIN_PATH = os.path.join(DATA_DIR, 'sample_train.jsonl')\n",
    "STUDENT_DRAFTS_PATH = os.path.join(DATA_DIR, 'student_drafts.jsonl')\n",
    "TEACHER_OUTPUTS_PATH = os.path.join(DATA_DIR, 'teacher_outputs.jsonl')\n",
    "BASELINE_PATH = os.path.join(DATA_DIR, 'train_baseline.jsonl')\n",
    "COT_PATH = os.path.join(DATA_DIR, 'train_cot.jsonl')\n",
    "\n",
    "# Print configuration\n",
    "print(\"=== Configuration ===\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"4-bit quantization: {USE_4BIT}\")\n",
    "print(f\"GPT-4 dry run: {DRY_RUN}\")\n",
    "print(\"====================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login, notebook_login\n",
    "\n",
    "# def smart_hf_login():\n",
    "#     \"\"\"Use HF_TOKEN env/secret if present, else fall back to interactive login.\"\"\"\n",
    "#     if HUGGINGFACE_TOKEN:         # works for Colab secrets, CI, docker, …\n",
    "#         login(HUGGINGFACE_TOKEN)\n",
    "#     elif 'google.colab' in sys.modules:   # inside a Colab kernel but no secret set\n",
    "#         notebook_login()\n",
    "#     else:                                 # local Jupyter; will prompt only once\n",
    "#         login()\n",
    "\n",
    "# smart_hf_login()\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "id": "149b7c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files in:\n",
      "- Train: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\\strategyqa_train.jsonl\n",
      "- Val: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\\strategyqa_validation.jsonl\n",
      "- Test: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\\strategyqa_test.jsonl\n",
      "Created directory: c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\n",
      "Files exist: True\n",
      "Loading dataset from local files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1038 examples [00:00, 28964.15 examples/s]\n",
      "Generating validation split: 565 examples [00:00, 51359.57 examples/s]\n",
      "Generating test split: 687 examples [00:00, 76042.72 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 200 examples with seed 42\n",
      "Full training set saved to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\\strategyqa_train.jsonl\n",
      "Validation set saved to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\raw\\strategyqa_validation.jsonl\n",
      "Sampled train set (≈200 entries) saved to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\sample_train.jsonl\n"
     ]
    }
   ],
   "source": [
=======
   "execution_count": null,
   "id": "149b7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
>>>>>>> origin/main
    "from datasets import load_dataset\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Check if we need to download the dataset\n",
<<<<<<< HEAD
    "# Fix: Use paths relative to the parent directory (project root)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "raw_dir = os.path.join(parent_dir, DATA_DIR, 'raw')\n",
=======
    "raw_dir = os.path.join(DATA_DIR, 'raw')\n",
>>>>>>> origin/main
    "train_path = os.path.join(raw_dir, 'strategyqa_train.jsonl')\n",
    "val_path = os.path.join(raw_dir, 'strategyqa_validation.jsonl')  # Changed from dev to validation to match download script\n",
    "test_path = os.path.join(raw_dir, 'strategyqa_test.jsonl')\n",
    "\n",
    "print(f\"Looking for files in:\")\n",
    "print(f\"- Train: {train_path}\")\n",
    "print(f\"- Val: {val_path}\")\n",
    "print(f\"- Test: {test_path}\")\n",
    "\n",
    "# Create raw directory if it doesn't exist\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "print(f\"Created directory: {raw_dir}\")\n",
    "\n",
    "# Check if files exist\n",
    "files_exist = all(os.path.exists(p) for p in [train_path, val_path])\n",
    "print(f\"Files exist: {files_exist}\")\n",
    "\n",
    "if not files_exist:\n",
    "    print(\"Dataset files not found. Running download script...\")\n",
<<<<<<< HEAD
    "    script_path = os.path.join(parent_dir, 'scripts', 'download_strategyqa.py')\n",
=======
    "    script_path = os.path.join('scripts', 'download_strategyqa.py')\n",
>>>>>>> origin/main
    "    print(f\"Running: {sys.executable} {script_path} --output-dir {raw_dir}\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, script_path, '--output-dir', raw_dir],\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(\"Download script output:\")\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"Errors:\")\n",
    "        print(result.stderr)\n",
    "    \n",
    "    # Verify files were created\n",
    "    print(\"\\nChecking if files were created:\")\n",
    "    for path in [train_path, val_path, test_path]:\n",
    "        exists = os.path.exists(path)\n",
    "        print(f\"- {path}: {'✓' if exists else '✗'}\")\n",
    "        if exists:\n",
    "            size = os.path.getsize(path)\n",
    "            print(f\"  Size: {size:,} bytes\")\n",
    "\n",
    "# Load the dataset from local JSONL files\n",
    "print(\"Loading dataset from local files...\")\n",
    "data_files = {\n",
    "    'train': train_path,\n",
    "    'validation': val_path,\n",
    "}\n",
    "if os.path.exists(test_path):\n",
    "    data_files['test'] = test_path\n",
    "\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "train = dataset['train']\n",
    "validation = dataset['validation']\n",
    "\n",
    "def sample_train_set(train_dataset, n_samples=TRAIN_SAMPLES, seed=RANDOM_SEED):\n",
    "    '''Return a random sample of the training set.'''\n",
    "    shuffled = train_dataset.shuffle(seed=seed)\n",
    "    return shuffled.select(range(min(n_samples, len(shuffled))))\n",
    "\n",
    "# Sample examples from the training set\n",
    "print(f\"Sampling {TRAIN_SAMPLES} examples with seed {RANDOM_SEED}\")\n",
    "target_train = sample_train_set(train)\n",
    "\n",
<<<<<<< HEAD
    "# Create output directories (also fix these paths)\n",
    "data_dir_full = os.path.join(parent_dir, DATA_DIR)\n",
    "os.makedirs(data_dir_full, exist_ok=True)\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "# Save the full dev/test sets and the sampled train set\n",
    "sample_train_path = os.path.join(parent_dir, SAMPLE_TRAIN_PATH)\n",
=======
    "# Create output directories\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "# Save the full dev/test sets and the sampled train set\n",
    "train_path = os.path.join(raw_dir, 'strategyqa_train.jsonl')\n",
    "val_path = os.path.join(raw_dir, 'strategyqa_validation.jsonl')  # Changed from dev to validation to match download script\n",
    "test_path = os.path.join(raw_dir, 'strategyqa_test.jsonl')\n",
    "sample_train_path = SAMPLE_TRAIN_PATH\n",
>>>>>>> origin/main
    "\n",
    "def save_jsonl(dataset_split, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset_split:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Save splits\n",
    "save_jsonl(train, train_path)\n",
    "save_jsonl(validation, val_path)\n",
    "if 'test' in dataset:\n",
    "    save_jsonl(dataset['test'], test_path)\n",
    "save_jsonl(target_train, sample_train_path)\n",
    "\n",
    "print(f\"Full training set saved to {train_path}\")\n",
    "print(f\"Validation set saved to {val_path}\")\n",
<<<<<<< HEAD
    "print(f\"Sampled train set (≈{TRAIN_SAMPLES} entries) saved to {sample_train_path}\")"
=======
    "print(f\"Sampled train set (≈{TRAIN_SAMPLES} entries) saved to {sample_train_path}\")\n"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "id": "8d2250a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model: microsoft/phi-3-mini-4k-instruct\n",
      "Loading model in 4-bit quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory after model load: 4.13 GB\n",
      "Loading dataset from c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\sample_train.jsonl with batch size 8\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "8d2250a0",
   "metadata": {},
   "outputs": [],
>>>>>>> origin/main
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def setup_dataset(input_path: str, tokenizer, batch_size: int = BATCH_SIZE):\n",
    "    \"\"\"Load and prepare dataset for GPU processing.\"\"\"\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset('json', data_files=input_path, split='train')\n",
    "    \n",
    "    # Keep the original questions for reference\n",
    "    original_questions = dataset['question']\n",
    "    \n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['question'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            return_tensors=None  # Return as list, not tensors\n",
    "        )\n",
    "    \n",
    "    # Apply tokenization\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Create a custom dataset that includes both tokenized data and original questions\n",
    "    class QADataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, tokenized_data, original_questions):\n",
    "            self.tokenized_data = tokenized_data\n",
    "            self.original_questions = original_questions\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.tokenized_data)\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            item = {\n",
    "                'input_ids': torch.tensor(self.tokenized_data[idx]['input_ids']),\n",
    "                'attention_mask': torch.tensor(self.tokenized_data[idx]['attention_mask']),\n",
    "                'question': self.original_questions[idx]\n",
    "            }\n",
    "            return item\n",
    "    \n",
    "    # Create custom dataset\n",
    "    custom_dataset = QADataset(tokenized_dataset, original_questions)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    loader = DataLoader(\n",
    "        custom_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False  # Keep order for output matching\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model setup\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Use 4-bit quantization if enabled and on GPU\n",
    "if device.type == 'cuda' and USE_4BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    print(\"Loading model in 4-bit quantization...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading model in standard precision...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# Ensure the tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Memory after model load: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
<<<<<<< HEAD
    "# Load and prepare dataset (fix path)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sample_train_path_full = os.path.join(parent_dir, SAMPLE_TRAIN_PATH)\n",
    "print(f\"Loading dataset from {sample_train_path_full} with batch size {BATCH_SIZE}\")\n",
    "train_loader = setup_dataset(sample_train_path_full, tokenizer, batch_size=BATCH_SIZE)"
=======
    "# Load and prepare dataset\n",
    "print(f\"Loading dataset from {SAMPLE_TRAIN_PATH} with batch size {BATCH_SIZE}\")\n",
    "train_loader = setup_dataset(SAMPLE_TRAIN_PATH, tokenizer, batch_size=BATCH_SIZE)\n"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09e8cc",
   "metadata": {},
   "source": [
    "## Generate Student Drafts\n",
    "\n",
    "In this section we load a base language model (e.g. `meta-llama/Llama-2-7b-hf` or `gpt2`) and generate a short *student draft* for each question in the sampled training set.  A draft consists of a yes/no answer followed by one or two clarifying questions, as specified in the data‑generation loop.  Adjust the model name based on your available hardware and licences.\n",
    "\n",
    "> **Tip:** On Colab, you can enable a GPU via *Runtime → Change runtime type → GPU* and use half‑precision weights to reduce memory usage.  For demonstration, we use `gpt2` (which is small) to keep the example runnable on CPU.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": null,
>>>>>>> origin/main
   "id": "aab9c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are an assistant that ALWAYS replies in exactly two lines.\\n\"\n",
    "    \"Line 1: Answer: <Yes/No>\\n\"\n",
    "    \"Line 2: Clarifying questions: <Q1>? <Q2>?\\n\"\n",
    "    \"Never repeat the user's question or the instructions.\"\n",
    ")\n",
    "\n",
    "DEMO = (\n",
    "    \"Example\\n\"\n",
    "    \"Question: Is the sky blue?\\n\"\n",
    "    \"Answer: Yes\\n\"\n",
    "    \"Clarifying questions: At what altitude? Under clear-sky conditions?\\n\"\n",
    ")\n",
    "\n",
    "def build_messages(q: str):\n",
    "    return [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"Answer ONLY in two lines.\\n\"\n",
    "                    \"Line 1: Answer: <Yes/No>\\n\"\n",
    "                    \"Line 2: Clarifying questions: <Q1>? <Q2>?\"},\n",
    "        # worked example (assistant answer **must** be its own turn)\n",
    "        {\"role\": \"user\",      \"content\": \"Is the sky blue?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Answer: Yes\\n\"\n",
    "                                        \"Clarifying questions: \"\n",
    "                                        \"At what altitude? Under clear-sky conditions?\"},\n",
    "        # your real question\n",
    "        {\"role\": \"user\", \"content\": q},\n",
    "    ]\n",
    "\n",
    "def generate_batch_drafts(batch):\n",
    "    \"\"\"Generate drafts for a batch of questions.\"\"\"\n",
    "    # Set padding side to left for generation (decoder-only models need this)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    # Create prompts for each question\n",
    "    prompts = [\n",
    "        tokenizer.apply_chat_template(build_messages(q),\n",
    "                                    tokenize=False,\n",
    "                                    add_generation_prompt=True)\n",
    "        for q in batch[\"question\"]\n",
    "    ]\n",
    "    inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Track memory usage before generation\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"GPU Memory before generation: {torch.cuda.memory_allocated()/1e9:.2f} GB\", end='\\r')\n",
    "    \n",
    "    # Generate responses\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            temperature=TEMPERATURE if DO_SAMPLE else 1.0,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "    prompt_lens = inputs[\"attention_mask\"].sum(dim=1)\n",
    "    decoded = [tokenizer.decode(seq[p_len:], skip_special_tokens=True).strip()\n",
    "            for seq, p_len in zip(outputs.sequences, prompt_lens)]\n",
    "    drafts = [ans[ans.find(\"Answer:\"):] for ans in decoded]\n",
    "    return drafts\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "id": "9f3433bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing student drafts to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\student_drafts.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\.venv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:457: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "Generating drafts:   4%|▍         | 1/25 [00:14<05:51, 14.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:   8%|▊         | 2/25 [00:26<04:57, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  12%|█▏        | 3/25 [00:38<04:40, 12.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  16%|█▌        | 4/25 [00:51<04:27, 12.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  20%|██        | 5/25 [01:01<03:57, 11.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  24%|██▍       | 6/25 [01:14<03:51, 12.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  28%|██▊       | 7/25 [01:26<03:35, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  32%|███▏      | 8/25 [01:37<03:20, 11.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  36%|███▌      | 9/25 [01:54<03:34, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  40%|████      | 10/25 [02:06<03:13, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  44%|████▍     | 11/25 [02:45<04:52, 20.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  48%|████▊     | 12/25 [02:56<03:52, 17.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  52%|█████▏    | 13/25 [03:08<03:14, 16.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  56%|█████▌    | 14/25 [03:23<02:52, 15.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  60%|██████    | 15/25 [03:37<02:31, 15.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  64%|██████▍   | 16/25 [03:48<02:05, 13.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  68%|██████▊   | 17/25 [04:02<01:52, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  72%|███████▏  | 18/25 [04:13<01:31, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  76%|███████▌  | 19/25 [04:25<01:16, 12.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  80%|████████  | 20/25 [04:40<01:07, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  84%|████████▍ | 21/25 [04:51<00:50, 12.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  88%|████████▊ | 22/25 [05:05<00:39, 13.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  92%|█████████▏| 23/25 [05:18<00:25, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts:  96%|█████████▌| 24/25 [05:29<00:12, 12.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before generation: 4.14 GB\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating drafts: 100%|██████████| 25/25 [05:39<00:00, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student drafts written to c:\\Users\\noham\\Desktop\\Self-Improving-LLM\\data\\student_drafts.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist (fix path)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "student_drafts_path_full = os.path.join(parent_dir, STUDENT_DRAFTS_PATH)\n",
    "os.makedirs(os.path.dirname(student_drafts_path_full), exist_ok=True)\n",
    "\n",
    "# Process batches and write outputs\n",
    "print(f\"Writing student drafts to {student_drafts_path_full}\")\n",
    "with open(student_drafts_path_full, 'w', encoding='utf-8') as out_f:\n",
=======
   "execution_count": null,
   "id": "9f3433bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create output directory if it doesn't exist\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(STUDENT_DRAFTS_PATH), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Process batches and write outputs\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting student drafts to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTUDENT_DRAFTS_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(STUDENT_DRAFTS_PATH), exist_ok=True)\n",
    "\n",
    "# Process batches and write outputs\n",
    "print(f\"Writing student drafts to {STUDENT_DRAFTS_PATH}\")\n",
    "with open(STUDENT_DRAFTS_PATH, 'w', encoding='utf-8') as out_f:\n",
>>>>>>> origin/main
    "    for batch in tqdm(train_loader, desc='Generating drafts'):\n",
    "        # Get original questions directly from the batch\n",
    "        questions = batch['question']\n",
    "        \n",
    "        # Move input_ids and attention_mask to device\n",
    "        input_batch = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        \n",
    "        # Generate drafts for the batch\n",
    "        drafts = generate_batch_drafts({'question': questions})\n",
    "        \n",
    "        # Write results\n",
    "        for q, draft in zip(questions, drafts):\n",
    "            out_rec = {\n",
    "                'question': q,\n",
    "                'student_draft': draft\n",
    "            }\n",
    "            out_f.write(json.dumps(out_rec, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        # Print memory usage periodically\n",
    "        if device.type == 'cuda':\n",
    "            print(f\"Current GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\", end='\\r')\n",
    "\n",
<<<<<<< HEAD
    "print(f\"Student drafts written to {student_drafts_path_full}\")"
=======
    "print(f\"Student drafts written to {STUDENT_DRAFTS_PATH}\")\n"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105e4488",
   "metadata": {},
   "source": [
    "## Generate Teacher Responses\n",
    "\n",
    "We now call GPT‑4 to obtain chain‑of‑thought (CoT) reasoning and final yes/no answers for each question/draft pair.  The prompt format follows the plan:\n",
    "\n",
    "```\n",
    "Q: <original yes/no question>\n",
    "Student draft: <answer + clarifying questions>\n",
    "Teacher: Please think step-by-step and provide your thought process and final Yes/No answer.\n",
    "```\n",
    "\n",
    "To run the actual API calls, you must provide a valid OpenAI API key.  If you set `dry_run=True`, dummy responses will be generated for testing purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082d0cd",
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": "import os\nimport json\nfrom openai import OpenAI\nimport re\nfrom tqdm import tqdm\n\n# Load student drafts (fix path)\nparent_dir = os.path.dirname(os.getcwd())\nstudent_drafts_path_full = os.path.join(parent_dir, STUDENT_DRAFTS_PATH)\nprint(f\"Loading student drafts from {student_drafts_path_full}\")\nwith open(student_drafts_path_full, 'r', encoding='utf-8') as f:\n    drafts = [json.loads(line) for line in f]\n\n# Get API key from environment\nif not OPENAI_API_KEY and not DRY_RUN:\n    print(\"Warning: OPENAI_API_KEY not set. Set DRY_RUN=True or provide an API key.\")\n\ndef extract_yes_no(text: str) -> str:\n    m = re.search(r'(yes|no)', text, re.IGNORECASE)\n    return m.group(1).capitalize() if m else text.strip()\n\ndef call_gpt4(q: str, draft: str) -> str:\n    client = OpenAI(api_key=OPENAI_API_KEY)\n    \n    system_prompt = \"\"\"You are an expert teacher helping a student AI model learn to reason through complex yes/no questions. Your role is to:\n\n1. Analyze the student's draft answer and clarifying questions\n2. Provide clear, step-by-step reasoning that demonstrates good thinking patterns\n3. Address the student's specific concerns when they raise valid points\n4. Explain your thought process in a way the student can learn from and apply to similar questions\n5. Always conclude with a confident final Yes/No answer\n\nYour reasoning should be educational and transferable, helping the student learn general problem-solving approaches.\"\"\"\n    \n    user_prompt = f\"\"\"Question: {q}\n\nStudent's draft attempt: {draft}\n\nPlease provide step-by-step reasoning and your final Yes/No answer.\"\"\"\n    \n    response = client.chat.completions.create(\n        model=GPT4_MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt}\n        ],\n        max_tokens=GPT4_MAX_TOKENS,\n        temperature=GPT4_TEMPERATURE\n    )\n    return response.choices[0].message.content.strip()\n\n# Create output directory if it doesn't exist (fix path)\nteacher_outputs_path_full = os.path.join(parent_dir, TEACHER_OUTPUTS_PATH)\nos.makedirs(os.path.dirname(teacher_outputs_path_full), exist_ok=True)\n\nprint(f\"Generating teacher responses with model: {GPT4_MODEL} (dry_run: {DRY_RUN})\")\nwith open(teacher_outputs_path_full, 'w', encoding='utf-8') as out_f:\n    for rec in tqdm(drafts, desc='Generating teacher responses'):\n        q = rec['question']\n        draft = rec['student_draft']\n        \n        if DRY_RUN or not OPENAI_API_KEY:\n            response_text = '[Dummy CoT] This is a placeholder reasoning; replace DRY_RUN with False for real calls.'\n        else:\n            response_text = call_gpt4(q, draft)\n            \n        answer = extract_yes_no(response_text)\n        out_record = {\n            'question': q,\n            'student_draft': draft,\n            'teacher_thought': response_text,\n            'teacher_answer': answer\n        }\n        out_f.write(json.dumps(out_record, ensure_ascii=False) + '\\n')\n\nprint(f\"Teacher outputs written to {teacher_outputs_path_full}\")"
=======
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load student drafts\n",
    "print(f\"Loading student drafts from {STUDENT_DRAFTS_PATH}\")\n",
    "with open(STUDENT_DRAFTS_PATH, 'r', encoding='utf-8') as f:\n",
    "    drafts = [json.loads(line) for line in f]\n",
    "\n",
    "# Get API key from environment\n",
    "if not OPENAI_API_KEY and not DRY_RUN:\n",
    "    print(\"Warning: OPENAI_API_KEY not set. Set DRY_RUN=True or provide an API key.\")\n",
    "\n",
    "def extract_yes_no(text: str) -> str:\n",
    "    m = re.search(r'(yes|no)', text, re.IGNORECASE)\n",
    "    return m.group(1).capitalize() if m else text.strip()\n",
    "\n",
    "def call_gpt4(prompt: str) -> str:\n",
    "    openai.api_key = OPENAI_API_KEY\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT4_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert teacher providing chain-of-thought reasoning and final yes/no answers.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=GPT4_MAX_TOKENS,\n",
    "        temperature=GPT4_TEMPERATURE\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(TEACHER_OUTPUTS_PATH), exist_ok=True)\n",
    "\n",
    "print(f\"Generating teacher responses with model: {GPT4_MODEL} (dry_run: {DRY_RUN})\")\n",
    "with open(TEACHER_OUTPUTS_PATH, 'w', encoding='utf-8') as out_f:\n",
    "    for rec in tqdm(drafts, desc='Generating teacher responses'):\n",
    "        q = rec['question']\n",
    "        draft = rec['student_draft']\n",
    "        prompt = f\"Q: {q} \\nStudent draft: {draft} \\nTeacher: Please think step-by-step and provide your thought process and final Yes/No answer.\"\n",
    "        \n",
    "        if DRY_RUN or not OPENAI_API_KEY:\n",
    "            response_text = '[Dummy CoT] This is a placeholder reasoning; replace DRY_RUN with False for real calls.'\n",
    "        else:\n",
    "            response_text = call_gpt4(prompt)\n",
    "            \n",
    "        answer = extract_yes_no(response_text)\n",
    "        out_record = {\n",
    "            'question': q,\n",
    "            'student_draft': draft,\n",
    "            'teacher_thought': response_text,\n",
    "            'teacher_answer': answer\n",
    "        }\n",
    "        out_f.write(json.dumps(out_record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Teacher outputs written to {TEACHER_OUTPUTS_PATH}\")\n"
   ]
>>>>>>> origin/main
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8f3eb",
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": "import json\n\n# Load teacher outputs (fix path)\nparent_dir = os.path.dirname(os.getcwd())\nteacher_outputs_path_full = os.path.join(parent_dir, TEACHER_OUTPUTS_PATH)\nprint(f\"Loading teacher outputs from {teacher_outputs_path_full}\")\nwith open(teacher_outputs_path_full, 'r', encoding='utf-8') as f:\n    teacher_data = [json.loads(line) for line in f]\n\n# Create baseline and CoT records\nbaseline_records = []\ncot_records = []\nfor rec in teacher_data:\n    q = rec['question']\n    draft = rec['student_draft']\n    thought = rec['teacher_thought']\n    answer = rec['teacher_answer']\n    \n    # Track A: Baseline (question → answer)\n    baseline_records.append({'prompt': q, 'answer': answer})\n    \n    # Track B: CoT with student draft context (self-improvement format)\n    cot_prompt = f\"Question: {q}\\nStudent draft: {draft}\\nTeacher reasoning: {thought}\"\n    cot_records.append({'prompt': cot_prompt, 'answer': answer})\n\n# Create output directories if they don't exist (fix paths)\nbaseline_path_full = os.path.join(parent_dir, BASELINE_PATH)\ncot_path_full = os.path.join(parent_dir, COT_PATH)\nos.makedirs(os.path.dirname(baseline_path_full), exist_ok=True)\nos.makedirs(os.path.dirname(cot_path_full), exist_ok=True)\n\n# Write output files\nprint(f\"Writing baseline corpus to {baseline_path_full}\")\nwith open(baseline_path_full, 'w', encoding='utf-8') as f:\n    for r in baseline_records:\n        f.write(json.dumps(r) + '\\n')\n\nprint(f\"Writing CoT corpus to {cot_path_full}\")\nwith open(cot_path_full, 'w', encoding='utf-8') as f:\n    for r in cot_records:\n        f.write(json.dumps(r) + '\\n')\n\nprint(f\"Baseline corpus saved to {baseline_path_full}\")\nprint(f\"CoT corpus saved to {cot_path_full}\")\nprint(f\"\\nTraining data generation complete!\")\nprint(f\"Summary:\")\nprint(f\"- {len(baseline_records)} examples in baseline corpus\")\nprint(f\"- {len(cot_records)} examples in CoT corpus\")\nprint(f\"\\nCoT training format preview:\")\nprint(\"Input:\", cot_records[0]['prompt'][:200] + \"...\")\nprint(\"Target:\", cot_records[0]['answer'])"
=======
   "source": [
    "import json\n",
    "\n",
    "# Load teacher outputs\n",
    "print(f\"Loading teacher outputs from {TEACHER_OUTPUTS_PATH}\")\n",
    "with open(TEACHER_OUTPUTS_PATH, 'r', encoding='utf-8') as f:\n",
    "    teacher_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Create baseline and CoT records\n",
    "baseline_records = []\n",
    "cot_records = []\n",
    "for rec in teacher_data:\n",
    "    q = rec['question']\n",
    "    thought = rec['teacher_thought']\n",
    "    answer = rec['teacher_answer']\n",
    "    baseline_records.append({'prompt': q, 'answer': answer})\n",
    "    cot_records.append({'prompt': f\"{q} {thought}\", 'answer': answer})\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(os.path.dirname(BASELINE_PATH), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(COT_PATH), exist_ok=True)\n",
    "\n",
    "# Write output files\n",
    "print(f\"Writing baseline corpus to {BASELINE_PATH}\")\n",
    "with open(BASELINE_PATH, 'w', encoding='utf-8') as f:\n",
    "    for r in baseline_records:\n",
    "        f.write(json.dumps(r) + '\\n')\n",
    "\n",
    "print(f\"Writing CoT corpus to {COT_PATH}\")\n",
    "with open(COT_PATH, 'w', encoding='utf-8') as f:\n",
    "    for r in cot_records:\n",
    "        f.write(json.dumps(r) + '\\n')\n",
    "\n",
    "print(f\"Baseline corpus saved to {BASELINE_PATH}\")\n",
    "print(f\"CoT corpus saved to {COT_PATH}\")\n",
    "print(f\"\\nTraining data generation complete!\")\n",
    "print(f\"Summary:\")\n",
    "print(f\"- {len(baseline_records)} examples in baseline corpus\")\n",
    "print(f\"- {len(cot_records)} examples in CoT corpus\")\n"
   ]
>>>>>>> origin/main
  },
  {
   "cell_type": "markdown",
   "id": "caa3c20c",
   "metadata": {},
   "source": [
    "## Build Training Corpora\n",
    "\n",
    "Finally, we build two parallel training corpora:\n",
    "\n",
    "1. **Baseline (Track A)** – pairs of `(question → answer)` for training a basic model.\n",
    "2. **CoT (Track B)** – pairs of `(question + teacher chain-of-thought → answer)` for CoT distillation【777585631172426†L42-L45】.\n",
    "\n",
    "These files will be used in later steps for model fine‑tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318d2267",
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": "import json\n\n# Fix paths for notebook execution from notebooks/ directory\nparent_dir = os.path.dirname(os.getcwd())\nteacher_outputs_path = os.path.join(parent_dir, 'data', 'teacher_outputs.jsonl')\n\nwith open(teacher_outputs_path, 'r', encoding='utf-8') as f:\n    teacher_data = [json.loads(line) for line in f]\n\nbaseline_records = []\ncot_records = []\nfor rec in teacher_data:\n    q = rec['question']\n    draft = rec['student_draft']\n    thought = rec['teacher_thought']\n    answer = rec['teacher_answer']\n    \n    # Track A: Baseline (question → answer)\n    baseline_records.append({'prompt': q, 'answer': answer})\n    \n    # Track B: CoT with student draft context (self-improvement format)\n    cot_prompt = f\"Question: {q}\\nStudent draft: {draft}\\nTeacher reasoning: {thought}\"\n    cot_records.append({'prompt': cot_prompt, 'answer': answer})\n\nbaseline_path = os.path.join(parent_dir, 'data', 'train_baseline.jsonl')\ncot_path = os.path.join(parent_dir, 'data', 'train_cot.jsonl')\n\nwith open(baseline_path, 'w', encoding='utf-8') as f:\n    for r in baseline_records:\n        f.write(json.dumps(r) + '\\n')\n        \nwith open(cot_path, 'w', encoding='utf-8') as f:\n    for r in cot_records:\n        f.write(json.dumps(r) + '\\n')\n\nprint(f\"Baseline corpus saved to {baseline_path}\")\nprint(f\"CoT corpus saved to {cot_path}\")\nprint(f\"\\nSelf-improvement CoT format:\")\nprint(\"The student model will learn to improve its own drafts by seeing:\")\nprint(\"Input: Question + Student draft + Teacher reasoning\")\nprint(\"Target: Final answer\")"
=======
   "source": [
    "\n",
    "import json\n",
    "\n",
    "with open('data/teacher_outputs.jsonl', 'r', encoding='utf-8') as f:\n",
    "    teacher_data = [json.loads(line) for line in f]\n",
    "\n",
    "baseline_records = []\n",
    "cot_records = []\n",
    "for rec in teacher_data:\n",
    "    q = rec['question']\n",
    "    thought = rec['teacher_thought']\n",
    "    answer = rec['teacher_answer']\n",
    "    baseline_records.append({'prompt': q, 'answer': answer})\n",
    "    cot_records.append({'prompt': f\"{q} {thought}\", 'answer': answer})\n",
    "\n",
    "baseline_path = 'data/train_baseline.jsonl'\n",
    "cot_path = 'data/train_cot.jsonl'\n",
    "with open(baseline_path, 'w', encoding='utf-8') as f:\n",
    "    for r in baseline_records:\n",
    "        f.write(json.dumps(r) + '\\n')\n",
    "with open(cot_path, 'w', encoding='utf-8') as f:\n",
    "    for r in cot_records:\n",
    "        f.write(json.dumps(r) + '\\n')\n",
    "\n",
    "print(f\"Baseline corpus saved to {baseline_path}\")\n",
    "print(f\"CoT corpus saved to {cot_path}\")\n"
   ]
>>>>>>> origin/main
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
<<<<<<< HEAD
}
=======
}
>>>>>>> origin/main
